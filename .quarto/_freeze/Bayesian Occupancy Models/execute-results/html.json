{
  "hash": "1c0fbf822f6ba9d8a91667f273fd0f33",
  "result": {
    "markdown": "---\ntitle: \"Bayesian Occupancy Models\"\nauthor: \"Deon Roos\"\ndate: \"2025-04-22\"\noutput: html_document\n---\n\n\n# Statistical frameworks\n\nMost of the statistics you've probably learned so far has been based on what's called \"frequentist\" statistics. The analysis you'll be using for occupancy models (at least this particular implementation) will be using something called \"Bayesian\" statistics (pronounced \"Bay-zi-an\"). These are the two main frameworks currently used to learn something from data (unless you want to start splitting philosophical hairs). Within this document we'll go through what the differences are and how to move from frequentist to Bayesian stats, with a particular focus on Occupancy Models.\n\n# What is frequentist statistics?\n\nAlmost every scientist understands statistics under the frequentist framework. That's not because it's philosophically better or has a cleaner underlying logic. It's because that's what they were taught. As simple as that. This has led to frequentist methods becoming the default; not because they’re necessarily better, but simply because that’s how people were taught.\n\nFrequentist statistics is based on the idea that there's one fixed, true value (like the average height of all people) that we're trying to estimate. We can't see that true value directly, but we can estimate it by using stats to analyse data we've collected. The more data we have, especially if we had an infinite amount, the better our estimate would be (i.e. the closer we'll get to the elusive \"Truth\"). Frequentist methods revolve around the idea of something called long-run frequencies: if we repeated an experiment over and over, how often would we get a result as \"extreme\" as the one we observed? It doesn’t assign probabilities to hypotheses or parameters, only to the data we might observe under certain assumptions.\n\nIn contrast, Bayesian statistics treats unknown values (like parameters, e.g. the average human height) as something that is inherently uncertain (i.e. we won't ever be able to get to that \"Truth\"). Instead it allows us to assign probabilities to such values based on what we already believe (called a prior), then updates those beliefs with data. So where frequentists say, \"this result would happen 5% of the time by chance,\" Bayesians say, \"given the data, there’s an 80% chance this hypothesis is true.\"\n\nThink of it like this: a frequentist sees probability as something that only makes sense when talking about random events we could repeat (so called \"long-run frequencies\"), like flipping a coin or running an experiment thousands of times. So they’d never say, “there’s a 70% chance the average height is 1.75 m”, because the True average height is a fixed number, even if we don’t know it yet. Instead, they'd say, “if we did this study again and again, 95% of the time our results would fall within this range.” That's not particularly intuitive and is one of the big criticisms (at least mine) of frequentist statistics (so much of it is not intuitive).\n\nA Bayesian, on the other hand, is comfortable saying, “given what we knew before and what the data tells us now, there’s a 70% chance the average height is 1.75 m.” They treat unknowns, like the average human height, more like things we can be uncertain about and put probabilities on. It's like the difference between saying, \"How often would this result happen if we kept repeating the study?\" versus, \"Given everything we know, how confident are we in this being true?\" This is one of the big benefits of Bayesian statistics. It's closer to how normal humans actually think (at least for me).\n\nFor example, if I asked you what you think your chances are of getting a first on your thesis, you might be pessimistic and say something like 50%. That’s your **prior**; your belief about the outcome (here your grade) before seeing any data. Now, what if I told you that I’ve supervised 100 students before, and every single one of them got a first? With that new information, you’d probably update your belief and maybe revise your estimate up to, say, 80%. Still not 100% because of you still believe you only have a 50% chance but you're also not ignoring that 100% of students have had firsts. That’s the Bayesian approach: start with a prior belief, then update it using data. Conversely, if we were frequentists, we wouldn’t talk about the probability that you will get a first. Instead, we’d ask something like: \"If many students like you wrote their theses under similar conditions, how often would they get a first?\".\n\n# Technical differences\n\nThe equation you may well see whenever you look up Bayesian statistics is **Bayes' Theorem**, which is:\n\n$$\nP(Parameters | Data) = \\frac{P(Data | Parameters)\\times P(Parameters)}{P(Data)}\n$$\n\nwhere:\n\n* $P(Parameters | Data)$ is something called the **posterior**. In the above thesis grade example, this is the 80% probability to get a first (the $Parameter$), given 100 students all got firsts (the $Data$).\n\n* $P(Parameters)$ is the **prior** - you're belief before seeing the data. In the thesis example, this is your belief that you have a 50% chance of getting a first.\n\n* $P(Data | Parameters)$ is the **likelihood**, which is a measure of how likely the data is, given the parameter values.\n\n* $P(Data)$ is the evidence, or data. More technically, it's the probability of observing the data under any possible parameter value.\n\nMore plainly, it would translate to:\n\n> $Posterior = (Likelihood \\times Prior) \\div Evidence$\n\nFrequentists don’t actually calculate $P(Parameters | Data)$ because they treat the parameters as fixed (not random), i.e. there is one singular True value, not a distribution. So instead, they focus on the likelihood:\n\n$$\nL(Paramaters | Data) = P(Data | Parameters)\n$$\nwhich is **kind of** like saying that the posterior is simply the likelihood (but don't say that to any statisticians).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot(fit, \"beta\")\n# plot(fit, \"alpha\")\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}