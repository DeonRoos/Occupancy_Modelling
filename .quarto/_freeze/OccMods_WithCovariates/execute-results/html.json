{
  "hash": "3ee791e7e5aff2b734d20674ce348d00",
  "result": {
<<<<<<< HEAD
    "markdown": "---\ntitle: \"Including Covariates in Occupancy Models\"\nauthor: \"Deon Roos\"\ndate: \"2025-04-30\"\nformat:\n  html:\n    theme: flatly\n    highlight-style: monochrome\n    code-fold: true\n    toc: true\n    toc-depth: 2\n    toc-location: right\n---\n\n\n# The `R` packages\n\nAs before, this code uses `spOccupancy`, `ggplot2` and `patchwork` for the analysis and visualisations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spOccupancy)\nlibrary(ggplot2)\nlibrary(patchwork)\n```\n:::\n\n\n# The theory\n\nIn the previous document we went through a simple example to get a sense of how occupancy models work. We're now going to begin the process of slowly increasing the complexity. To start, we're going to need to include covariates (often called explanatory variables or independent variables or a wide variety of other names - it's a mess).\n\nLet's revisit our simple occupancy model:\n\n$$\nz_i \\sim Bernoulli(\\psi_i)\\\\\n$$\n\n$$\nlogit(\\psi_i) = \\beta_0\\\\\n$$\n\n$$\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n$$\n\n$$\nlogit(p_{i,j}) = \\alpha_0\n$$\n\nwhere the first two formula represent the state model (i.e. are our species present or absent from site $i$), and the last two form the detection model.\n\nWhat we're going to do in this section is to add in covariates to these equations, starting with the detection model.\n\nThe first thing to note is that $y$ is indexed by both $i$ and $j$, where $i$ was the site and $j$ was the survey. That $j$ is important because it allows us to specify covariates that vary not just by site (e.g. site 1 has 20 trees while site 2 has 10 trees) but by survey as well (e.g. on survey 1 in site 1, the temperature was 20 degrees, 15 degrees in survey 2 and 5 degrees in survey 3). This means that anything that was different in one survey to the next can be accounted for and included in the model (so long as we can measure it).\n\nSpecifically, these \"survey varying covariates\" are features that we think may have made us (or whoever or whatever collected the data) to be more or less effective. Using cameras? Well maybe the presence of fog has a big impact on how likely we are to detect elephants. Doing surveys yourself? Well maybe the hour of day that you did the survey had a big impact. The point is, we can include these variables that deal with differences in detection probability.\n\nI'll simulate a new dataset (with a bit more data for us to work with - 64 sites surveyed 3 times each) to show this off. We can say that the detection covariate in this case is rainfall, but note that the values for rainfall will be centred on zero (don't worry about this).\n\nHere's what the data looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ndat <- simOcc(J.x = 8, \n              J.y = 8, \n              n.rep = rep(3, times = 8 * 8), \n              beta = c(1), \n              alpha = c(-2, 0.5))\nobs <- dat$y\ndet_cov <- dat$X.p[,,2]\ndf <- data.frame(\n  survey = rep(1:3, each = 64),\n  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),\n  y = c(obs[,1], obs[,2], obs[,3])\n)\n\np1 <- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = cov)) +\n  geom_jitter(aes(x = factor(survey), y = cov),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Rainfall\") +\n  theme_minimal()\n\np2 <- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = cov),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Rainfall\", y = \"Elephant detection\") +\n  theme_minimal()\n\np1 + p2 + plot_annotation(tag_levels = \"A\", tag_suffix = \")\")\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nNotice anything? In **A)** we can see that in each survey, there's a lot of variation across the 64 sites in how much rainfall there was; maybe peaking in survey 2. In **B)**, it looks like it might be more likely that we detect elephants more often when there's more rainfall.\n\nAre you sure there's an influence? If so, how strong is it? *Exactly* how strong is it?\n\nThere's no way you can answer those questions. That's where we need stats. So let's add in rainfall to our model. I'll recycle the code from the previous document and include rainfall. Here's how we do that.\n\n# Data preparation\n\nJust like in the previous document, we need to include our different datasets into a `list`. If you're still not sure what a `list` is in `R`, think of it like a folder on your computer. You can add lots of different files to a folder, but they're all \"tied\" together by being within the same folder. That's the same as a `list` in `R`.\n\nHere, I'm going to do something seemingly strange. I'm going to create a `list` and include this into our `etosha` `list`. The reason is that we might have more than one detection covariate, so having a list, though redundant here, will make adding additional variables easier in the future.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note you wouldn't need to do the dat$X.p[,,2] bit\n# That's just because the data is simulated.\ndet.covs <- list(rain = dat$X.p[,,2])\n\netosha <- list(\n  y = dat$y,\n  det.covs = det.covs\n)\n```\n:::\n\n\nAnd we're good to go on to the modelling.\n\n# Fitting the model\n\nA few things to note in the `R` code;\n\n* `occ.formula = ~ 1` is the equivalent to $logit(\\psi_{i,j}) = \\beta_0$\n\n  + I.e. an intercept only occupancy model, meaning we are not including any covariates for the probability that a site is occupied.\n  \n* `det.formula = ~ rain` is the equivalent to $logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i$\n\n  + I.e. we have both an intercept and slope given we have included rain in the model.\n  \n* We specify that all data is contained in the list (i.e. \"folder\") called `etosha`.\n\n* The remainder of the arguments (e.g. `n.chains`) can be ignored for now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- PGOcc(\n  # The state model (i.e. what % that elephants are present?)\n  # ~ 1 means we want an intercept only model (no covariates)\n  occ.formula = ~ 1, \n  # The observation model (i.e. what % that we see elephants if present?)\n  det.formula = ~ rain, \n  # Our carefully formatted dataset\n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n```\n:::\n\n\n# Interpreting\n\nHaving now fit the model to the data, we can see what we've learnt:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.017\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n```\n:::\n:::\n\n\nCompared to the model in the previous document we now have additional information for `Detection (logit scale)`; we have both an `(Intercept)` and `rain`. These are $\\alpha_0$ and $\\alpha_1$ from our detection model $logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i$. If we really wanted to, we could now replace the parameter labels (e.g. the $\\alpha$s and $\\beta$s) with their now estimated values which would look like (rounding the estimates to two decimal points arbitrarily):\n\n$$\nz_i \\sim Bernoulli(\\psi_i)\\\\$$\n$$\n\n$$\nlogit(\\psi_i) = 1.88\\\\\n$$\n\n$$\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n$$\n\n$$\nlogit(p_{i,j}) = -1.89 + 0.69 \\times Rain_i\n$$\n\nWith this, we can swap out $Rain$ for any value that we might be interested in, to see how our detection probability changes. For example, let's see what happens when $Rain = 1$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n-1.89 + 0.69 * 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -1.2\n```\n:::\n:::\n\n\nOur logit value is -1.2. How do we get that into probabilities that we can actually understand? Backtransform out of logit using `plogis()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplogis(-1.89 + 0.69 * 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2314752\n```\n:::\n:::\n\n\nAnd we get a ca. 23% chance to detect an elephant when $Rain = 1$. What about when $Rain = 0$? Well, we can do that easily enough now that we know thew general steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplogis(-1.89 + 0.69 * 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1312445\n```\n:::\n:::\n\n\nWhen $Rain = 0$, we predict a ca. 13% chance to detect elephants.\n\nThis approach, whereby we make a prediction for a specific value of $Rain$ can be extended into making multiple predictions at once, such that we can then draw a line through them. Here's how we'd do that.\n\n# Plot predicted relationships\n\nWe start by creating a sequence of $Rain$ values, rather than doing one a time. Here we use the `seq()` function to create a sequence, which will range from the minimum $Rain$ value to the maximum within our dataset. The number of values that we want in this sequence is specified as 20 but we can choose any value here - we just need enough that the line is drawn \"accurately\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrain <- seq(from = min(det.covs$rain),\n            to = max(det.covs$rain),\n            length.out = 20)\n\nrain\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -2.71815687 -2.44127881 -2.16440076 -1.88752271 -1.61064465 -1.33376660\n [7] -1.05688855 -0.78001049 -0.50313244 -0.22625439  0.05062367  0.32750172\n[13]  0.60437977  0.88125783  1.15813588  1.43501393  1.71189199  1.98877004\n[19]  2.26564809  2.54252615\n```\n:::\n:::\n\n\nNow we have our values, we don't want to manually enter each value into our equation. Instead we can use the fact that `R` works with vectors (i.e. columns of data) to do this quickly and easily:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- plogis(-1.89 + 0.69 * rain)\npred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.02263134 0.02726568 0.03281714 0.03945308 0.04736516 0.05677017\n [7] 0.06790956 0.08104689 0.09646267 0.11444547 0.13527876 0.15922259\n[13] 0.18649040 0.21722152 0.25145143 0.28908330 0.32986526 0.37337882\n[19] 0.41904309 0.46613767\n```\n:::\n:::\n\n\nNow for each value of rain, we have the predicted probability of detecting an elephant. Useful but you wouldn't want to throw these two columns at your audience/reader and expect them to make sense of it. It'd be better if we include these in a figure.\n\nTo do so, we'll combine both columns into a single dataset and plot using `ggplot2`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(\n  pred,\n  rain\n)\n\nggplot(df) +\n  geom_line(aes(x = rain, y = pred))\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nFrom this figure it now appears much more intuitive that increasing rain makes elephants easier to detect. We can do a little \"tidying\" of the figure to make it more visually pleasing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n# Uncertainty\n\nThe above figure is a good start but we're missing any measure of uncertainty. If we were doing frequentist statistics, we would use 95% confidence intervals but these are exclusively frequentist. There are no 95% confidence intervals when we use the Bayesian statistical framework. Instead we have *credible intervals*. I'll explain the Bayesian framework in a subsequent workflow but for now here is the formal definition of a credible interval:\n\n> There is a 95% probability that the **True** parameter value lies within the interval range, given the data and model.\n\nThis is in contrast with the frequentist confidence interval whose formal definition is so bizarre and unintuitive that it's barely useful. 95% *credible intervals* are useful and work exactly the way people *think* frequentist intervals work.\n\nBut how do we include these in the figure? Well, the model summary makes it easy to find the values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.017\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n```\n:::\n:::\n\n\nThey're the `2.5%` and `97.5%` values in our summary table. So all we need to do is repeat our predicions using these values to get out measure of uncertainty to include in the figure. Importantly, if you remember back to BI3010 where you had to multiple standard error by 1.96, we *do not* need to do that here. That's frequentist nonsense - we're Bayesian now.\n\nLet's do just that:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$low <- plogis(-2.4610 + 0.2328 * df$rain)\ndf$upp <- plogis(-1.2939 + 1.1801 * df$rain)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         pred        rain        low        upp\n1  0.02263134 -2.71815687 0.04336427 0.01096960\n2  0.02726568 -2.44127881 0.04611831 0.01514457\n3  0.03281714 -2.16440076 0.04903828 0.02087495\n4  0.03945308 -1.88752271 0.05213304 0.02871039\n5  0.04736516 -1.61064465 0.05541172 0.03936862\n6  0.05677017 -1.33376660 0.05888379 0.05376451\n7  0.06790956 -1.05688855 0.06255900 0.07302436\n8  0.08104689 -0.78001049 0.06644741 0.09846565\n9  0.09646267 -0.50313244 0.07055932 0.13151304\n10 0.11444547 -0.22625439 0.07490526 0.17351714\n11 0.13527876  0.05062367 0.07949599 0.22545433\n12 0.15922259  0.32750172 0.08434241 0.28752905\n13 0.18649040  0.60437977 0.08945559 0.35877811\n14 0.21722152  0.88125783 0.09484663 0.43685701\n15 0.25145143  1.15813588 0.10052670 0.51819600\n16 0.28908330  1.43501393 0.10650691 0.59858193\n17 0.32986526  1.71189199 0.11279825 0.67399363\n18 0.37337882  1.98877004 0.11941156 0.74135968\n19 0.41904309  2.26564809 0.12635738 0.79895748\n20 0.46613767  2.54252615 0.13364590 0.84638633\n```\n:::\n:::\n\n\nAnd we can add in our uncertainty using `geom_ribbon()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  geom_ribbon(aes(x = rain, ymin = low, ymax = upp),\n              alpha = 0.3) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWith that, we have a publication ready figure.\n\n# Multiple covariates\n\nLet's increase the complexity a bit and have the simulation include multiple covariates. We'll say that tree height and average temperature affect whether or not a site is occupied (elephants will like tall trees and cooler locations). We'll still have rain affect our detection probability, as above.\n\nTo create the figures below we need to do some tweaks to the data. The `df` object I create has one row per survey, with three surveys per site. Our occupancy covariates, `tree` and `temp` have just one value; these do not change from one survey to the next. If a tree is 3 m tall in survey one, then it'll still be 3 m tall in surveys two and three.\n\nA note here is that the covariates are still centered on zero. That's just the way the data is simulated but the data you collect does not need to be the same (so ignore the fact that we will have trees that are -1 m tall - the general idea doesn't change).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ndat <- simOcc(J.x = 8, \n              J.y = 8, \n              n.rep = rep(3, times = 8 * 8), \n              beta = c(1, -0.2, 0.3), \n              alpha = c(-2, 0.5))\nobs <- dat$y\ntemp <- dat$X[,2]\ntree <- dat$X[,3]\ndet_cov <- dat$X.p[,,2]\ndf <- data.frame(\n  survey = rep(1:3, each = 64),\n  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),\n  tree = rep(tree, times = 3),\n  temp = rep(temp, times = 3),\n  y = c(obs[,1], obs[,2], obs[,3])\n)\n\np1 <- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = temp)) +\n  geom_jitter(aes(x = factor(survey), y = temp),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Temperature\") +\n  theme_minimal()\n\np2 <- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = temp),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Temperature\", y = \"Elephant detection\") +\n  theme_minimal()\n\np3 <- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = tree)) +\n  geom_jitter(aes(x = factor(survey), y = tree),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Tree\") +\n  theme_minimal()\n\np4 <- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = tree),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Tree\", y = \"Elephant detection\") +\n  theme_minimal()\n\n(p1 + p2) / (p3 + p4)\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThere are two things worth highlighting from the above figures. Firstly, the boxplots on the right are identical for each of the three surveys (the points are jittered so are randomly placed but trust me that these are identical). Secondly, it becomes quite hard to see any clear pattern in the two right hand figures. Keep in mind that the zeros here are actively misleading us. Some of the zeros are genuine in that there were no elephants there and `tree` and `temp` likely caused that (which we only know because the data is simulated) but the other zeros are false negatives; The elephants were there, we just didn't see them.\n\nThis is why we need a model to figure out what the relationships are. We can no longer trust our eyes to see the pattern.\n\nSo let's get our data organised such that we can fit our model.\n\n# Data preparation\n\nAs before, we provide our detection covariates as a list but XXX\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndet.covs <- list(rain = dat$X.p[,,2])\nocc.covs <- data.frame(tree = tree, temp = temp)\netosha <- list(\n  y = dat$y,\n  det.covs = det.covs,\n  occ.covs = occ.covs\n)\n```\n:::\n\n\n# Fitting the model\n\nThe core model we're going to fit is:\n\n$$\nz_i \\sim Bernoulli(\\psi_i)\\\\\nlogit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\nlogit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\n$$\n\nThe main thing that I want to highlight here, other than having included $Tree$ and $Temp$ as effecting occupancy probability ($\\psi$), is that the subscripts are different. \n\nIn the occupancy model ($logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i$) the variables are subscript by $i$, indicating that we have one value of $Tree$ or $Rain$ for each site $i$.\n\nBut in the detection model ($logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}$) $Rain$ is subscript by both $i$ and $j$. That's because we have a different $Rain$ value for each survey. This would be something we record every time we visit the site (or extract from some online source for each day).\n\nKeep this in mind when you're collecting your own data. Variables that you think affect occupancy will have one value per site. Variables that you think affect detection will (generally) have one value per survey.\n\nTo fit this is relatively straight forward. For the occupancy model, we write `occ.formula = ~ tree + temp`, and for the detection model we write `det.formula = ~ rain`, and let the machinery do it's thing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n```\n:::\n\n\nOnce it's run, we can check the `summary()` to see what the parameters were estimated as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.019\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.9424 1.0250  0.1512  1.8670 4.0917 1.0187 347\ntree         0.3904 1.3242 -2.0614  0.3232 3.0704 1.0196 267\ntemp        -1.2186 1.0959 -3.2786 -1.2776 1.1046 1.0246 267\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8408 0.2706 -2.3623 -1.8447 -1.2875 1.0066 1117\nrain         0.1666 0.2196 -0.2592  0.1611  0.6073 1.0038 3054\n```\n:::\n:::\n\n\nWe see that `tree` has a positive effect on occupancy and `temp` has a negative effect. This is how the simulation was carried out, but the values do not match up particularly well. Specifically, in the simulation `tree` was set to `0.3` but has been estimated as `0.2`, while `temp` was set to `-0.2` but has been estimated as `-1.2`.\n\nSo we've got the general relationships reasonably well estimated but it's not particularly accurate. This is where having *credible intervals* is useful. Keep in mind that credible intervals are, correctly, interpreted as having a 95% chance of containing the \"True\" value (the True value here is what each parameter was set to in the simulation). For both `tree` and `temp` the True parameter value is indeed within the 95% credible intervals! Both 95% CI are wide, but that's good here! The model isn't entirely sure what the values are (partly because of sample size and the effects being subtle) and that is being conveyed to us! We're not deluding ourselves into thinking we have a perfect understanding when we don't!\n\nThere are some other warning signs that the model might not be performing especially well. The first is that the `Rhat` values are getting uncomfortably large for the `Occurence` parameters. My personal rule of thumb is `Rhat` values close to 1.05 is where I get worried. None of our parameters are at 1.05 but they're close enough that I'm paying attention to them and want to see if I can fix the problem.\n\nThe other warning sign is the `ESS` for the `Occurence` model are all \"low\". At least noticeably lower than the `Detection` model parameters. Again, the rule of thumb here is that we want hundreds or thousands of \"Effective Sample Size\", so we're technically OK but I'm still concerned.\n\nNow, to be clear, we ignored these issues with the first model we ran in this document, we here we're going to see if we can't resolve this problem.\n\n# Resolving issues\n\nTo fully appreciate the solution we'll attempt requires a deeper understanding of Bayesian statistics. But for now, we're not going to cover this. Instead, I'll simply say that the \"machinery\" of Bayesian statistics revolves around giving a number of algorithms (called \"chains\") a number of guesses (called \"iterations\") to try and figure out the most likely values for our parameters.\n\nThe two metrics we used above, `Rhat` and `ESS`, both monitor these chains and iterations and let us know if they are *not in agreement*. When `Rhat` values get high, and `ESS` values get low, it can suggests we simply need to give the algorithms more iterations to figure out the best values.\n\nInevitably, the details are more complex than I can convey in two paragraphs but the general idea is there. If either number looks a bit worrying, it's relatively cheap and easy to just rerun the model with more iterations and see if that solves the problem.\n\nLet's do just that by increasing the number of iterations to 3000 per chain:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # We're using four chains/algorithms\n  n.chains = 4,\n  # We allow 3000 guesses (increased from 2000)\n  n.samples = 3000,\n  # We ignore the first 300 (increased from 200)\n  # We ignore them because we assume the algorithms are not particularly reliable\n  # in the first ca. 10% of guesses\n  n.burn = 300,\n  verbose = FALSE)\n```\n:::\n\n\nOnce run, we can check how well they ran:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 3000, verbose = FALSE, n.burn = 300, n.chains = 4)\n\nSamples per Chain: 3000\nBurn-in: 300\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 10800\nRun Time (min): 0.027\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0655 1.0246  0.1977  2.0138 4.2114 1.0010 529\ntree         0.3721 1.3145 -2.0763  0.3028 3.0319 1.0662 375\ntemp        -1.2063 1.1214 -3.3214 -1.2363 1.1050 1.0220 422\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8530 0.2644 -2.3777 -1.8521 -1.3301 1.0020 1914\nrain         0.1715 0.2210 -0.2506  0.1682  0.6072 1.0004 4390\n```\n:::\n:::\n\n\nNow when we look at the `Rhat` and `ESS` values, we doing better. Still not perfect but enough for me to be happy that *by these two metrics alone* there's nothing to suggest the machinery is struggling and that the parameters are being estimated as robustly as possible.\n\n> Importantly, these are not the only metrics or tools available to us. We'll check out the other options in a later document.\n\nAs far as we can tell, for now, we can trust this model and move on to plotting the predicted relationships.\n\n# Plot predicted relationships\n\nPreviously, we made the predictions ourselves by \"hand\". This time, we're going to use the `predict()` function to do this for us to make life easier on ourselves. Importantly, the process is exactly the same; \n\n1. Create a \"fake\" dataset with the covariate values we want to predict for.\n\n2. Apply this fake data to our equation, with the now estimated parameter values, to generate the predictions.\n\n3. Convert from logit values to probabilities.\n\n4. Plot the predicted probabilities against the fake covariate values to show the estimated relationship.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX.0 <- cbind(\n  1,\n  temp = seq(from = min(occ.covs$temp), \n             to = max(occ.covs$temp),\n             length.out = 50),\n  tree = 0\n)\n\n#predict(fit, X.0)\n```\n:::\n\n\n# How should I store my data?\n\nIn order to be able to run the model, we need the data to be in a particular order\n",
=======
    "engine": "knitr",
    "markdown": "---\ntitle: \"Including Covariates in Occupancy Models\"\nauthor: \"Deon Roos\"\ndate: \"2025-04-30\"\nformat:\n  html:\n    theme: flatly\n    highlight-style: monochrome\n    code-fold: true\n    toc: true\n    toc-depth: 2\n    toc-location: right\n---\n\n\n\n# The `R` packages\n\nAs before, this code uses `spOccupancy`, `ggplot2` and `patchwork` for the analysis and visualisations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spOccupancy)\nlibrary(ggplot2)\nlibrary(patchwork)\n```\n:::\n\n\n\n# The theory\n\nIn the previous document we went through a simple example to get a sense of how occupancy models work. We're now going to begin the process of slowly increasing the complexity. To start, we're going to need to include covariates (often called explanatory variables or independent variables or a wide variety of other names - it's a mess).\n\nLet's revisit our simple occupancy model:\n\n$$\nz_i \\sim Bernoulli(\\psi_i)\\\\\n$$\n\n$$\nlogit(\\psi_i) = \\beta_0\\\\\n$$\n\n$$\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n$$\n\n$$\nlogit(p_{i,j}) = \\alpha_0\n$$\n\nwhere the first two formula represent the state model (i.e. are our species present or absent from site $i$), and the last two form the detection model.\n\nWhat we're going to do in this section is to add in covariates to these equations, starting with the detection model.\n\nThe first thing to note is that $y$ is indexed by both $i$ and $j$, where $i$ was the site and $j$ was the survey. That $j$ is important because it allows us to specify covariates that vary not just by site (e.g. site 1 has 20 trees while site 2 has 10 trees) but by survey as well (e.g. on survey 1 in site 1, the temperature was 20 degrees, 15 degrees in survey 2 and 5 degrees in survey 3). This means that anything that was different in one survey to the next can be accounted for and included in the model (so long as we can measure it).\n\nSpecifically, these \"survey varying covariates\" are features that we think may have made us (or whoever or whatever collected the data) to be more or less effective. Using cameras? Well maybe the presence of fog has a big impact on how likely we are to detect elephants. Doing surveys yourself? Well maybe the hour of day that you did the survey had a big impact. The point is, we can include these variables that deal with differences in detection probability.\n\nI'll simulate a new dataset (with a bit more data for us to work with - 64 sites surveyed 3 times each) to show this off. We can say that the detection covariate in this case is rainfall, but note that the values for rainfall will be centred on zero (don't worry about this).\n\nHere's what the data looks like:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ndat <- simOcc(J.x = 8, \n              J.y = 8, \n              n.rep = rep(3, times = 8 * 8), \n              beta = c(1), \n              alpha = c(-2, 0.5))\nobs <- dat$y\ndet_cov <- dat$X.p[,,2]\ndf <- data.frame(\n  survey = rep(1:3, each = 64),\n  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),\n  y = c(obs[,1], obs[,2], obs[,3])\n)\n\np1 <- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = cov)) +\n  geom_jitter(aes(x = factor(survey), y = cov),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Rainfall\") +\n  theme_minimal()\n\np2 <- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = cov),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Rainfall\", y = \"Elephant detection\") +\n  theme_minimal()\n\np1 + p2 + plot_annotation(tag_levels = \"A\", tag_suffix = \")\")\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nNotice anything? In **A)** we can see that in each survey, there's a lot of variation across the 64 sites in how much rainfall there was; maybe peaking in survey 2. In **B)**, it looks like it might be more likely that we detect elephants more often when there's more rainfall.\n\nAre you sure there's an influence? If so, how strong is it? *Exactly* how strong is it?\n\nThere's no way you can answer those questions. That's where we need stats. So let's add in rainfall to our model. I'll recycle the code from the previous document and include rainfall. Here's how we do that.\n\n# Data preparation\n\nJust like in the previous document, we need to include our different datasets into a `list`. If you're still not sure what a `list` is in `R`, think of it like a folder on your computer. You can add lots of different files to a folder, but they're all \"tied\" together by being within the same folder. That's the same as a `list` in `R`.\n\nHere, I'm going to do something seemingly strange. I'm going to create a `list` and include this into our `etosha` `list`. The reason is that we might have more than one detection covariate, so having a list, though redundant here, will make adding additional variables easier in the future.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note you wouldn't need to do the dat$X.p[,,2] bit\n# That's just because the data is simulated.\ndet.covs <- list(rain = dat$X.p[,,2])\n\netosha <- list(\n  y = dat$y,\n  det.covs = det.covs\n)\n```\n:::\n\n\n\nAnd we're good to go on to the modelling.\n\n# Fitting the model\n\nA few things to note in the `R` code;\n\n* `occ.formula = ~ 1` is the equivalent to $logit(\\psi_{i,j}) = \\beta_0$\n\n  + I.e. an intercept only occupancy model, meaning we are not including any covariates for the probability that a site is occupied.\n  \n* `det.formula = ~ rain` is the equivalent to $logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i$\n\n  + I.e. we have both an intercept and slope given we have included rain in the model.\n  \n* We specify that all data is contained in the list (i.e. \"folder\") called `etosha`.\n\n* The remainder of the arguments (e.g. `n.chains`) can be ignored for now.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- PGOcc(\n  # The state model (i.e. what % that elephants are present?)\n  # ~ 1 means we want an intercept only model (no covariates)\n  occ.formula = ~ 1, \n  # The observation model (i.e. what % that we see elephants if present?)\n  det.formula = ~ rain, \n  # Our carefully formatted dataset\n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n```\n:::\n\n\n\n# Interpreting\n\nHaving now fit the model to the data, we can see what we've learnt:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0152\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n```\n\n\n:::\n:::\n\n\n\nCompared to the model in the previous document we now have additional information for `Detection (logit scale)`; we have both an `(Intercept)` and `rain`. These are $\\alpha_0$ and $\\alpha_1$ from our detection model $logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i$. If we really wanted to, we could now replace the parameter labels (e.g. the $\\alpha$s and $\\beta$s) with their now estimated values which would look like (rounding the estimates to two decimal points arbitrarily):\n\n$$\nz_i \\sim Bernoulli(\\psi_i)\\\\$$\n$$\n\n$$\nlogit(\\psi_i) = 1.88\\\\\n$$\n\n$$\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n$$\n\n$$\nlogit(p_{i,j}) = -1.89 + 0.69 \\times Rain_i\n$$\n\nWith this, we can swap out $Rain$ for any value that we might be interested in, to see how our detection probability changes. For example, let's see what happens when $Rain = 1$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n-1.89 + 0.69 * 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.2\n```\n\n\n:::\n:::\n\n\n\nOur logit value is -1.2. How do we get that into probabilities that we can actually understand? Backtransform out of logit using `plogis()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplogis(-1.89 + 0.69 * 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2314752\n```\n\n\n:::\n:::\n\n\n\nAnd we get a ca. 23% chance to detect an elephant when $Rain = 1$. What about when $Rain = 0$? Well, we can do that easily enough now that we know thew general steps:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplogis(-1.89 + 0.69 * 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1312445\n```\n\n\n:::\n:::\n\n\n\nWhen $Rain = 0$, we predict a ca. 13% chance to detect elephants.\n\nThis approach, whereby we make a prediction for a specific value of $Rain$ can be extended into making multiple predictions at once, such that we can then draw a line through them. Here's how we'd do that.\n\n# Plot predicted relationships\n\nWe start by creating a sequence of $Rain$ values, rather than doing one a time. Here we use the `seq()` function to create a sequence, which will range from the minimum $Rain$ value to the maximum within our dataset. The number of values that we want in this sequence is specified as 20 but we can choose any value here - we just need enough that the line is drawn \"accurately\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrain <- seq(from = min(det.covs$rain),\n            to = max(det.covs$rain),\n            length.out = 20)\n\nrain\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] -2.71815687 -2.44127881 -2.16440076 -1.88752271 -1.61064465 -1.33376660\n [7] -1.05688855 -0.78001049 -0.50313244 -0.22625439  0.05062367  0.32750172\n[13]  0.60437977  0.88125783  1.15813588  1.43501393  1.71189199  1.98877004\n[19]  2.26564809  2.54252615\n```\n\n\n:::\n:::\n\n\n\nNow we have our values, we don't want to manually enter each value into our equation. Instead we can use the fact that `R` works with vectors (i.e. columns of data) to do this quickly and easily:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- plogis(-1.89 + 0.69 * rain)\npred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.02263134 0.02726568 0.03281714 0.03945308 0.04736516 0.05677017\n [7] 0.06790956 0.08104689 0.09646267 0.11444547 0.13527876 0.15922259\n[13] 0.18649040 0.21722152 0.25145143 0.28908330 0.32986526 0.37337882\n[19] 0.41904309 0.46613767\n```\n\n\n:::\n:::\n\n\n\nNow for each value of rain, we have the predicted probability of detecting an elephant. Useful but you wouldn't want to throw these two columns at your audience/reader and expect them to make sense of it. It'd be better if we include these in a figure.\n\nTo do so, we'll combine both columns into a single dataset and plot using `ggplot2`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(\n  pred,\n  rain\n)\n\nggplot(df) +\n  geom_line(aes(x = rain, y = pred))\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nFrom this figure it now appears much more intuitive that increasing rain makes elephants easier to detect. We can do a little \"tidying\" of the figure to make it more visually pleasing:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n# Uncertainty\n\nThe above figure is a good start but we're missing any measure of uncertainty. If we were doing frequentist statistics, we would use 95% confidence intervals but these are exclusively frequentist. There are no 95% confidence intervals when we use the Bayesian statistical framework. Instead we have *credible intervals*. I'll explain the Bayesian framework in a subsequent workflow but for now here is the formal definition of a credible interval:\n\n> There is a 95% probability that the **True** parameter value lies within the interval range, given the data and model.\n\nThis is in contrast with the frequentist confidence interval whose formal definition is so bizarre and unintuitive that it's barely useful. 95% *credible intervals* are useful and work exactly the way people *think* frequentist intervals work.\n\nBut how do we include these in the figure? Well, the model summary makes it easy to find the values:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0152\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n```\n\n\n:::\n:::\n\n\n\nThey're the `2.5%` and `97.5%` values in our summary table. So all we need to do is repeat our predicions using these values to get out measure of uncertainty to include in the figure. Importantly, if you remember back to BI3010 where you had to multiple standard error by 1.96, we *do not* need to do that here. That's frequentist nonsense - we're Bayesian now.\n\nLet's do just that:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$low <- plogis(-2.4610 + 0.2328 * df$rain)\ndf$upp <- plogis(-1.2939 + 1.1801 * df$rain)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         pred        rain        low        upp\n1  0.02263134 -2.71815687 0.04336427 0.01096960\n2  0.02726568 -2.44127881 0.04611831 0.01514457\n3  0.03281714 -2.16440076 0.04903828 0.02087495\n4  0.03945308 -1.88752271 0.05213304 0.02871039\n5  0.04736516 -1.61064465 0.05541172 0.03936862\n6  0.05677017 -1.33376660 0.05888379 0.05376451\n7  0.06790956 -1.05688855 0.06255900 0.07302436\n8  0.08104689 -0.78001049 0.06644741 0.09846565\n9  0.09646267 -0.50313244 0.07055932 0.13151304\n10 0.11444547 -0.22625439 0.07490526 0.17351714\n11 0.13527876  0.05062367 0.07949599 0.22545433\n12 0.15922259  0.32750172 0.08434241 0.28752905\n13 0.18649040  0.60437977 0.08945559 0.35877811\n14 0.21722152  0.88125783 0.09484663 0.43685701\n15 0.25145143  1.15813588 0.10052670 0.51819600\n16 0.28908330  1.43501393 0.10650691 0.59858193\n17 0.32986526  1.71189199 0.11279825 0.67399363\n18 0.37337882  1.98877004 0.11941156 0.74135968\n19 0.41904309  2.26564809 0.12635738 0.79895748\n20 0.46613767  2.54252615 0.13364590 0.84638633\n```\n\n\n:::\n:::\n\n\n\nAnd we can add in our uncertainty using `geom_ribbon()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  geom_ribbon(aes(x = rain, ymin = low, ymax = upp),\n              alpha = 0.3) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nWith that, we have a publication ready figure.\n\n# Multiple covariates\n\nLet's increase the complexity a bit and have the simulation include multiple covariates. We'll say that tree height and average temperature affect whether or not a site is occupied (elephants will like tall trees and cooler locations). We'll still have rain affect our detection probability, as above.\n\nTo create the figures below we need to do some tweaks to the data. The `df` object I create has one row per survey, with three surveys per site. Our occupancy covariates, `tree` and `temp` have just one value; these do not change from one survey to the next. If a tree is 3 m tall in survey one, then it'll still be 3 m tall in surveys two and three.\n\nA note here is that the covariates are still centered on zero. That's just the way the data is simulated but the data you collect does not need to be the same (so ignore the fact that we will have trees that are -1 m tall - the general idea doesn't change).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ndat <- simOcc(J.x = 8, \n              J.y = 8, \n              n.rep = rep(3, times = 8 * 8), \n              beta = c(1, -0.2, 0.3), \n              alpha = c(-2, 0.5))\nobs <- dat$y\ntemp <- dat$X[,2]\ntree <- dat$X[,3]\ndet_cov <- dat$X.p[,,2]\ndf <- data.frame(\n  survey = rep(1:3, each = 64),\n  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),\n  tree = rep(tree, times = 3),\n  temp = rep(temp, times = 3),\n  y = c(obs[,1], obs[,2], obs[,3])\n)\n\np1 <- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = temp)) +\n  geom_jitter(aes(x = factor(survey), y = temp),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Temperature\") +\n  theme_minimal()\n\np2 <- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = temp),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Temperature\", y = \"Elephant detection\") +\n  theme_minimal()\n\np3 <- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = tree)) +\n  geom_jitter(aes(x = factor(survey), y = tree),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Tree\") +\n  theme_minimal()\n\np4 <- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = tree),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Tree\", y = \"Elephant detection\") +\n  theme_minimal()\n\n(p1 + p2) / (p3 + p4)\n```\n\n::: {.cell-output-display}\n![](OccMods_WithCovariates_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nThere are two things worth highlighting from the above figures. Firstly, the boxplots on the right are identical for each of the three surveys (the points are jittered so are randomly placed but trust me that these are identical). Secondly, it becomes quite hard to see any clear pattern in the two right hand figures. Keep in mind that the zeros here are actively misleading us. Some of the zeros are genuine in that there were no elephants there and `tree` and `temp` likely caused that (which we only know because the data is simulated) but the other zeros are false negatives; The elephants were there, we just didn't see them.\n\nThis is why we need a model to figure out what the relationships are. We can no longer trust our eyes to see the pattern.\n\nSo let's get our data organised such that we can fit our model.\n\n# Data preparation\n\nAs before, we provide our detection covariates as a list but XXX\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndet.covs <- list(rain = dat$X.p[,,2])\nocc.covs <- data.frame(tree = tree, temp = temp)\netosha <- list(\n  y = dat$y,\n  det.covs = det.covs,\n  occ.covs = occ.covs\n)\n```\n:::\n\n\n\n# Fitting the model\n\nThe core model we're going to fit is:\n\n$$\nz_i \\sim Bernoulli(\\psi_i)\\\\\nlogit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\nlogit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\n$$\n\nThe main thing that I want to highlight here, other than having included $Tree$ and $Temp$ as effecting occupancy probability ($\\psi$), is that the subscripts are different. \n\nIn the occupancy model ($logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i$) the variables are subscript by $i$, indicating that we have one value of $Tree$ or $Rain$ for each site $i$.\n\nBut in the detection model ($logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}$) $Rain$ is subscript by both $i$ and $j$. That's because we have a different $Rain$ value for each survey. This would be something we record every time we visit the site (or extract from some online source for each day).\n\nKeep this in mind when you're collecting your own data. Variables that you think affect occupancy will have one value per site. Variables that you think affect detection will (generally) have one value per survey.\n\nTo fit this is relatively straight forward. For the occupancy model, we write `occ.formula = ~ tree + temp`, and for the detection model we write `det.formula = ~ rain`, and let the machinery do it's thing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n```\n:::\n\n\n\nOnce it's run, we can check the `summary()` to see what the parameters were estimated as:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0168\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.9424 1.0250  0.1512  1.8670 4.0917 1.0187 347\ntree         0.3904 1.3242 -2.0614  0.3232 3.0704 1.0196 267\ntemp        -1.2186 1.0959 -3.2786 -1.2776 1.1046 1.0246 267\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8408 0.2706 -2.3623 -1.8447 -1.2875 1.0066 1117\nrain         0.1666 0.2196 -0.2592  0.1611  0.6073 1.0038 3054\n```\n\n\n:::\n:::\n\n\n\nWe see that `tree` has a positive effect on occupancy and `temp` has a negative effect. This is how the simulation was carried out, but the values do not match up particularly well. Specifically, in the simulation `tree` was set to `0.3` but has been estimated as `0.2`, while `temp` was set to `-0.2` but has been estimated as `-1.2`.\n\nSo we've got the general relationships reasonably well estimated but it's not particularly accurate. This is where having *credible intervals* is useful. Keep in mind that credible intervals are, correctly, interpreted as having a 95% chance of containing the \"True\" value (the True value here is what each parameter was set to in the simulation). For both `tree` and `temp` the True parameter value is indeed within the 95% credible intervals! Both 95% CI are wide, but that's good here! The model isn't entirely sure what the values are (partly because of sample size and the effects being subtle) and that is being conveyed to us! We're not deluding ourselves into thinking we have a perfect understanding when we don't!\n\nThere are some other warning signs that the model might not be performing especially well. The first is that the `Rhat` values are getting uncomfortably large for the `Occurence` parameters. My personal rule of thumb is `Rhat` values close to 1.05 is where I get worried. None of our parameters are at 1.05 but they're close enough that I'm paying attention to them and want to see if I can fix the problem.\n\nThe other warning sign is the `ESS` for the `Occurence` model are all \"low\". At least noticeably lower than the `Detection` model parameters. Again, the rule of thumb here is that we want hundreds or thousands of \"Effective Sample Size\", so we're technically OK but I'm still concerned.\n\nNow, to be clear, we ignored these issues with the first model we ran in this document, we here we're going to see if we can't resolve this problem.\n\n# Resolving issues\n\nTo fully appreciate the solution we'll attempt requires a deeper understanding of Bayesian statistics. But for now, we're not going to cover this. Instead, I'll simply say that the \"machinery\" of Bayesian statistics revolves around giving a number of algorithms (called \"chains\") a number of guesses (called \"iterations\") to try and figure out the most likely values for our parameters.\n\nThe two metrics we used above, `Rhat` and `ESS`, both monitor these chains and iterations and let us know if they are *not in agreement*. When `Rhat` values get high, and `ESS` values get low, it can suggests we simply need to give the algorithms more iterations to figure out the best values.\n\nInevitably, the details are more complex than I can convey in two paragraphs but the general idea is there. If either number looks a bit worrying, it's relatively cheap and easy to just rerun the model with more iterations and see if that solves the problem.\n\nLet's do just that by increasing the number of iterations to 3000 per chain:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # We're using four chains/algorithms\n  n.chains = 4,\n  # We allow 3000 guesses (increased from 2000)\n  n.samples = 3000,\n  # We ignore the first 300 (increased from 200)\n  # We ignore them because we assume the algorithms are not particularly reliable\n  # in the first ca. 10% of guesses\n  n.burn = 300,\n  verbose = FALSE)\n```\n:::\n\n\n\nOnce run, we can check how well they ran:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 3000, verbose = FALSE, n.burn = 300, n.chains = 4)\n\nSamples per Chain: 3000\nBurn-in: 300\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 10800\nRun Time (min): 0.0243\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0655 1.0246  0.1977  2.0138 4.2114 1.0010 529\ntree         0.3721 1.3145 -2.0763  0.3028 3.0319 1.0662 375\ntemp        -1.2063 1.1214 -3.3214 -1.2363 1.1050 1.0220 422\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8530 0.2644 -2.3777 -1.8521 -1.3301 1.0020 1914\nrain         0.1715 0.2210 -0.2506  0.1682  0.6072 1.0004 4390\n```\n\n\n:::\n:::\n\n\n\nNow when we look at the `Rhat` and `ESS` values, we doing better. Still not perfect but enough for me to be happy that *by these two metrics alone* there's nothing to suggest the machinery is struggling and that the parameters are being estimated as robustly as possible.\n\n> Importantly, these are not the only metrics or tools available to us. We'll check out the other options in a later document.\n\nAs far as we can tell, for now, we can trust this model and move on to plotting the predicted relationships.\n\n# Plot predicted relationships\n\nPreviously, we made the predictions ourselves by \"hand\". This time, we're going to use the `predict()` function to do this for us to make life easier on ourselves. Importantly, the process is exactly the same; \n\n1. Create a \"fake\" dataset with the covariate values we want to predict for.\n\n2. Apply this fake data to our equation, with the now estimated parameter values, to generate the predictions.\n\n3. Convert from logit values to probabilities.\n\n4. Plot the predicted probabilities against the fake covariate values to show the estimated relationship.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX.0 <- cbind(\n  1,\n  temp = seq(from = min(occ.covs$temp), \n             to = max(occ.covs$temp),\n             length.out = 50),\n  tree = 0\n)\n\n#predict(fit, X.0)\n```\n:::\n\n\n\n# How should I store my data?\n\nIn order to be able to run the model, we need the data to be in a particular order\n",
>>>>>>> d548521aa3beeabf225108329da82b7c8eb947ac
    "supporting": [
      "OccMods_WithCovariates_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}