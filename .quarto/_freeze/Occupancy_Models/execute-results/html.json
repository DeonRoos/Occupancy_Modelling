{
  "hash": "3a9bdd6a8165e227f9fb8f95dc07e6e4",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Occupancy Models\"\nauthor: \"Deon Roos\"\ndate: \"2025-04-29\"\nformat:\n  html:\n    theme: flatly\n    highlight-style: monochrome\n    code-fold: true\n    toc: true\n    toc-depth: 2\n    toc-location: right\n---\n\n\n\n\n# Where are species present and why are they present?\n\nA fundamental question in ecology is: where are species present, and why? For example, why are elephants present in the north of Etosha nature reserve in Namibia, but not in the south of the reserve? Is it due to water availability? Food? Could predators play a role?\n\nThe best tool available for answering these types of questions is occupancy modeling, originally developed by Daryl MacKenzie et al in [2002](https://doi.org/10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2). This modelling framework built on concepts already developed for estimating survival of *individual* animals, called the Cormack-Jolly-Seber model (note that Cormack and Jolly independently developed this model *while they worked in Aberdeen University* - a vital and internationally renowned model was developed right here in bloody Aberdeen!)\n\nThe problem that occupancy models solve is subtle but insanely crucial.\n\nAssume I want to determine where elephants are present in Etosha nature reserve in Namibia. I do a bunch of surveys where I go to various sites throughout the park. Whenever I spot an elephant I note down that elephants are present at that location. When I don't see any elephants I also record that. My dataset might look something like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1988)\netosha <- data.frame(\n  site = 1:10,\n  elephants = rbinom(n = 10, size = 1, prob = 0.4)\n)\netosha\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   site elephants\n1     1         0\n2     2         0\n3     3         0\n4     4         0\n5     5         0\n6     6         0\n7     7         0\n8     8         1\n9     9         1\n10   10         0\n```\n:::\n:::\n\n\nWhen `elephants` is `1`, then I saw them at that site, and when `elephants` is `0`, then I didn't see them.\n\nTo figure out the probability that a site is occupied, I can run a Bernoulli Generalised Linear Model. The Bernoulli distribution (named after [Jacob Bernoulli](https://en.wikipedia.org/wiki/Jacob_Bernoulli)) is a type of distribution that will generate (or expect) values of `1` or `0`; perfect here because our data can only be `1` or `0`.\n\nThe model would be:\n\n$$\ny_i \\sim Bernoulli(p_i) \\\\\n$$\n\n$$\nlogit(p_i) = \\beta_0\n$$\n\nwhere:\n\n* $y$ is our observation (`elephants` in the `etosha` dataset)\n\n* $i$ is the index, here being which `site` the data was collected from\n\n* $\\sim$ means \"generated according to\" (or \"our data is the same as would be generated by the following distribution\")\n\n* $Bernoulli$ is a type of distribution that will generate either 0 or 1\n\n* $p$ is the probability of success (i.e. there is $p$ probability that we see an elephant). We can't possibly know what $p$ is when we collect the data, so we need to figure it out with statistics. (The $i$ means each site *could* have a different probability - but we're not doing that yet)\n\n* $logit$ is the link function to ensure that $p$ remains between 0% and 100%. Specifically, it's a little bit of maths: $log(\\frac{p}{1-p})$, which is the natural log of the probability to succeed ($p$) divided by the probability to fail ($1-p$).\n\n* $\\beta_0$ is the intercept which here, given we have nothing else in this part of the model, means the average probability to see an elephant.\n\nHere's how we'd run that model in `R` (click the `Show` button to see the code):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- glm(elephants ~ 1,\n           data = etosha,\n           family = binomial)\n```\n:::\n\n\nWhich returns the estimate of $\\beta_0$ (or average probability to detect elephants *on the link function scale*, i.e. it's a logit value):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = elephants ~ 1, family = binomial, data = etosha)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  -1.3863     0.7906  -1.754   0.0795 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10.008  on 9  degrees of freedom\nResidual deviance: 10.008  on 9  degrees of freedom\nAIC: 12.008\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nThe estimate for `(Intercept)` (or $\\beta_0$) is `-1.3863`, which we can convert into a probability by doing the inverse logit (`R` has a nice way to do this using the function `plogis()`):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplogis(-1.3863)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1999991\n```\n:::\n:::\n\n\nWe now have our estimate as a proportion (which we can multiply by 100 to get to percentage), so we can say there's a roughly 20% chance that an elephant occupies a site in Etosha.\n\nOr can we?\n\n# Imperfect detection\n\nThis 20% estimate is actually the product of two probabilities. For us to detect an elephant two things need to happen. \n\n1. The elephant must obviously be there, with some probability. We can call this probability $\\psi$ (called \"psi\", pronounced like \"sigh\").\n\n2. I have to *see* the elephant. This will never have a 100% chance no matter how good I am at spotting elephants. We can call this probability $p$.\n\nSo all of the `1`'s in our `etosha` dataset are the result of succeeding in both of these probabilities. However, all of the `0`s are the result of failing either probability; either there were no elephants ($1-\\psi$), or there were you just didn't see them ($\\psi \\times 1-p$). Fundamentally, the problem is that `elephants` represents $\\psi \\times p$, and not just $\\psi$ like we want.\n\nThere's no shortage of reasons why you might not see an elephant despite it being there. It can be as simple and dumb as the elephant was behind a bush when I checked that site. This reflects something called \"imperfect detection\", i.e. just cause it's there doesn't mean we're going to see it.\n\nImperfect detection is not an issue for the `1`'s in our data - when we see an elephant. If you see an elephant, you know it's there. That's easy and obvious. The complication created by imperfect detections happens when we __*don't*__ detect an elephant; when `elephants` is `0`. What does that mean? Are elephants actually absent from that site or are they present but I didn't see them?\n\nThat's the problem! The `0`'s have multiple meanings. It's not as simple as \"if I don't see an elephant, there are not elephants there\".\n\nGenerally, when people say they ran an SDM (a \"Species Distribution Model\"), what they *mean* is, they ran something akin to the GLM above. Specifically, they would treat the `elephants` column in our `etosha` dataset as being a faithful measure of elephants being either present or absent, and not elephants are present __*and detected*__ or just absent.\n\nThey ignore imperfect detection; they ignore the fact that `0`'s have multiple meanings.\n\nOccupancy models do not ignore this, and that's why they're such a powerful tool. Here's how they do it.\n\n# Occupancy models\n\nThe first part of the occupancy model (which I'll call the \"state model\" because it's trying to determine the \"state\" of a site - either occupied or not) looks remarkably similar to the GLM above:\n\n$$\nz_i \\sim Bernoulli(\\psi_i)\\\\\nlogit(\\psi_i) = \\beta_0\n$$\n\nThey look similar, because they're both $Bernoulli$ GLMs! But they differ in two important ways,\n\n* $z$ is the **true** presence or absence of elephants in site $i$\n\n* $\\psi$ is the probability to be present\n\nOk, so the labels have changed, but how does that magically solve the problem of imperfect detection? Well, if we left it there it wouldn't be solved. We need something that will deal with impefect detection; a second GLM.\n\nThe second GLM (which I'll call the \"observation model\") also looks remarkably similar:\n\n$$\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\nlogit(p_{i,j}) = \\alpha_0\n$$\n\nIt's yet another $Bernoulli$ GLM but with some really important changes.\n\n* $y$ is now the **detection** or not of an elephant\n\n* $j$ is *survey*, which means we have *multiple surveys*, not just one like in our first GLM example above\n\n* $p$ is the probability to detect an elephant at site $i$ in survey $j$ (e.g. what is the probability to detect an elephant in site 1 in the third survey?)\n\nImportantly, the probability to detect elephants ($p$) is *multiplied* by $z$. What's $z$? Well that's the true occupancy state of that site from the first GLM. It's this multiplication that allows the two models to \"speak\" to each other.\n\nIf there are elephants in a site ($z = 1$), then $p \\times 1 = p$. If elephants are absent from a site ($z = 0$), then $p \\times 0 = 0$. This means you cannot detect elephants if they aren't there. That's blindingly obvious... But this stupidly simple logic is missing from our starting GLM!\n\nKeep in mind that we don't know $z$ - that's the \"true\" occupancy state of a site. It's something called a \"latent variable\", meaning we never actually measure or record this in the field. Instead, much like with parameters, we use the data we have collected to estimate this variable for each site that we have data from.\n\nThat's how occupancy model knows that you can only detect elephants if they're present, and if they aren't present then you can't see them! That's the beauty of occupancy models and why they are far, far superior to traditional SDMs!\n\n# The robust design and closure\n\nOk, great, we've got a clever modelling framework but how does the above help us resolve if we go to a site and don't see an elephant? How do we distinguish between a \"true negative\" (i.e. we don't see elephants because there aren't any elephants) versus a \"false negative\" (i.e. we don't see elephants but they were there)?\n\nThis is where the subscript $j$ in the observation model becomes important. Imagine we visit a site once and we see no elephants. With no additional information you have absolutely no way to determine if it was occupied or not. You just know you didn't see any elephants.\n\nBut imagine I went back to that same site the next day. This time I do see elephants. We learn a few things from this. First, we know elephants are present at the site. Secondly, we learn that the first survey must have been a false negative - we just didn't see elephants despite them being there.\n\nThis introduces a key assumption in occupancy models. For my above statement to be \"true\" and make sense, then I have to assume elephants were *always* present, and they didn't just happen to move into the site on the second day.\n\nThis assumption is called the \"assumption of closure\". This is often poorly understood, where people interpret it as meaning the site is physically \"closed\" - as in there is some kind of \"fence\" that prevents animals from moving. This is not correct. \"Closure\" here, means \"demographically closed\", i.e. if the species was present in survey one, then it is present in survey two and survey three, survey four, and so on, until you stop surveying.\n\n> This imposes two important considerations for field work. We need to survey sites on multiple occasions. Meaning, we can't have a \"one-and-done\" approach to surveying; we need to leave cameras in situ for more than one sampling period (though it's for us to define what our sampling period is - it could be an hour, a day, a week - so long as we can assume that if the species is present at the first survey, then it will be present at the last survey). Secondly, these sampling occasions cannot extend over such a long period of time that it becomes increasingly hard, or unreasonable, to assume the species could not have gone locally extinct or (re)colonised the area. \n\nIn practice, a minimum of three survey occasions is required (e.g., three days, three hours, or three defined sampling periods). But additionally, you don't want to monitor the same site for so long the species could go locally extinct (obviously dependent on the species you're working with). If such local extinctions and recolonisations are likely at a site, then you can extend from the single season occupancy models (the type of occupancy model that we're talking about here) into multi-season occupancy models.\n\nThis form of sampling (repeatedly sampling the same site a minimum of three times) is called \"the robust design\" and is very often used in more \"robust\" (i.e. responsible or trustworthy) research.\n\n# The data\n\nI'm going to simulate data for an occupancy model to give you an idea of how you data should be organised. To do so, I'll use the `R` package that you'll eventually use in your own analysis, `spOccupancy` (short for spatial occupancy - we'll get to the spatial part later).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spOccupancy)\nset.seed(1234)\ndat <- simOcc(J.x = 5, J.y = 2, n.rep = rep(3, times = 10), beta = c(1), alpha = c(0))\nobs <- dat$y\n```\n:::\n\n\nOur dataset will look slightly different. Since the robust design requires monitoring each site multiple times (typically three or more), we cannot use a single column for detections as we did in the `etosha` example. Instead, here would data will look like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1] [,2] [,3]\n [1,]    0    1    0\n [2,]    0    0    1\n [3,]    0    0    0\n [4,]    0    0    0\n [5,]    0    1    1\n [6,]    1    1    0\n [7,]    1    0    1\n [8,]    0    0    0\n [9,]    0    1    0\n[10,]    0    1    1\n```\n:::\n:::\n\n\nEach row is one of our ten sites, and each column is one of our three sampling periods. Notice anything? In site 3, or `[3,]`, and sites 4 and 8, we never see elephants. Without any further information, we don't know if this is because elephants weren't in those sites, or they were present but we didn't see. Same problem as before.\n\nHowever, we clearly have sites where elephants are present. In sites 1, 2, 5, 6, 7, 9, and 10, we see elephants at least once, so we know for sure that they are present there. But! We can also use these detections to help us figure out what's likely going on in those sites where we never saw elephants.\n\nTo see how that works, let's focus on just site 5, or `[5,]`. Over the three days that we surveyed, we didn't see any elephants on day 1, we then saw them on day 2, then again on day 3. Are elephants present at this site? Yes, clearly. We see them at least once but there's additional information we learn from having used a robust design (surveying the same site multiple times).\n\nIgnore occupancy models for a moment. For the detection history of site 5 (not detected, detected, detected, or `011`), we might guess that our detection probability is about 66% given we have two detections out of three surveys. If so, then we can think of this as being the equivalent to us having a 66% chance of detecting elephants on any survey (importantly, if they're present).\n\nArmed with this 66%, we can figure out how likely it is that elephants were actually absent from site 3 and 4. Let's assume elephants were present at site 3. What's the probability that we would _**fail**_ to detect them? Well, if there's a 66% chance to detect elephants, then failure to detect is $100\\% -66\\% = 34\\%$ (or $1-0.66=0.34$). So a 34% chance not to detect an elephant even if it's there. What about two days in a row? $0.34 \\times 0.34 = 0.12$, or $0.34^2 = 0.12$, - about 12% chance to not detect an elephant two days in a row if it was there. Three days? $0.34^3 = 0.04$, or 4% chance that, if there was an elephant in site 5 that we'd fail to detect it, three days in a row.\n\nGiven a measly 4% chance to completely miss elephants on all three days, it seems more likely that we didn't fail to detect, but that there was no elephant there. Since this is a simulation, we can verify our estimate by checking the true occupancy status of site 3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat$z[3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nWe see that elephants were actually not present in site 5 (above should read `[1] 0` which is absent). Our guess was correct - it was more likely that they weren't there and we _**literally could not**_ see them, rather than they were there and we just got very unlucky.\n\nThat's kind of how occupancy models work, except that rather than using a single site to estimate detection probability, we use all data. Here's how we do the analysis more formally.\n\n# Data preparation\n\nBefore getting to running a fancy * *Bayesian* * model, we need to get our dataset organised in a way that works for `spOccupancy`. Contrary to linear models and GLMs, we don't provide a simple `data.frame` object. Instead, we provide a `list`, which itself can, and eventually will, contain *multiple* data sets.\n\nFor the starting model we'll fit below, we only need our detection history \"matrix\" to be added to this list (which I'll call `etosha`). That detection history matrix is the data we saw above (shown again below). So for now, we're basically taking our data set and adding it to a list. A bit of a pain, given it's a seemingly unnecessary step, but it's needed for the more complex versions of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat$y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1] [,2] [,3]\n [1,]    0    1    0\n [2,]    0    0    1\n [3,]    0    0    0\n [4,]    0    0    0\n [5,]    0    1    1\n [6,]    1    1    0\n [7,]    1    0    1\n [8,]    0    0    0\n [9,]    0    1    0\n[10,]    0    1    1\n```\n:::\n:::\n\n\nThe code to create the `etosha` list is actually relatively straightforward. The function `list()` will create the list, and we can just add our detection history (currently stored as `dat$y`) and have it be called `y` (for the reason that we call it $y$ in our equations above). If you want to see the code, press the `Show` button to the right.\n\n\n::: {.cell}\n\n```{.r .cell-code}\netosha <- list(\n  y = dat$y\n)\n```\n:::\n\n# Fitting the model\n\nBelow we're going to fit the simplest version of an occupancy model that we can, specifically:\n\n$$\nz_i \\sim Bernoulli(\\psi_i)\\\\\nlogit(\\psi_i) = \\beta_0\\\\\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\nlogit(p_{i,j}) = \\alpha_0\n$$\n\nThis version of the model only has two average probabilities. One for the probability that elephants occupy a site ($\\beta_0$ which is fit on the $logit$ link function), and another to detect elephants if they are present ($\\alpha_0$, also fit on the $logit$ link function).\n\nThe model is not fit using the so-called \"frequentist framework\". Instead, it uses the Bayesian framework. I'll gloss over what these are and what the differences are, other than to say that maybe 95% (for all the NHST lovers) of science uses the frequentist framework. Not by coincidence, it's also the framework that is generally taught to students (hence why most people use it).\n\nThe code below uses the `PGOcc()` function to fit a simple single species, single season occupancy model with no explanatory variables. Click the `Show` button to see the code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- PGOcc(\n  # The state model (i.e. what % that elephants are present?)\n  # ~ 1 means we want an intercept only model (no covariates)\n  occ.formula = ~ 1, \n  # The observation model (i.e. what % that we see elephants if present?)\n  # ~ 1 means the same as above - intercept only\n  det.formula = ~1, \n  # Our carefully formatted dataset\n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n```\n:::\n\n\nHaving fit the model, we can inspect the parameter estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~1, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0027\n\nOccurrence (logit scale): \n              Mean     SD    2.5%    50%  97.5%   Rhat  ESS\n(Intercept) 1.4601 0.9914 -0.1904 1.3426 3.7274 1.0065 1576\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat  ESS\n(Intercept) -0.1952 0.4643 -1.0869 -0.2025 0.7266 1.0039 3274\n```\n:::\n:::\n\n\nThere's a lot of information in this summary but we only really care about a few things.\n\n`Occurrence (logit scale)`\n\n* This is the state model (i.e. what's the probability that an elephant is present in any site?)\n\n* The `(Intercept)` row contains all of the information on the... intercept. Because we only have an intercept, we only have an intercept row.\n\n* `Mean` is the mean of the \"posterior\". Crudely, this is Bayesian terminology for the \"parameter\" (it's a bit more complicated than that, but we're leaving the Bayesian stuff for later).\n\n* `SD` is the standard deviation of the posterior.\n\n* `2.5%` is the lower 95% **credible** interval (this is not a confidence interval but you can think of them as being the same for now).\n\n* `50%` is the median of the posterior.\n\n* `97.5%` is the upper 95% **credible** interval.\n\n* `Rhat` will be entirely new to you. This is a measure of whether or not there were any problems in figuring out the posterior. If the Rhat value is close to 1, then it suggests that it could not find any problems (note this does not mean there are no problems, just that it couldn't find any). If the value goes above, say 1.1, then it suggests the model is not \"convinced\" that it's found the best posterior estimate. In that case, you would need to make some tweaks to the model.\n\n* `ESS` stands for \"Effective Sample Size\". This is another new Bayesian *thing* that I'll gloss over the details of but for now, we want this value to be in the multiple hundreds or thousands. Having \"too low\" an ESS suggests problems that may require tweaks to the model.\n\nIn our case, all looks good. The Rhats are generally very close to 1 and the ESS are all in the thousands.\n\n# Interpreting the results\n\nSo what do our results mean? For the state model (that `spOccupancy` calls `Occurence`), our mean is estimated as 1.4601 but we would surely expect this to be a probability, right? Keep in mind, we're fitting fancy Bernoulli GLMs, and with our Bernoulli GLMs we've used the logit link. So the 1.4601 value is *on the logit* link scale. \n\nWe need to convert this logit value to probabilities ourselves using the same `plogis()` function that we used before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplogis(1.4601)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.811548\n```\n:::\n:::\n\n\nSo we have a roughly 80% chance to have elephants in any of our sites. If you have a look at our actual data set, you'll see we don't have detections in 80% of the sites. We have detections in 70% of the sites. That's where the detection part of the model is helping us out - we don't have 100% detection probability, so we should find that we have more sites occupied than we would expect by default.\n\nWhat was our detection probability? The mean of the intercept is estimated (as a logit value) at -0.1952. We know how to back transform this into probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplogis(-0.1952)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4513544\n```\n:::\n:::\n\n\nThe model has estimated that we have roughly 45% chance of seeing an elephant if it's actually there. If we do have a 45% detection probability, then the chance of not seeing an elephant in sites 3, 4 or 8 is $0.45^3 = 0.09 = 9\\%$. Not huge, but not trivial either.\n\nMost importantly, we learn that detection probability is not 100%. That alone is important. The consequence of that is to show, quite clearly, why the Bernoulli GLM approach we used at the start is not appropriate. Because it ignored imperfect detections, it's unreliable. Our occupancy model is not.\n\n# What next?\n\nThere are three big elements this document has not covered.\n\nThe first is Bayesian statistics. Summarised crudely, Bayesian statistics allows statisticians to include their (or others) understanding of the world into the model. What does this mean specifically? Well, we might have had some idea of what a sensible range of probabilities that elephants were present in any of our sites. Maybe we think 20% to 90% is sensible, i.e. we don't think it's possible that there aren't *any* elephants present or *all* sites are occupied by elephants. These \"guesses\" are formally called \"priors\", and these are the hallmarks of Bayesian statistics.\n\nThe second is including covariates. Just like in the modelling you did in BI3010, we can add anything that we think is playing a role in _**either**_ detection of elephants or why elephants are there. Do you think rainfall influences how likely people are to detect elephants? Well, include rainfall in the detection model. Like I say, in exactly the same way as you did in BI3010!\n\nThe last one is spatial autocorrelation. Imagine one site is elephant utopia. It has everything an elephant could ever want. This site is almost certainly occupied. What about a site right next to it? This next door site is no utopia, so in isolation we would expect a lower occupancy probability, but because it's right next to utopia, it's probably more likely to be occupied than the local features of the site would suggest. This is spatial autocorrelation. The occupancy (or whatever it is we're measuring in space) of one site influences the occupancy of sites depending on how close they are to each other in space. If we don't account for spatial autocorrelation, then we might think there's some feature of the non-utopian elephant site that is more attractive to elephants than it actually is; leading to flawed inference.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}