---
title: "Including Covariates in Occupancy Models"
author: "Deon Roos"
date: "`r Sys.Date()`"
format:
  html:
    theme: flatly
    highlight-style: monochrome
    code-fold: true
    toc: true
    toc-depth: 2
    toc-location: right
---

# The `R` packages

As before, this code uses `spOccupancy`, `ggplot2` and `patchwork` for the analysis and visualisations.

```{r, message = FALSE, warning = FALSE}
library(spOccupancy)
library(ggplot2)
library(patchwork)
```

# The theory

In the previous document we went through a simple example to get a sense of how occupancy models work. We're now going to begin the process of slowly increasing the complexity. To start, we're going to need to include covariates (often called explanatory variables or independent variables or a wide variety of other names - it's a mess).

Let's revisit our simple occupancy model:

$$
z_i \sim Bernoulli(\psi_i)\\
$$

$$
logit(\psi_i) = \beta_0\\
$$

$$
y_{i,j} \sim Bernoulli(p_{i,j} \times z_i)\\
$$

$$
logit(p_{i,j}) = \alpha_0
$$

where the first two formula represent the state model (i.e. are our species present or absent from site $i$), and the last two form the detection model.

What we're going to do in this section is to add in covariates to these equations, starting with the detection model.

The first thing to note is that $y$ is indexed by both $i$ and $j$, where $i$ was the site and $j$ was the survey. That $j$ is important because it allows us to specify covariates that vary not just by site (e.g. site 1 has 20 trees while site 2 has 10 trees) but by survey as well (e.g. on survey 1 in site 1, the temperature was 20 degrees, 15 degrees in survey 2 and 5 degrees in survey 3). This means that anything that was different in one survey to the next can be accounted for and included in the model (so long as we can measure it).

Specifically, these "survey varying covariates" are features that we think may have made us (or whoever or whatever collected the data) to be more or less effective. Using cameras? Well maybe the presence of fog has a big impact on how likely we are to detect elephants. Doing surveys yourself? Well maybe the hour of day that you did the survey had a big impact. The point is, we can include these variables that deal with differences in detection probability.

I'll simulate a new dataset (with a bit more data for us to work with - 64 sites surveyed 3 times each) to show this off. We can say that the detection covariate in this case is rainfall, but note that the values for rainfall will be centred on zero (don't worry about this).

Here's what the data looks like:

```{r}
set.seed(1234)
dat <- simOcc(J.x = 8, 
              J.y = 8, 
              n.rep = rep(3, times = 8 * 8), 
              beta = c(1), 
              alpha = c(-2, 0.5))
obs <- dat$y
det_cov <- dat$X.p[,,2]
df <- data.frame(
  survey = rep(1:3, each = 64),
  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),
  y = c(obs[,1], obs[,2], obs[,3])
)

p1 <- ggplot(df) +
  geom_boxplot(aes(x = factor(survey), y = cov)) +
  geom_jitter(aes(x = factor(survey), y = cov),
              width = 0.2, height = 0) +
  labs(x = "Survey", y = "Rainfall") +
  theme_minimal()

p2 <- ggplot(df) +
  geom_jitter(aes(y = factor(y), x = cov),
              alpha = 0.4, width = 0, height = 0.1) +
  labs(x = "Rainfall", y = "Elephant detection") +
  theme_minimal()

p1 + p2 + plot_annotation(tag_levels = "A", tag_suffix = ")")
```

Notice anything? In **A)** we can see that in each survey, there's a lot of variation across the 64 sites in how much rainfall there was; maybe peaking in survey 2. In **B)**, it looks like it might be more likely that we detect elephants more often when there's more rainfall.

Are you sure there's an influence? If so, how strong is it? *Exactly* how strong is it?

There's no way you can answer those questions. That's where we need stats. So let's add in rainfall to our model. I'll recycle the code from the previous document and include rainfall. Here's how we do that.

# Data preparation

Just like in the previous document, we need to include our different datasets into a `list`. If you're still not sure what a `list` is in `R`, think of it like a folder on your computer. You can add lots of different files to a folder, but they're all "tied" together by being within the same folder. That's the same as a `list` in `R`.

Here, I'm going to do something seemingly strange. I'm going to create a `list` and include this into our `etosha` `list`. The reason is that we might have more than one detection covariate, so having a list, though redundant here, will make adding additional variables easier in the future.

```{r}
# Note you wouldn't need to do the dat$X.p[,,2] bit
# That's just because the data is simulated.
det.covs <- list(rain = dat$X.p[,,2])

etosha <- list(
  y = dat$y,
  det.covs = det.covs
)
```

And we're good to go on to the modelling.

# Fitting the model

A few things to note in the `R` code;

* `occ.formula = ~ 1` is the equivalent to $logit(\psi_{i,j}) = \beta_0$

  + I.e. an intercept only occupancy model, meaning we are not including any covariates for the probability that a site is occupied.
  
* `det.formula = ~ rain` is the equivalent to $logit(p_i) = \alpha_0 + \alpha_1 \times Rain_i$

  + I.e. we have both an intercept and slope given we have included rain in the model.
  
* We specify that all data is contained in the list (i.e. "folder") called `etosha`.

* The remainder of the arguments (e.g. `n.chains`) can be ignored for now.

```{r}
fit <- PGOcc(
  # The state model (i.e. what % that elephants are present?)
  # ~ 1 means we want an intercept only model (no covariates)
  occ.formula = ~ 1, 
  # The observation model (i.e. what % that we see elephants if present?)
  det.formula = ~ rain, 
  # Our carefully formatted dataset
  data = etosha, 
  
  # Details to get the machinery to run that we'll ignore for now
  n.chains = 4,
  n.samples = 2000,
  n.burn = 200,
  verbose = FALSE)
```

# Interpreting

Having now fit the model to the data, we can see what we've learnt:

```{r}
summary(fit)
```

Compared to the model in the previous document we now have additional information for `Detection (logit scale)`; we have both an `(Intercept)` and `rain`. These are $\alpha_0$ and $\alpha_1$ from our detection model $logit(p_i) = \alpha_0 + \alpha_1 \times Rain_i$. If we really wanted to, we could now replace the parameter labels (e.g. the $\alpha$s and $\beta$s) with their now estimated values which would look like (rounding the estimates to two decimal points arbitrarily):

$$
z_i \sim Bernoulli(\psi_i)\\$$
$$

$$
logit(\psi_i) = 1.88\\
$$

$$
y_{i,j} \sim Bernoulli(p_{i,j} \times z_i)\\
$$

$$
logit(p_{i,j}) = -1.89 + 0.69 \times Rain_i
$$

With this, we can swap out $Rain$ for any value that we might be interested in, to see how our detection probability changes. For example, let's see what happens when $Rain = 1$.

```{r}
-1.89 + 0.69 * 1
```

Our logit value is -1.2. How do we get that into probabilities that we can actually understand? Backtransform out of logit using `plogis()`:

```{r}
plogis(-1.89 + 0.69 * 1)
```

And we get a ca. 23% chance to detect an elephant when $Rain = 1$. What about when $Rain = 0$? Well, we can do that easily enough now that we know thew general steps:

```{r}
plogis(-1.89 + 0.69 * 0)
```

When $Rain = 0$, we predict a ca. 13% chance to detect elephants.

This approach, whereby we make a prediction for a specific value of $Rain$ can be extended into making multiple predictions at once, such that we can then draw a line through them. Here's how we'd do that.

# Plot predicted relationships

We start by creating a sequence of $Rain$ values, rather than doing one a time. Here we use the `seq()` function to create a sequence, which will range from the minimum $Rain$ value to the maximum within our dataset. The number of values that we want in this sequence is specified as 20 but we can choose any value here - we just need enough that the line is drawn "accurately".

```{r}
rain <- seq(from = min(det.covs$rain),
            to = max(det.covs$rain),
            length.out = 20)

rain
```

Now we have our values, we don't want to manually enter each value into our equation. Instead we can use the fact that `R` works with vectors (i.e. columns of data) to do this quickly and easily:

```{r}
pred <- plogis(-1.89 + 0.69 * rain)
pred
```

Now for each value of rain, we have the predicted probability of detecting an elephant. Useful but you wouldn't want to throw these two columns at your audience/reader and expect them to make sense of it. It'd be better if we include these in a figure.

To do so, we'll combine both columns into a single dataset and plot using `ggplot2`:

```{r}
df <- data.frame(
  pred,
  rain
)

ggplot(df) +
  geom_line(aes(x = rain, y = pred))
```

From this figure it now appears much more intuitive that increasing rain makes elephants easier to detect. We can do a little "tidying" of the figure to make it more visually pleasing:

```{r}
ggplot(df) +
  geom_line(aes(x = rain, y = pred)) +
  scale_y_continuous(labels = scales::percent,
                     limits = c(0,1)) +
  theme_minimal() +
  labs(x = "Mean rainfall",
       y = "Predicted detection\nprobability of elephants")
```

# Uncertainty

The above figure is a good start but we're missing any measure of uncertainty. If we were doing frequentist statistics, we would use 95% confidence intervals but these are exclusively frequentist. There are no 95% confidence intervals when we use the Bayesian statistical framework. Instead we have *credible intervals*. I'll explain the Bayesian framework in a subsequent workflow but for now here is the formal definition of a credible interval:

> There is a 95% probability that the **True** parameter value lies within the interval range, given the data and model.

This is in contrast with the frequentist confidence interval whose formal definition is so bizarre and unintuitive that it's barely useful. 95% *credible intervals* are useful and work exactly the way people *think* frequentist intervals work.

But how do we include these in the figure? Well, the model summary makes it easy to find the values:

```{r}
summary(fit)
```

They're the `2.5%` and `97.5%` values in our summary table. So all we need to do is repeat our predicions using these values to get out measure of uncertainty to include in the figure. Importantly, if you remember back to BI3010 where you had to multiple standard error by 1.96, we *do not* need to do that here. That's frequentist nonsense - we're Bayesian now.

Let's do just that:

```{r}
df$low <- plogis(-2.4610 + 0.2328 * df$rain)
df$upp <- plogis(-1.2939 + 1.1801 * df$rain)
df
```

And we can add in our uncertainty using `geom_ribbon()`:

```{r}
ggplot(df) +
  geom_line(aes(x = rain, y = pred)) +
  geom_ribbon(aes(x = rain, ymin = low, ymax = upp),
              alpha = 0.3) +
  scale_y_continuous(labels = scales::percent,
                     limits = c(0,1)) +
  theme_minimal() +
  labs(x = "Mean rainfall",
       y = "Predicted detection\nprobability of elephants")
```

With that, we have a publication ready figure.

# Multiple covariates

Let's increase the complexity a bit and have the simulation include multiple covariates. We'll say that tree height and average temperature affect whether or not a site is occupied (elephants will like tall trees and cooler locations). We'll still have rain affect our detection probability, as above.

To create the figures below we need to do some tweaks to the data. The `df` object I create has one row per survey, with three surveys per site. Our occupancy covariates, `tree` and `temp` have just one value; these do not change from one survey to the next. If a tree is 3 m tall in survey one, then it'll still be 3 m tall in surveys two and three.

A note here is that the covariates are still centered on zero. That's just the way the data is simulated but the data you collect does not need to be the same (so ignore the fact that we will have trees that are -1 m tall - the general idea doesn't change).

```{r}
set.seed(1234)
dat <- simOcc(J.x = 8, 
              J.y = 8, 
              n.rep = rep(3, times = 8 * 8), 
              beta = c(1, -0.2, 0.3), 
              alpha = c(-2, 0.5))
obs <- dat$y
temp <- dat$X[,2]
tree <- dat$X[,3]
det_cov <- dat$X.p[,,2]
df <- data.frame(
  survey = rep(1:3, each = 64),
  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),
  tree = rep(tree, times = 3),
  temp = rep(temp, times = 3),
  y = c(obs[,1], obs[,2], obs[,3])
)

p1 <- ggplot(df) +
  geom_boxplot(aes(x = factor(survey), y = temp)) +
  geom_jitter(aes(x = factor(survey), y = temp),
              width = 0.2, height = 0) +
  labs(x = "Survey", y = "Temperature") +
  theme_minimal()

p2 <- ggplot(df) +
  geom_jitter(aes(y = factor(y), x = temp),
              alpha = 0.4, width = 0, height = 0.1) +
  labs(x = "Temperature", y = "Elephant detection") +
  theme_minimal()

p3 <- ggplot(df) +
  geom_boxplot(aes(x = factor(survey), y = tree)) +
  geom_jitter(aes(x = factor(survey), y = tree),
              width = 0.2, height = 0) +
  labs(x = "Survey", y = "Tree") +
  theme_minimal()

p4 <- ggplot(df) +
  geom_jitter(aes(y = factor(y), x = tree),
              alpha = 0.4, width = 0, height = 0.1) +
  labs(x = "Tree", y = "Elephant detection") +
  theme_minimal()

(p1 + p2) / (p3 + p4)
```

There are two things worth highlighting from the above figures. Firstly, the boxplots on the right are identical for each of the three surveys (the points are jittered so are randomly placed but trust me that these are identical). Secondly, it becomes quite hard to see any clear pattern in the two right hand figures. Keep in mind that the zeros here are actively misleading us. Some of the zeros are genuine in that there were no elephants there and `tree` and `temp` likely caused that (which we only know because the data is simulated) but the other zeros are false negatives; The elephants were there, we just didn't see them.

This is why we need a model to figure out what the relationships are. We can no longer trust our eyes to see the pattern.

So let's get our data organised such that we can fit our model.

# Data preparation

As before, we provide our detection covariates as a list but XXX

```{r}
det.covs <- list(rain = dat$X.p[,,2])
occ.covs <- data.frame(tree = tree, temp = temp)
etosha <- list(
  y = dat$y,
  det.covs = det.covs,
  occ.covs = occ.covs
)
```

# Fitting the model

The core model we're going to fit is:

$$
z_i \sim Bernoulli(\psi_i)\\
logit(\psi_i) = \beta_0 + \beta_1 \times Tree_i + \beta_2 \times Temp_i\\
y_{i,j} \sim Bernoulli(p_{i,j} \times z_i)\\
logit(p_{i,j}) = \alpha_0 + \alpha_1 \times Rain_{i,j}
$$

The main thing that I want to highlight here, other than having included $Tree$ and $Temp$ as effecting occupancy probability ($\psi$), is that the subscripts are different. 

In the occupancy model ($logit(\psi_i) = \beta_0 + \beta_1 \times Tree_i + \beta_2 \times Temp_i$) the variables are subscript by $i$, indicating that we have one value of $Tree$ or $Rain$ for each site $i$.

But in the detection model ($logit(p_{i,j}) = \alpha_0 + \alpha_1 \times Rain_{i,j}$) $Rain$ is subscript by both $i$ and $j$. That's because we have a different $Rain$ value for each survey. This would be something we record every time we visit the site (or extract from some online source for each day).

Keep this in mind when you're collecting your own data. Variables that you think affect occupancy will have one value per site. Variables that you think affect detection will (generally) have one value per survey.

To fit this is relatively straight forward. For the occupancy model, we write `occ.formula = ~ tree + temp`, and for the detection model we write `det.formula = ~ rain`, and let the machinery do it's thing.

```{r}
fit <- PGOcc(
  occ.formula = ~ tree + temp, 
  det.formula = ~ rain, 
  data = etosha, 
  
  # Details to get the machinery to run that we'll ignore for now
  n.chains = 4,
  n.samples = 2000,
  n.burn = 200,
  verbose = FALSE)
```

Once it's run, we can check the `summary()` to see what the parameters were estimated as:

```{r}
summary(fit)
```

We see that `tree` has a positive effect on occupancy and `temp` has a negative effect. This is how the simulation was carried out, but the values do not match up particularly well. Specifically, in the simulation `tree` was set to `0.3` but has been estimated as `0.2`, while `temp` was set to `-0.2` but has been estimated as `-1.2`.

So we've got the general relationships reasonably well estimated but it's not particularly accurate. This is where having *credible intervals* is useful. Keep in mind that credible intervals are, correctly, interpreted as having a 95% chance of containing the "True" value (the True value here is what each parameter was set to in the simulation). For both `tree` and `temp` the True parameter value is indeed within the 95% credible intervals! Both 95% CI are wide, but that's good here! The model isn't entirely sure what the values are (partly because of sample size and the effects being subtle) and that is being conveyed to us! We're not deluding ourselves into thinking we have a perfect understanding when we don't!

There are some other warning signs that the model might not be performing especially well. The first is that the `Rhat` values are getting uncomfortably large for the `Occurence` parameters. My personal rule of thumb is `Rhat` values close to 1.05 is where I get worried. None of our parameters are at 1.05 but they're close enough that I'm paying attention to them and want to see if I can fix the problem.

The other warning sign is the `ESS` for the `Occurence` model are all "low". At least noticeably lower than the `Detection` model parameters. Again, the rule of thumb here is that we want hundreds or thousands of "Effective Sample Size", so we're technically OK but I'm still concerned.

Now, to be clear, we ignored these issues with the first model we ran in this document, we here we're going to see if we can't resolve this problem.

# Resolving issues

To fully appreciate the solution we'll attempt requires a deeper understanding of Bayesian statistics. But for now, we're not going to cover this. Instead, I'll simply say that the "machinery" of Bayesian statistics revolves around giving a number of algorithms (called "chains") a number of guesses (called "iterations") to try and figure out the most likely values for our parameters.

The two metrics we used above, `Rhat` and `ESS`, both monitor these chains and iterations and let us know if they are *not in agreement*. When `Rhat` values get high, and `ESS` values get low, it can suggests we simply need to give the algorithms more iterations to figure out the best values.

Inevitably, the details are more complex than I can convey in two paragraphs but the general idea is there. If either number looks a bit worrying, it's relatively cheap and easy to just rerun the model with more iterations and see if that solves the problem.

Let's do just that by increasing the number of iterations to 3000 per chain:

```{r}
fit <- PGOcc(
  occ.formula = ~ tree + temp, 
  det.formula = ~ rain, 
  data = etosha, 
  
  # We're using four chains/algorithms
  n.chains = 4,
  # We allow 3000 guesses (increased from 2000)
  n.samples = 3000,
  # We ignore the first 300 (increased from 200)
  # We ignore them because we assume the algorithms are not particularly reliable
  # in the first ca. 10% of guesses
  n.burn = 300,
  verbose = FALSE)
```

Once run, we can check how well they ran:

```{r}
summary(fit)
```

Now when we look at the `Rhat` and `ESS` values, we doing better. Still not perfect but enough for me to be happy that *by these two metrics alone* there's nothing to suggest the machinery is struggling and that the parameters are being estimated as robustly as possible.

> Importantly, these are not the only metrics or tools available to us. We'll check out the other options in a later document.

As far as we can tell, for now, we can trust this model and move on to plotting the predicted relationships.

# Plot predicted relationships

Previously, we made the predictions ourselves by "hand". This time, we're going to use the `predict()` function to do this for us to make life easier on ourselves. Importantly, the process is exactly the same; 

1. Create a "fake" dataset with the covariate values we want to predict for.

2. Apply this fake data to our equation, with the now estimated parameter values, to generate the predictions.

3. Convert from logit values to probabilities.

4. Plot the predicted probabilities against the fake covariate values to show the estimated relationship.

```{r}
X.0 <- cbind(
  1,
  temp = seq(from = min(occ.covs$temp), 
             to = max(occ.covs$temp),
             length.out = 50),
  tree = 0
)

#predict(fit, X.0)
```

# How should I store my data?

In order to be able to run the model, we need the data to be in a particular order
