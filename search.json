[
  {
    "objectID": "Bayesian_Occupancy_Models.html",
    "href": "Bayesian_Occupancy_Models.html",
    "title": "Bayesian Occupancy Models",
    "section": "",
    "text": "In the previous page, we went through the underlying theory of Bayesian statistics; how our prior knowledge is updated using real data through Bayes’ Theorem. Now, we’re going to move into the practical application of these ideas using the spOccupancy package. My goal here is to help you understand how Bayesian occupancy models are fit, how to monitor convergence, and how to interpret uncertainty through credible intervals.\nWe’ll again work with the Etosha elephant dataset. Importantly, we are not yet including spatial autocorrelation. We’ll add that in the next section.\n\n\nCode\nlibrary(spOccupancy)\n\n# These are needed to produce the website\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggthemes)\n\n\n\nHow do we estimate a posterior?\nIn the previous page we spoke about posteriors and how these are a combination of our prior belief and our data. But how does that actually happen? Well, the answer is pretty simple but took hundreds of years for statisticians to figure out. Markov Chain Monte Carlo is an algorithm that was developed by scientists at Los Alamos in the 1950s working on the Manhattan Project (yes that Manhattan Project). The algorithm is called Markov Chain Monte Carlo (or MCMC) for two reasons. A Markov Chain describes a sequence of states, where the probability to move from one state to the next state depends only on what the current state is.\nFor example, imagine you are walking through a series of rooms. Where you go next depends only on the room you’re currently in. You can’t instantly teleport to the other side of the building! Nor does it depend on the path you have taken to get to your current room.\n\n\nWhat does MCMC look like?\nBelow is an animation of a simple MCMC algorithm to help give an intuition for what’s happening behind the scenes.\nFor this, we have the following simple linear model:\n\\[y_i \\sim Normal(\\mu_i, \\sigma^2)\\]\n\\[\\mu_i = \\beta_0 + \\beta_1 \\times x_i \\]\nSo our objective here is to figure out \\(\\beta_0\\) and \\(\\beta_1\\). To do so, we’ll set up an MCMC using one “chain”. A chain is a Markov Chain - it’s the black dot and orange line you can see in the animation below. This chain is trying to find the “True value” location marked with a teal X. At each step the black dot asks “if I take a step in a random direction, will I be closer to X or further away?”. If it’s closer, then it takes the step. If it’s further away, then it’s less likely to take the step but it’s not impossible. This seems like a bad choice. Why move somewhere if it’s further away from X? The answer is a bit nuanced, but the concise explanation is that sometimes there might be areas of the “parameter space” (the parameter space is all possible values of \\(\\beta_0\\) and \\(\\beta_1\\)) which “work” quite well despite not being the true value. If we didn’t have the behaviour, where the chain might take a step even though it’s worse, then we might get stuck in one of these “bad” areas (technically, these “bad” areas are called “local minima”).\nHere’s our MCMC in action:\n\n\nCode\ntrue_param &lt;- c(2, -1)\ntarget_density &lt;- function(x, y) {\n  exp(-0.5 * ((x - true_param[1])^2 / 1^2 + (y - true_param[2])^2 / 0.5^2))\n}\n\nset.seed(123)\nn_iter &lt;- 100\nx &lt;- y &lt;- 0\nsamples &lt;- tibble(x = x, y = y, iteration = 1)\n\nfor (i in 2:n_iter) {\n  x_prop &lt;- rnorm(1, x, 0.4)\n  y_prop &lt;- rnorm(1, y, 0.4)\n  accept_ratio &lt;- target_density(x_prop, y_prop) / target_density(x, y)\n  \n  if (runif(1) &lt; accept_ratio) {\n    x &lt;- x_prop\n    y &lt;- y_prop\n  }\n  samples &lt;- samples |&gt; add_row(x = x, y = y, iteration = i)\n}\n\nggplot(samples, aes(x = x, y = y)) +\n  geom_path(color = \"#FF5733\", linewidth = 0.7) +\n  geom_point(aes(x = x, y = y), color = \"black\", size = 2) +\n  geom_point(aes(x = true_param[1], y = true_param[2]), \n             color = \"#00A68A\", size = 3, shape = 4, stroke = 2) +\n  annotate(\"text\", x = true_param[1] + 0.1, y = true_param[2],\n           label = \"True value\", color = \"#00A68A\", hjust = 0) +\n  transition_reveal(iteration) +\n  coord_fixed() +\n  labs(\n    title = \"MCMC Chain Step: {frame_along}\",\n    x = bquote(beta[0]),\n    y = bquote(beta[1])\n  ) +\n  theme_bw()\n\n\n\nHere’s where the really clever bit comes in. At each iteration, if we record the parameter value it tried, and store it, when we build it up the end result is our posterior! This is what made Bayesian statistics possible! We don’t need to use any crazy (and often impossible) maths to figure out the posterior, we just have MCMC walk around and the end result is an insanely good reflection of the posterior!\n\n\nCode\nsamples_long &lt;- samples |&gt;\n  pivot_longer(cols = c(x, y), names_to = \"parameter\", values_to = \"value\")\n\nsamples_long &lt;- samples_long |&gt;\n  mutate(true_value = if_else(parameter == \"x\", true_param[1], true_param[2]))\n\nsamples_long_cumulative &lt;- samples_long |&gt;\n  group_by(parameter) |&gt;\n  group_split() |&gt;\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    map_dfr(1:max(df$iteration), function(i) {\n      df |&gt;\n        filter(iteration &lt;= i) |&gt;\n        mutate(frame = i, parameter = param)\n    })\n  })\n\nsamples_long_cumulative &lt;- samples_long_cumulative %&gt;%\n  mutate(parameter_label = case_when(\n    parameter == \"x\" ~ \"beta[0]\",\n    parameter == \"y\" ~ \"beta[1]\"\n  ))\n\nggplot(samples_long_cumulative, aes(x = value)) +\n  geom_histogram(binwidth = 0.4, fill = \"#FF5733\", color = \"white\", boundary = 0) +\n  geom_vline(aes(xintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", ncol = 1, labeller = label_parsed) +\n  transition_manual(frame) +\n  labs(title = \"Cumulative Posterior up to Iteration {current_frame}\",\n       x = \"Parameter Value\", y = \"Frequency\") +\n  theme_bw()\n\n\n\nAnother way to show what the MCMC was up to is using something called “traceplots”. These are relatively simple but actually quite powerful for determining if we trust the model output. We’ll come back to this in a bit, but for now, we can show the MCMC exploring the parameter space using these traceplots.\n\n\nCode\nsamples_long_cumulative_trace &lt;- samples_long |&gt;\n  group_by(parameter) |&gt;\n  group_split() |&gt;\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    map_dfr(1:max(df$iteration), function(i) {\n      df |&gt;\n        filter(iteration &lt;= i) |&gt;\n        mutate(frame = i, parameter = param)\n    })\n  })\n\nsamples_long_cumulative_trace &lt;- samples_long_cumulative_trace %&gt;%\n  mutate(parameter_label = case_when(\n    parameter == \"x\" ~ \"beta[0]\",\n    parameter == \"y\" ~ \"beta[1]\"\n  ))\n\nggplot(samples_long_cumulative_trace, aes(x = iteration, y = value)) +\n  geom_line(color = \"#FF5733\", linewidth = 0.8) +\n  geom_hline(aes(yintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", ncol = 1, labeller = label_parsed) +\n  transition_manual(frame) +\n  labs(title = \"Traceplot up to Iteration {current_frame}\",\n       x = \"Iteration\", y = \"Parameter Value\") +\n  theme_bw()\n\n\n\n\n\nImproving our MCMC\nWe’ve made a good start but we can improve this quite a bit. Firstly, limiting the algorithm to 100 iterations doesn’t give it many opportunities to find the true value. In general, you often give MCMC thousands of iterations, rather than a paltry 100. So first improvement is to increase the number of iterations (you may remember we did this when we were going through the occupancy theory pages - now you know why!).\nSecondly, we’re using one MCMC chain. Why not more? Afterall, if we have say four chains, then if all four agree that they’re close to the true value that would give us more comfort. If they find different “True values”, well, then it seems likely that we haven’t actually found it.\nLet’s implement our improvement and see what our plots now look like:\n\n\nCode\ntrue_param &lt;- c(2, -1)\n\ntarget_density &lt;- function(x, y) {\n  exp(-0.5 * ((x - true_param[1])^2 / 1^2 + (y - true_param[2])^2 / 0.5^2))\n}\n\nset.seed(123)\nn_iter &lt;- 1000\nn_chains &lt;- 4\n\nchains_list &lt;- map_dfr(1:n_chains, function(chain_id) {\n  x &lt;- y &lt;- 0  # Start at (0, 0)\n  samples &lt;- tibble(x = x, y = y, iteration = 1, chain = chain_id)\n  \n  for (i in 2:n_iter) {\n    x_prop &lt;- rnorm(1, x, 0.4)\n    y_prop &lt;- rnorm(1, y, 0.4)\n    accept_ratio &lt;- target_density(x_prop, y_prop) / target_density(x, y)\n    \n    if (runif(1) &lt; accept_ratio) {\n      x &lt;- x_prop\n      y &lt;- y_prop\n    }\n    \n    samples &lt;- samples %&gt;% add_row(x = x, y = y, iteration = i, chain = chain_id)\n  }\n  samples\n})\n\nggplot(chains_list, aes(x = x, y = y, group = chain, color = as.factor(chain))) +\n  geom_path(linewidth = 0.7) +\n  geom_point(aes(x = x, y = y), size = 1.5) +\n  geom_point(aes(x = true_param[1], y = true_param[2]), \n             color = \"white\", size = 4, shape = 4, stroke = 2, inherit.aes = FALSE) +\n  annotate(\"text\", x = true_param[1] + 0.1, y = true_param[2],\n           label = \"True value\", color = \"white\", hjust = 0) +\n  transition_reveal(along = iteration) +\n  coord_fixed() +\n  scale_color_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"MCMC Chains Step: {round(frame_along, digits = 0)}\",\n    x = bquote(beta[0]),\n    y = bquote(beta[1])) +\n  theme_bw()\n\n\n\n\n\nCode\nsamples_long &lt;- chains_list %&gt;%\n  pivot_longer(cols = c(x, y), names_to = \"parameter\", values_to = \"value\") %&gt;%\n  mutate(true_value = if_else(parameter == \"x\", true_param[1], true_param[2]),\n         parameter_label = case_when(\n           parameter == \"x\" ~ \"beta[0]\",\n           parameter == \"y\" ~ \"beta[1]\"\n         ))\n\n# Build cumulative data\nsamples_long_cumulative &lt;- samples_long %&gt;%\n  group_by(parameter, chain) %&gt;%\n  group_split() %&gt;%\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    chain_id &lt;- unique(df$chain)\n    map_dfr(1:max(df$iteration), function(i) {\n      df %&gt;%\n        filter(iteration &lt;= i) %&gt;%\n        mutate(frame = i, parameter = param, chain = chain_id)\n    })\n  })\n\nggplot(samples_long_cumulative, aes(x = value, fill = as.factor(chain))) +\n  geom_histogram(binwidth = 0.4, color = \"white\", boundary = 0, \n                 position = position_dodge(), alpha = 0.6) +\n  geom_vline(aes(xintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", labeller = label_parsed, ncol = 1) +\n  transition_manual(frame) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"Cumulative Posterior up to Iteration {current_frame}\",\n       x = \"Parameter Value\", y = \"Frequency\") +\n  theme_minimal()\n\nggplot(samples_long_cumulative, aes(x = iteration, y = value, color = as.factor(chain))) +\n  geom_line(linewidth = 0.7) +\n  geom_hline(aes(yintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_y\", labeller = label_parsed, ncol = 1) +\n  transition_manual(frame) +\n  scale_color_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"Traceplot up to Iteration {current_frame}\",\n       x = \"Iteration\", y = \"Parameter Value\") +\n  theme_minimal()\n\n\n\n\nI have a bit more confidence in our posteriors now:\n\nAll chains seem to agree on the same value (they have “converged” to the same answer)\nThe posteriors look reasonably well estimated (they’ll close to the true value - but keep in mind that with real analysis we don’t know what the truth is)\nThe traceplots resemble “hairy caterpillars”, which matches the Markov Chain idea that the next value depends only on the current value (it doesn’t “remember” older values).\n\nThe final thing I want to bring your attention to are the first hundred or so iterations. Generally, these iterations can be pretty wild, fluctuating massively. If you think about it, that’s kind of fair enough. We’re starting each MCMC chain at random locations, so it’s fair if things are a bit wobbly at the start. So what we do with these early iterations is to simply ignore them. These are called “burn in” iterations, as in, we’re letting the engine warm up, so these are just used to get up to speed. Mostly arbitrarily, we typically ignore the first 10% of iterations, so where we’ve used 1000 iterations, we’d generally ignore the first 100.\n\n\nFitting the occupancy model using MCMC\nLet’s now revisit our Bayesian model. As a reminder, here’s what it looked like:\n\\[z_i \\sim Bernoulli(\\psi_i)\\\\\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\nWhich we can translate into the following code. But pay attention to the n.chains, n.samples and n.burn arguments. This is where we specify how many chains we want (n.chains), how many iterations we want (n.samples, note an iteration can also be called a sample), and how many of the first iterations we want to ignore (n.burn).\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  n.chains = 4,     # 4 chains just like in our simple example\n  n.samples = 2000, # 2000 iterations for each chain\n  n.burn = 200,     # We ignore the first 200 iterations to give MCMC a chance to get it's feet\n  \n  verbose = FALSE   # This just says don't spit out details while fitting \n  # (normally I would leave verbose = TRUE so I can keep track of the model while it's fitting)\n  )\n\n\nWe can now check how our model worked. The code is pretty simple. The only tricky thing is to specify if you want beta or alpha. Importantly, beta here refers to the occupancy parameters, and alpha refers to the detection parameters.\nSo here are the traceplots and posteriors for \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) in:\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\n\nCode\nplot(fit, 'beta') # Occupancy parameters.\n\n\n\n\n\nAnd here are the traceplots and posteriors for \\(\\alpha_0\\) and \\(\\alpha_1\\) in:\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\n\n\nCode\nplot(fit, 'alpha') # Detection parameters.\n\n\n\n\n\nAnd we can also get a more numeric summary of the model using summary(). Importantly, the information you see below is the same as the data in the figures above. It’s just summarised! So the Mean is just the mean of the posteriors above!\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0188\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0536 1.0657  0.1991  1.9727 4.2706 1.0151 315\ntree         0.2259 1.2968 -2.1384  0.1544 2.9006 1.0797 278\ntemp        -1.1362 1.1440 -3.2916 -1.1758 1.3257 1.0396 272\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8539 0.2623 -2.3783 -1.8533 -1.3435 1.0018 1142\nrain         0.1730 0.2207 -0.2558  0.1719  0.6087 1.0015 2994\n\n\n\n\nCredible intervals\nNotice the 2.5% and 97.5% in the tables above? These are the credible intervals that I briefly mentioned in the Occupancy Models: Covariates page. Technically, these are just the quantiles of the posterior. Or, phrased alternatively, 95% of all iterations are within this interval. And keep in mind, that a Bayesian credible interval is not the same as a frequentist confidence interval. For our purposes, these intervals represent a 95% probability to contain the true value (based on the data we collected and the model we fit)!\n\n\nCode\nbeta_0_samples &lt;- fit$beta.samples[,1]\n\nbeta_0_df &lt;- tibble(beta_0 = beta_0_samples)\n\nsummary_stats &lt;- beta_0_df %&gt;%\n  summarise(\n    mean = mean(beta_0),\n    median = median(beta_0),\n    lower = quantile(beta_0, 0.025),\n    upper = quantile(beta_0, 0.975)\n  )\n\nggplot(beta_0_df, aes(x = beta_0)) +\n  geom_histogram(fill = \"#00A68A\", colour = \"white\", alpha = 0.5, bins = 50) +\n  \n  # Vertical lines\n  geom_vline(xintercept = summary_stats$mean, linetype = \"solid\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$median, linetype = \"dashed\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$lower, linetype = \"dotted\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$upper, linetype = \"dotted\", color = \"black\", linewidth = 1) +\n\n  # curved arrows\n  geom_curve(aes(x = summary_stats$mean + 0.5, y = 400, \n                 xend = summary_stats$mean, yend = 350), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = 0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$median - 0.5, y = 300, \n                 xend = summary_stats$median, yend = 250), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = -0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$lower - 0.5, y = 100, \n                 xend = summary_stats$lower, yend = 75), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = -0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$upper + 0.5, y = 100, \n                 xend = summary_stats$upper, yend = 75), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = 0.3, color = \"black\") +\n\n  annotate(\"text\", x = summary_stats$mean + 0.5, y = 400, label = \"Mean\", hjust = 0, size = 5) +\n  annotate(\"text\", x = summary_stats$median - 0.5, y = 300, label = \"Median\", hjust = 1, size = 5) +\n  annotate(\"text\", x = summary_stats$lower - 0.5, y = 100, label = \"2.5% CI\", hjust = 1, size = 5) +\n  annotate(\"text\", x = summary_stats$upper + 0.5, y = 100, label = \"97.5% CI\", hjust = 0, size = 5) +\n\n  labs(\n    x = expression(paste(beta[0], \" or '(Intercept)'\")),\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nFrom our summary table, the mean for the \\(\\beta_0\\) or (Intercept) posterior was estimated as 2.0536, the median (called 50% in the summary table) was 1.9727, and the 95% credible intervals were 0.1991 and 4.2706, just like we see in the figure.\nThis might seem trivial but having credible intervals be so simple, and the interpretation having a useful meaning is one of the big selling points of Bayesian analysis in my mind. It’s not something to trivialise.\n\n\nSpecifying priors\nSo far, we’ve let the spOccupancy package handle setting the priors for us behind the scenes. But we can actually specify our own priors if we want to.\nIn the background, spOccupancy uses something called Pólya-Gamma data augmentation, and under the hood, this method assumes \\(Normal\\) priors for both the occupancy and detection parameters (including their intercepts).\nIf you don’t specify anything, spOccupancy will set:\n\nThe “hypermean” (mean of \\(Normal\\) distribution for all priors) = 0\nThe “hypervariance” (variance \\(Normal\\) distribution for all priors) = 2.72\n\nThis corresponds to a relatively flat prior on the probability scale (the 0–1 scale for occupancy or detection probabilities). In other words, by default, the prior doesn’t strongly pull your estimates toward any particular value; it lets the data mostly speak for itself.\nBut we can set these explicitly if we want to! Here’s what that looks like:\n\n\nCode\n# Specify priors for detection (alpha) and occupancy (beta)\npriors &lt;- list(\n  alpha.normal = list(mean = 0, var = 2.72),  # Detection priors\n  beta.normal = list(mean = 0, var = 2.72)    # Occupancy priors\n)\n\n\nThe above code is basically doing what spOccupancy does by default. All priors will get \\(Normal(0,2.72)\\) in both the detection and occupancy sub models. We can either give a single number (like 0 and 2.72 above), and it will apply to all parameters, or you can give a vector if you want different priors for different parameters.\nHere’s how we’d give different priors for each parameter:\n\n\nCode\n# Different priors for each occupancy parameter\noccurrence_priors &lt;- list(\n  mean = c(0, 0.5, -0.5),    # One mean for each occupancy parameter\n  var = c(2.72, 1, 2)        # One variance for each occupancy parameter\n)\n\n# Different priors for each detection parameter\ndetection_priors &lt;- list(\n  mean = c(0, 0.2),          # One mean for each detection parameter\n  var = c(0.5, 1)           # One variance for each detection parameter\n)\n\n# Combine into the priors list\npriors &lt;- list(\n  beta.normal = occurrence_priors,\n  alpha.normal = detection_priors\n)\n\n\nThis would translate into this model:\n\\[z_i \\sim Bernoulli(\\psi_i)\\\\\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[\\beta_0 \\sim Normal(0, 2.72)\\]\n\\[\\beta_1 \\sim Normal(0.5, 1)\\]\n\\[\\beta_2 \\sim Normal(-0.5, 2)\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\n\\[\\alpha_0 \\sim Normal(0, 0.5)\\]\n\\[\\alpha_1 \\sim Normal(0.2, 1)\\]\nNow, to be very, very clear; I am choosing these priors completely at random just for demonstration. I have no reason, in this case, to think that a reasonable prior for \\(\\beta_1\\) is \\(Normal(0.5, 1)\\).\nBut we can always fit this model to see what happens:\n\n\nCode\nfit_priors &lt;- PGOcc(\n  occ.formula = ~ tree + temp,\n  det.formula = ~ rain,\n  data = etosha,\n  priors = priors,     # We add our priors here\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE\n)\n\n\nHaving fit, we can see what the posteriors look like:\n\n\nCode\nplot(fit_priors, 'beta') # Occupancy parameters.\n\n\n\n\n\nCode\nplot(fit_priors, 'alpha') # Detection parameters.\n\n\n\n\n\nAnd pull up the summary:\n\n\nCode\nsummary(fit_priors)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    priors = priors, n.samples = 2000, verbose = FALSE, n.burn = 200, \n    n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0165\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.5053 0.9644 -0.2024  1.4383 3.5766 1.0514 304\ntree         0.5140 0.9042 -1.2174  0.4798 2.3414 1.0165 412\ntemp        -1.2473 0.9496 -3.0085 -1.2664 0.7709 1.0097 365\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.6451 0.2634 -2.1407 -1.6515 -1.0959 1.0112 1004\nrain         0.1595 0.2133 -0.2483  0.1565  0.6006 1.0048 3385\n\n\nIf we compare this with the original model, where we left the priors at their defaults we can see that different priors can lead to different posteriors. Keep in mind how Bayesian statistics work - the posterior is a combination of our data and prior beliefs. We’ve changed our prior belief and our posteriors have changed as a result.\nNow in this case, the difference in posteriors are pretty minimal. It might not always be. And in truth, we would want to think a bit more carefully about the priors. Don’t be fooled here. Just because my randomly chosen priors are different from the default ones doesn’t mean the default model is necessarily better. The best option is to choose priors you think are reasonable. If you have different prior beliefs (e.g. I have some informative priors but also some uninformative priors) I can run the model with both, and see how much of a difference it makes. This is something called “prior sensitivity analysis” and is a fairly useful tool to have in your back pocket when you’re not sure about your priors.\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0188\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0536 1.0657  0.1991  1.9727 4.2706 1.0151 315\ntree         0.2259 1.2968 -2.1384  0.1544 2.9006 1.0797 278\ntemp        -1.1362 1.1440 -3.2916 -1.1758 1.3257 1.0396 272\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8539 0.2623 -2.3783 -1.8533 -1.3435 1.0018 1142\nrain         0.1730 0.2207 -0.2558  0.1719  0.6087 1.0015 2994\n\n\n\n\nFin\nWith that, you should be good to run a Bayesian occupancy model. Something you can add to your CV and have employers fawn over you. Even better if you understand it, so that when they ask you about it you can have a conversation!\nThe only part left in the analysis is including “spatial autocorrelation”. We’ll cover that in the next page.\n\n\nA subtle point: Priors and the link function\nWhen we choose priors in a Bayesian model, it’s really important to remember what scale those priors live on.\nIn our occupancy model, we specify priors for the parameters on the logit scale, not directly on the probability (0–1) scale.\nFor example, if we write:\n\\[logit(p_i) = \\beta_0 + \\beta_1 \\times x_i\\]\nthen \\(\\beta_0\\) and \\(\\beta_1\\) are in logit space.\nRemember, the logit function stretches the 0–1 probability scale onto the whole real line:\n\nProbabilities near 0.5 correspond to logits near 0.\nProbabilities near 0 or 1 correspond to logits of \\(-\\infty\\) and \\(+\\infty\\).\n\nThis means that a \\(Normal\\) prior with mean 0 and large variance on the logit scale is not flat on the probability scale! Even “uninformative” \\(Normal\\) priors on the logit scale can actually imply very strong beliefs on the probability scale.\nTo build some intuition, we’ll do a little prior predictive simulation:\n\nWe’ll randomly draw values for \\(\\beta_0\\) and \\(\\beta_1\\) from a \\(Normal(0, 2.72)\\) prior.\nWe’ll simulate the relationship between \\(x\\) and \\(p(x)\\) by plugging those \\(\\beta_0\\) and \\(\\beta_1\\) values into the logit equation.\nWe’ll repeat this 100 times to show many possible relationships.\n\n\n\nCode\nset.seed(123)\n\nn_draws &lt;- 100\nx_seq &lt;- seq(-3, 3, length.out = 100)\n\nbeta_0_draws &lt;- rnorm(n_draws, mean = 0, sd = sqrt(2.72))\nbeta_1_draws &lt;- rnorm(n_draws, mean = 0, sd = sqrt(2.72))\n\nprior_simulations &lt;- map2_dfr(\n  beta_0_draws, beta_1_draws,\n  .f = function(b0, b1) {\n    tibble(\n      x = x_seq,\n      logit_p = b0 + b1 * x,\n      p = plogis(logit_p)\n    )\n  },\n  .id = \"draw\"\n)\n# Plot\nggplot(prior_simulations, aes(x = x, y = p, group = draw)) +\n  geom_line(alpha = 0.2, color = \"#FF5733\") +\n  theme_minimal() +\n  labs(\n    title = \"Prior Predictive Simulation\",\n    x = \"Covariate (x)\",\n    y = \"Probability (p)\"\n  )\n\n\n\n\n\nEach orange line is a possible relationship between \\(x\\) and \\(p(x)\\) given the priors we chose. Notice that some lines are almost flat at 0 or 1? While others are very steep, flipping from 0 to 1 over a narrow range of \\(x\\)? Even though the prior on \\(\\beta_0\\) and \\(\\beta_1\\) was centered at 0 with large variance, the resulting priors on \\(p\\) are not uniform or “neutral.”\nIf I were being hyper cautious, I might be worried these priors are pushing the model towards the extreme flipping behaviour. In some cases that might be good, in others it might be bad.\nThe broader points I am making here are:\n\nDon’t stress too much about priors. If you have a lot of data your prior will often not be terribly important.\nBut give a little thought as to what a sensible prior would be, especially when working with link functions.\n\nIf you’re in doubt speak with me! I think having a discussion about your priors would be an excellent use of one of our meetings (hint, hint)."
  },
  {
    "objectID": "Bayesian_Statistics.html",
    "href": "Bayesian_Statistics.html",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "",
    "text": "Most of the statistics you’ve probably learned so far has been based in what’s called frequentist statistics. The analysis you’ll be using for occupancy models (at least this particular implementation) will be using something called Bayesian statistics (pronounced “Bay-zee-’n”). These are the two statistical frameworks currently used to gather evidence, to answer a question, from data (unless you want to start splitting philosophical hairs). Within this document we’re going to start with what Bayesian statistics is, how to move from frequentist to Bayesian statistics, and how a Bayesian model works. We’ll leave occupancy models to the side for the time being.\nGiven you probably didn’t know you were even using a statistical framework before this (I didn’t really mention this in BI3010), it’s worth going over what frequentist statistics is, before I explain the alternative.\nAlmost every quantitative scientist understands statistics under the frequentist framework. That’s not because it’s philosophically better or has a cleaner underlying logic or that scientists have developed an understanding of both frequentist and Bayesian frameworks, carefully considered which is more appropriate for their purposes and made a principled decision to use frequentist statistics. It’s because that’s what they were taught. They were taught frequentist stats and that’s what they now use. As simple as that. Across the quantitative sciences, this has led to frequentist methods becoming the default framework."
  },
  {
    "objectID": "Bayesian_Statistics.html#your-prior-belief",
    "href": "Bayesian_Statistics.html#your-prior-belief",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "Your Prior Belief",
    "text": "Your Prior Belief\nYou start off thinking you have a 50% chance of passing. That doesn’t mean you’re certain that it’s exactly 50%. You might be unsure, so you’re open to the idea that it could be lower or higher.\nYour prior might look like this:\n\n\nCode\ntheta &lt;- seq(0, 1, length.out = 1000)\n\nprior &lt;- dbeta(theta, 20, 20)\n\ndata.frame(theta, prior) |&gt;\n  ggplot(aes(x = theta, y = prior)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Chance of Passing\", y = \"Belief Density\") +\n  theme_minimal()"
  },
  {
    "objectID": "Bayesian_Statistics.html#the-likelihood",
    "href": "Bayesian_Statistics.html#the-likelihood",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "The Likelihood",
    "text": "The Likelihood\nNow you find out that 100 students have done their thesis before you, and all of them passed. That’s really strong evidence that the pass rate is high, probably very near to 100%.\nWe can visualise how likely the observed data is for different values of the pass rate, this is the likelihood.\n\n\nCode\nlikelihood &lt;- theta^100\nlikelihood &lt;- likelihood / max(likelihood)\n\ndata.frame(theta, likelihood) |&gt;\n  ggplot(aes(x = theta, y = likelihood)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Chance of Passing\", y = \"Likelihood (scaled)\") +\n  theme_minimal()\n\n\n\n\n\nIn Bayesian terms, this evidence updates your belief."
  },
  {
    "objectID": "Bayesian_Statistics.html#the-posterior",
    "href": "Bayesian_Statistics.html#the-posterior",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "The Posterior",
    "text": "The Posterior\nYour updated belief, the posterior, combines your initial guess of roughly 50% with the strong evidence we have from 100 students. The resulting curve now shifts to the right; it’s taller near 85%-90% but still not at 100%, because you haven’t thrown out your original uncertainty.\n\n\nCode\nposterior &lt;- dbeta(theta, 120, 20)\n\nposterior_df &lt;- data.frame(\n  theta = rep(theta, 3),\n  density = c(prior, likelihood, posterior),\n  belief = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(theta))\n)\n\nposterior_df |&gt;\n  ggplot(aes(x = theta, y = density, color = belief)) +\n  geom_line(size = 1.2) +\n  labs(x = \"Chance of Passing\", y = \"Belief / Likelihood (scaled)\", colour = \"\") +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"darkgreen\", \"firebrick\", \"steelblue\"))\n\n\n\n\n\nYour posterior is more-or-less the multiplication of your prior belief with the likelihood (\\(P(Data | Parameters)\\times P(Parameters)\\)). In this case, giving you a happy middle ground of ca. 85% chance to pass.\nThis is how Bayesian statistics works in practice:\n\nStart with a belief (or hypothesis).\nSee some evidence (i.e. collect some data).\nUpdate your belief accordingly.\n\nIn short, this is kinda the scientific method, isn’t it? (pls don’t tell any frequentists that I said that…)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to your site",
    "section": "",
    "text": "This site was made for you both and introduces the fundamentals and advanced topics in occupancy modelling that you’ll need to do your honours project. I’ve tried to write the material to be accessible for you both, in terms of your honours projects, but also to help boost your skillset for when you finish you undergraduate.\nAs always, if there is anything in the site that is either wrong or confusing, just let me know. If it helps, I would be amazed if you understood everything that’s covered, so it’s my prior belief that parts will need to be explained in person.\n\n\nTo get the most out of this material, I recommend the following order:\n\nOccupancy Models: The basics\nLearn the basic theory of occupancy models and how they help us infer species presence or absence from imperfect detection.\nOccupancy Models: Covariates\nLearn how site and survey covariates can improve our biological understanding and enhance the accuracy of our inferences.\nBayesian models: The concept\nLearn the basic theory of the Bayesian statistical framework: what’s a prior, and what’s a posterior?\nFitting Bayesian Models\nCombine occupancy models with the Bayesian framework to fit models using the spOccupancy package.\nTo be added – Spatial autocorrelation\nLearn the basic theory behind spatial autocorrelation.\nTo be added – Occupancy models with spatial autocorrelation\nCombine occupancy models, the Bayesian framework, and spatial autocorrelation to fit more advanced models that account for spatial structure.\n\nBe aware that this site is very much an on going work in progress. Let me know if you spot anything that needs some work.\nEnjoy!"
  },
  {
    "objectID": "index.html#suggested-order",
    "href": "index.html#suggested-order",
    "title": "Welcome to your site",
    "section": "",
    "text": "To get the most out of this material, I recommend the following order:\n\nOccupancy Models: The basics\nLearn the basic theory of occupancy models and how they help us infer species presence or absence from imperfect detection.\nOccupancy Models: Covariates\nLearn how site and survey covariates can improve our biological understanding and enhance the accuracy of our inferences.\nBayesian models: The concept\nLearn the basic theory of the Bayesian statistical framework: what’s a prior, and what’s a posterior?\nFitting Bayesian Models\nCombine occupancy models with the Bayesian framework to fit models using the spOccupancy package.\nTo be added – Spatial autocorrelation\nLearn the basic theory behind spatial autocorrelation.\nTo be added – Occupancy models with spatial autocorrelation\nCombine occupancy models, the Bayesian framework, and spatial autocorrelation to fit more advanced models that account for spatial structure.\n\nBe aware that this site is very much an on going work in progress. Let me know if you spot anything that needs some work.\nEnjoy!"
  },
  {
    "objectID": "OccMods_WithCovariates.html",
    "href": "OccMods_WithCovariates.html",
    "title": "Including Covariates in Occupancy Models",
    "section": "",
    "text": "On this page, we’ll start expanding on the intercept-only occupancy model from Occupancy Models: The basics by injecting some biology into our etosha elephant model.\nAs before, we’ll use spOccupancy to fit the model. For visualisation, we’ll also use ggplot2 and patchwork (which lets you combine multiple ggplots into one).\nCode\nlibrary(spOccupancy)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "OccMods_WithCovariates.html#data-preparation",
    "href": "OccMods_WithCovariates.html#data-preparation",
    "title": "Including Covariates in Occupancy Models",
    "section": "Data preparation",
    "text": "Data preparation\nJust like on the previous page, we need to include our datasets in a list. If you’re still unsure what a list is in R, think of it like a folder on your computer. You can add lots of different files to a folder—any type of file—and they’re all “tied together” by being in the same place. A list in R works the same way.\nThe complication now is that I have two datasets. One for my detection history matrix and another for the rain covariate. To include both, I’m going to do something that might seem a little strange at first. I’ll create a list for the detection covariate and then include that list inside our main etosha list. That gives us a list within a list.\nWhy? Because we might want to add more than one detection covariate later. Even though it feels redundant now, this structure makes it easier to expand the model in the future.\n\n\nCode\n# Note you wouldn't need to do the dat$X.p[,,2] bit\n# That's just because the data is simulated.\ndet.covs &lt;- list(rain = dat$X.p[,,2])\n\netosha &lt;- list(\n  y = dat$y,\n  det.covs = det.covs\n)\n\n\nAnd with that, we’re ready to move on to modelling."
  },
  {
    "objectID": "OccMods_WithCovariates.html#fitting-the-model",
    "href": "OccMods_WithCovariates.html#fitting-the-model",
    "title": "Including Covariates in Occupancy Models",
    "section": "Fitting the model",
    "text": "Fitting the model\nHere are a few things to note about the model specification:\n\nocc.formula = ~ 1 corresponds to the equation\n\\[\n\\text{logit}(\\psi_i) = \\beta_0\n\\]\nThis is an intercept-only occupancy model, meaning we are not including any covariates for the probability that a site is occupied.\ndet.formula = ~ rain corresponds to the equation\n\\[\n\\text{logit}(p_i) = \\alpha_0 + \\alpha_1 \\times \\text{Rain}_i\n\\]\nThis means we are modelling detection probability as a function of rainfall. The model includes both an intercept (\\(\\alpha_0\\)) and a slope for rain (\\(\\alpha_1\\)).\nWe specify that all the data needed for the model is contained in the list called etosha (think of it like a folder that holds everything in one place).\nThe remaining arguments (such as n.chains) control how the Bayesian model is run. You can ignore these for now.\n\n\n\nCode\nfit &lt;- PGOcc(\n  # The state model (i.e. what % that elephants are present?)\n  # ~ 1 means we want an intercept only model (no covariates)\n  occ.formula = ~ 1, \n  # The observation model (i.e. what % that we see elephants if present?)\n  det.formula = ~ rain, \n  # Our carefully formatted dataset\n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)"
  },
  {
    "objectID": "OccMods_WithCovariates.html#interpreting",
    "href": "OccMods_WithCovariates.html#interpreting",
    "title": "Including Covariates in Occupancy Models",
    "section": "Interpreting",
    "text": "Interpreting\nNow that we’ve fit the model to the data, we can see what we’ve learned.\nCompared to the model on the previous page, we now have additional output under Detection (logit scale). Specifically, we have estimates for both (Intercept) and rain. These correspond to \\(\\alpha_0\\) and \\(\\alpha_1\\) in our detection model:\n\\[\n\\text{logit}(p_i) = \\alpha_0 + \\alpha_1 \\times \\text{Rain}_i\n\\]\nIf we want, we can substitute in the estimated values (rounded to two decimal places) and rewrite the model like this:\n\\[\nz_i \\sim \\text{Bernoulli}(\\psi_i) \\tag{State Stochastic}\n\\]\n\\[\n\\text{logit}(\\psi_i) = 1.88 \\tag{State Deterministic}\n\\]\n\\[\ny_{i,j} \\sim \\text{Bernoulli}(p_{i,j} \\times z_i) \\tag{Observation Stochastic}\n\\]\n\\[\n\\text{logit}(p_{i,j}) = -1.89 + 0.69 \\times \\text{Rain}_i \\tag{Observation Deterministic}\n\\]\nThis final equation lets us calculate detection probabilities for different rainfall values. For example, when \\(\\text{Rain} = 1\\) (which corresponds to heavy rainfall in this dataset), we get:\n\\[\n\\text{logit}(p) = -1.89 + 0.69 \\times 1 = -1.2\n\\]\nTo convert this logit value back to a probability, we apply the inverse logit (also called the logistic function):\n\\[\np = \\frac{1}{1 + \\exp(-(-1.20))} = \\frac{1}{1 + \\exp(1.20)} = 0.23 = 23\\%\n\\]\nOr we could just do plogis(-1.89 + 0.69 * 1).\nWhen \\(Rain = 0\\) (moderate rainfall), the logit becomes -1.89, which converts to a detection probability of about 13%.\nThis process, whereby we plug in a covariate value to make a prediction, is something we can repeat for many values of rain. Doing so allows us to build the predicted relationship that shows how detection probability changes with rainfall.\nNext, we’ll create that prediction curve and visualise the effect of rainfall on detection."
  },
  {
    "objectID": "OccMods_WithCovariates.html#plot-predicted-relationships",
    "href": "OccMods_WithCovariates.html#plot-predicted-relationships",
    "title": "Including Covariates in Occupancy Models",
    "section": "Plot predicted relationships",
    "text": "Plot predicted relationships\nTo visualise the relationship between rainfall and detection probability, we first create a sequence of Rain values. Rather than calculating one at a time as we just did, we’ll use the seq() function to generate a smooth range of values between the minimum and maximum rainfall in our dataset. Here, we use 20 points to draw a smooth line, but you could choose any number depending on the level of detail you want.\n\n\nCode\nrain &lt;- seq(from = min(det.covs$rain),\n            to = max(det.covs$rain),\n            length.out = 20)\n\nrain\n\n\n [1] -2.71815687 -2.44127881 -2.16440076 -1.88752271 -1.61064465 -1.33376660\n [7] -1.05688855 -0.78001049 -0.50313244 -0.22625439  0.05062367  0.32750172\n[13]  0.60437977  0.88125783  1.15813588  1.43501393  1.71189199  1.98877004\n[19]  2.26564809  2.54252615\n\n\nNow that we have our sequence of rainfall values, we want to calculate the predicted detection probability for each one. Because R works with vectors (columns of values), we can apply our model equation to the entire sequence in one step.\n\n\nCode\npred &lt;- plogis(-1.89 + 0.69 * rain)\npred\n\n\n [1] 0.02263134 0.02726568 0.03281714 0.03945308 0.04736516 0.05677017\n [7] 0.06790956 0.08104689 0.09646267 0.11444547 0.13527876 0.15922259\n[13] 0.18649040 0.21722152 0.25145143 0.28908330 0.32986526 0.37337882\n[19] 0.41904309 0.46613767\n\n\nThis gives us a predicted detection probability for each rainfall value. That’s useful, but a raw table of numbers wouldn’t be very effective in a thesis or presentation. It’s far clearer to visualise this relationship in a figure.\nTo do that, we combine the rainfall values and their corresponding predicted probabilities into a new dataset and then plot the relationship using ggplot2.\n\n\nCode\ndf &lt;- data.frame(\n  pred,\n  rain\n)\n\nggplot(df) +\n  geom_line(aes(x = rain, y = pred))\n\n\n\n\n\nThe resulting figure makes it much easier to interpret the model. We can now clearly see that detection probability increases with rainfall.\nWe can also tidy up the plot to make it more visually effective, by doing relatively small things like adjusting axis labels, converting probabilities to percentages, and applying a clean theme.\n\n\nCode\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n\n\n\n\n\n\nBiological interpretation\nFrom the plot, it looks like elephants are more likely to be detected when it rains. If this were real data, we might start discussing ecological explanations. But here’s a key point:\nWe should have thought about this before fitting the model.\nInterpreting results after seeing them and retroactively inventing explanations is risky. It’s called HARKing; Hypothesising After the Results are Known and it’s a form of scientific misconduct. It leads to misleading conclusions and reduces the credibility of research. Don’t do it. Even if it’s just for your own integrity.\nHere, the data are simulated, so the stakes are low. The goal is to understand the modelling process. But in real research, it’s essential to define your question, hypotheses and state your predictions before doing the analysis. This starts going down the road of pre-registration but I won’t touch on that here.\nSo while visualising and interpreting your model is valuable, doing so responsibly is just as important."
  },
  {
    "objectID": "OccMods_WithCovariates.html#uncertainty",
    "href": "OccMods_WithCovariates.html#uncertainty",
    "title": "Including Covariates in Occupancy Models",
    "section": "Uncertainty",
    "text": "Uncertainty\nThe figure we created is a good start, but it’s missing something important: uncertainty.\nIf we were using frequentist statistics, we’d include 95% confidence intervals. But in the Bayesian framework, confidence intervals don’t exist. Instead, we use credible intervals.\nHere’s the formal definition of a 95% credible interval:\n\nThere is a 95% probability that the true parameter value lies within the interval range, given the data and model.\n\nThis contrasts with frequentist confidence intervals, whose definition is more convoluted and often misunderstood. In fact, credible intervals behave exactly the way most people think confidence intervals do. I’ll explain the Bayesian framework more fully in a later section but for now, just trust me that credible intervals are more intuitive and arguably more useful.\n\nHow do we show credible intervals on our figure?\nThe model summary includes everything we need. The 2.5% and 97.5% columns from the summary output give us the lower and upper bounds of the credible interval for each parameter.\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.018\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n\n\nTo incorporate these into our plot, we repeat the prediction process we used earlier but this time, we use the lower and upper parameter estimates to calculate the corresponding detection probabilities. These will form the bounds of our uncertainty ribbon.\nOne important note: you do not need to multiply the standard error by 1.96 (as we did in BI3010). That approach belongs to the frequentist framework. With Bayesian models, we work directly with the posterior distributions and their quantiles, no extra calculation needed.\nLet’s do just that:\n\n\nCode\ndf$low &lt;- plogis(-2.4610 + 0.2328 * df$rain)\ndf$upp &lt;- plogis(-1.2939 + 1.1801 * df$rain)\ndf\n\n\n         pred        rain        low        upp\n1  0.02263134 -2.71815687 0.04336427 0.01096960\n2  0.02726568 -2.44127881 0.04611831 0.01514457\n3  0.03281714 -2.16440076 0.04903828 0.02087495\n4  0.03945308 -1.88752271 0.05213304 0.02871039\n5  0.04736516 -1.61064465 0.05541172 0.03936862\n6  0.05677017 -1.33376660 0.05888379 0.05376451\n7  0.06790956 -1.05688855 0.06255900 0.07302436\n8  0.08104689 -0.78001049 0.06644741 0.09846565\n9  0.09646267 -0.50313244 0.07055932 0.13151304\n10 0.11444547 -0.22625439 0.07490526 0.17351714\n11 0.13527876  0.05062367 0.07949599 0.22545433\n12 0.15922259  0.32750172 0.08434241 0.28752905\n13 0.18649040  0.60437977 0.08945559 0.35877811\n14 0.21722152  0.88125783 0.09484663 0.43685701\n15 0.25145143  1.15813588 0.10052670 0.51819600\n16 0.28908330  1.43501393 0.10650691 0.59858193\n17 0.32986526  1.71189199 0.11279825 0.67399363\n18 0.37337882  1.98877004 0.11941156 0.74135968\n19 0.41904309  2.26564809 0.12635738 0.79895748\n20 0.46613767  2.54252615 0.13364590 0.84638633\n\n\nOnce we’ve calculated the lower and upper bounds, we can visualise the uncertainty using geom_ribbon() in ggplot2. This shaded area gives a clear, intuitive representation of the uncertainty around our predictions.\n\n\nCode\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  geom_ribbon(aes(x = rain, ymin = low, ymax = upp),\n              alpha = 0.3) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n\n\n\n\n\nThe result is a publication- or thesis-ready figure: a smooth line showing the predicted detection probability as rainfall increases, surrounded by a shaded band representing the credible interval."
  },
  {
    "objectID": "OccMods_WithCovariates.html#data-preparation-1",
    "href": "OccMods_WithCovariates.html#data-preparation-1",
    "title": "Including Covariates in Occupancy Models",
    "section": "Data preparation",
    "text": "Data preparation\nAs before, we include our survey covariates as a list, while site covariates can be supplied as a data.frame. Both are then combined in another list, which is what we pass to the model.\n\n\nCode\n# Survey covariate\ndet.covs &lt;- list(rain = dat$X.p[,,2])\n# Site covariates\nocc.covs &lt;- data.frame(tree = tree, temp = temp)\n# Combine into a single list called etosha\netosha &lt;- list(\n  y = dat$y, # Detection history\n  det.covs = det.covs,\n  occ.covs = occ.covs\n)"
  },
  {
    "objectID": "OccMods_WithCovariates.html#fitting-the-model-1",
    "href": "OccMods_WithCovariates.html#fitting-the-model-1",
    "title": "Including Covariates in Occupancy Models",
    "section": "Fitting the model",
    "text": "Fitting the model\nThe core model we’re fitting now includes two site covariates and one survey covariate:\n\\[z_i \\sim Bernoulli(\\psi_i)\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\nWhat’s important to notice here, aside from the inclusion of Tree and Temp in the state model, is the use of subscripts.\nIn the state model, the covariates are subscripted by \\(i\\), meaning each site has one value for Tree and Temp. In the observation model, Rain is subscripted by both \\(i\\) and \\(j\\), meaning it can vary between sites and surveys. This reflects how you’d record rainfall each time you visit a site.\nKeep this in mind when collecting your own data:\n\nOccupancy covariates usually vary by site, not by survey.\nDetection covariates typically vary by survey (i.e. time).\n\nThat’s the general rule, but if you’re unsure or have an unusual situation, ask me.\nTo fit this model, we specify the occupancy formula as ~ tree + temp and the detection formula as ~ rain. The rest of the model setup (e.g. number of chains, samples, etc.) stays the same.\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)"
  },
  {
    "objectID": "OccMods_WithCovariates.html#interpreting-the-results",
    "href": "OccMods_WithCovariates.html#interpreting-the-results",
    "title": "Including Covariates in Occupancy Models",
    "section": "Interpreting the results",
    "text": "Interpreting the results\nOnce the model has run, we can inspect the estimates using summary().\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0182\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.9515 1.0600  0.1862  1.8333 4.2716 1.0441 347\ntree         0.3649 1.2506 -2.0020  0.3180 2.8503 1.0383 293\ntemp        -1.1005 1.1484 -3.2573 -1.1617 1.3032 1.0452 277\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8428 0.2636 -2.3606 -1.8424 -1.3199 1.0023 1404\nrain         0.1730 0.2214 -0.2499  0.1707  0.6115 1.0023 2983\n\n\nWe see that tree has a positive effect on occupancy (estimated at 0.3649), and temp has a negative effect (-1.1005). These estimates are in the general direction of the true values used in the simulation, though they’re not perfect:\n\ntree was simulated at 0.3 and estimated at 0.37 (very close)\ntemp was simulated at -0.2 but estimated at -1.1 (more of a difference)\n\nWhile the estimated relationship for temp is “correct” in that it gets the direction right (it’s a negative effect), it’s kind of far away from the true value that was specified in the simulation. That’s where credible intervals become useful. Remember, a 95% credible interval means there’s a 95% chance the true parameter lies within that range. In our case, the true values for both tree and temp do fall within their respective 95% credible intervals. This is good, it tells us the model reflects its uncertainty accurately, which is especially important with small sample sizes or subtle effects."
  },
  {
    "objectID": "OccMods_WithCovariates.html#diagnosing-potential-issues",
    "href": "OccMods_WithCovariates.html#diagnosing-potential-issues",
    "title": "Including Covariates in Occupancy Models",
    "section": "Diagnosing potential issues",
    "text": "Diagnosing potential issues\nThere are two early warning signs that suggest this model might not be performing optimally:\n\nRhat values\nThe Rhat values for the Occurrence parameters are getting close to 1.05. While they haven’t crossed the danger threshold, they’re high enough to warrant attention. My personal rule of thumb is to start worrying around 1.05.\nEffective Sample Size (ESS)\nThe ESS for the Occurrence parameters is noticeably lower than for the detection parameters. While we still have several hundred samples, which is generally acceptable, it’s another flag that suggests the model might benefit from refinement.\n\nIn our first model earlier on the previous page (the intercept only occupancy model), we ignored these diagnostics (in part because there were no issues). This time, we’ll see if we can address them to improve model performance."
  },
  {
    "objectID": "OccMods_WithCovariates.html#resolving-issues",
    "href": "OccMods_WithCovariates.html#resolving-issues",
    "title": "Including Covariates in Occupancy Models",
    "section": "Resolving issues",
    "text": "Resolving issues\nTo fully understand how to resolve issues like high Rhat or low ESS values requires a deeper dive into Bayesian statistics. We’ll save that for a later section. For now, here’s a simplified explanation:\nThe Bayesian framework relies on algorithms, called chains, that take many iterations (or guesses) to estimate the most likely values of the model parameters.\nThe two metrics we use to assess how well this process is working are:\n\nRhat: Tells us whether the different chains agree with each other (or have converged to more-or-less the same answer). Higher values suggest poor convergence.\nESS (Effective Sample Size): Indicates how well the parameter was estimated. Lower values suggest greater uncertainty or inefficiency.\n\nIf Rhat values are high or ESS values are low, one common fix is to increase the number of iterations. This gives the algorithms more opportunities to find consistent and stable parameter estimates.\nSo let’s do just that: we increase the total number of iterations to 3000 per chain, and slightly increase the number of burn-in iterations (guesses that we ignore from when the algorithm starts) to account for that change.\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # We're using four chains/algorithms\n  n.chains = 4,\n  # We allow 3000 guesses (increased from 2000)\n  n.samples = 3000,\n  # We ignore the first 300 (increased from 200)\n  # We ignore them because we assume the algorithms are not particularly reliable\n  # in the first ca. 10% of guesses\n  n.burn = 300,\n  verbose = FALSE)\n\n\nAfter running the model again, we can check the summary output. If all goes well, we hope to see:\n\nRhat values closer to 1\nHigher ESS values for the occupancy parameters\n\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 3000, verbose = FALSE, n.burn = 300, n.chains = 4)\n\nSamples per Chain: 3000\nBurn-in: 300\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 10800\nRun Time (min): 0.0262\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.9586 1.0113  0.1496  1.8785 4.0979 1.0061 533\ntree         0.3562 1.2971 -2.0341  0.3101 2.9698 1.0498 415\ntemp        -1.2475 1.0628 -3.3183 -1.2672 0.9406 1.0176 470\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8461 0.2664 -2.3631 -1.8497 -1.3123 1.0014 1625\nrain         0.1791 0.2196 -0.2462  0.1772  0.6243 1.0016 4411\n\n\nNow when we look at the Rhat and ESS values, we are doing better. Still not perfect but good enough for me to be happy that by these two metrics alone there’s nothing to suggest the machinery is struggling and that the parameters are being estimated as robustly as possible.\nThis won’t always fix every problem, but in many cases it’s a simple and effective first step.\n\nNote: Rhat and ESS are not the only diagnostics we can use. In a later section, we’ll explore other ways to assess how well our model is performing.\n\nFor now, based on these two metrics, we’re happy that the model has converged and is estimating parameters robustly. We can now move on to plotting the predicted relationships."
  },
  {
    "objectID": "OccMods_WithCovariates.html#plot-predicted-relationships-1",
    "href": "OccMods_WithCovariates.html#plot-predicted-relationships-1",
    "title": "Including Covariates in Occupancy Models",
    "section": "Plot predicted relationships",
    "text": "Plot predicted relationships\nPreviously, we generated predictions by hand using parameter estimates and “manual” calculations. This time, we’ll (kind of) simplify things by using the predict() function, which does most of the heavy lifting for us. The overall process is still the same:\n\nCreate a “fake” dataset with covariate values for which we want predictions.\nApply the model to these values to generate predicted responses.\nConvert logit values to probabilities.\nPlot the predicted probabilities against covariate values to show the relationship.\n\nRemember, occupancy models are essentially two connected Bernoulli GLMs. When using predict(), we must specify which part of the model we’re predicting from: occupancy or detection.\nNormally, when plotting the effect of one covariate (e.g. tree), we hold the other covariates constant—typically at their median values, just like you did in BI3010.\nHowever, for these models there are a few extra (annoying) things we need to do and for me to explain here. As a warning some of these get quite technical. It might be a good time to take a break and come back when you’re fresh.\nHowever, with spOccupancy, there are a few additional steps involved. Some are a bit technical, so if you’re feeling tired, it might be a good point to pause and return later. If you’re confused by anything below, don’t worry, we can talk it through in person.\n\nModel matrix\nTo make predictions, we need to pass a design matrix to the predict() function. This is done using model.matrix(). It turns your covariate data (like tree and temp) into the format expected by the model, where each column represents a variable, and each row represents a combination of covariate values.\nWhen we fit a model using ~ tree + temp, R internally builds a model matrix. When predicting, we must do the same so the structure matches.\n\n\nqlogis and plogis\nThe predict() function returns thousands of posterior draws for each site’s occupancy or detection probability. These are already on the probability scale (i.e. backtransformed from the logit scale).\nHowever, there’s a subtle issue here. If we summarise these probability values directly (e.g. take their mean or credible intervals), the result can sometimes be misleading, especially if the underlying uncertainty is skewed. The shape of the probability distribution can become distorted and give odd patterns (like artificial peaks or dips).\nTo avoid this, when working with spOccupancy, we need to:\n\nTransform probabilities back to the logit scale using qlogis().\nSummarise the posterior draws on the logit scale (mean and credible intervals).\nTransform the summaries back to probabilities using plogis().\n\nThis avoids distortion and produces smoother, more reliable estimates.\n\n\napply()\nThe posterior samples are structured with one row per iteration (i.e. per guess) and one column per site. To summarise across iterations for each site, we use apply() with MARGIN = 2 (i.e. apply the function across columns). We calculate the mean, lower (2.5%), and upper (97.5%) quantiles for each.\nHonestly, the code is more complex than I would like. I wish I could think of a simpler way to do it (and I might ask my wife, who’s better at coding than me, to see if she can improve it) but this is the best I can do for now. Speak to me if you need help.\n\n\nPlotting predictions\nWe can now generate prediction curves for each covariate in our model:\n\nTree height (site-level covariate affecting occupancy): Create a range of values for tree, hold temp at its median, predict, summarise, and plot.\nTemperature (another site-level covariate): Hold tree at its median and repeat the steps above for temp.\nRainfall (survey-level covariate affecting detection): Generate a sequence of rainfall values and repeat the prediction and summarisation steps for detection probability.\n\n\n\nCode\n# Tree height ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n# Step 1: Create a new fake data frame for tree (hold temp at median)\nfake &lt;- data.frame(\n  tree = seq(min(etosha$occ.covs$tree), max(etosha$occ.covs$tree), length.out = 100),\n  temp = median(etosha$occ.covs$temp)\n)\n\n# Step 2: Convert this into a model matrix\nX.0 &lt;- model.matrix(~ tree + temp, data = fake)\n\n# Step 3: Predict occupancy from model matrix\npred_occ &lt;- predict(fit, X.0 = X.0, type = \"occupancy\")\n\n# Step 4: Transform back into logit values\nlogit_psi_samples &lt;- qlogis(pred_occ$psi.0.samples)\n\n# Step 5: Summarize the posteriors\nfake &lt;- data.frame(\n  tree = fake$tree,\n  mean.psi = apply(logit_psi_samples, 2, mean),\n  lower = apply(logit_psi_samples, 2, function(x) quantile(x, 0.025)),\n  upper = apply(logit_psi_samples, 2, function(x) quantile(x, 0.975))\n)\n\n# Step 6: Backtransform into probabilities\nfake &lt;- data.frame(\n  tree = fake$tree,\n  mean.psi = plogis(fake$mean.psi),\n  lower = plogis(fake$lower),\n  upper = plogis(fake$upper)\n)\n\n# Step 7: Plot\np1 &lt;- ggplot(fake, aes(x = tree, y = mean.psi)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  labs(x = \"Tree height\", y = \"Occupancy probability (ψ)\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n# Mean temperature ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n\n# Step 1: Create a new data frame for temp (hold tree at median)\nfake &lt;- data.frame(\n  tree = median(etosha$occ.covs$tree),\n  temp = seq(min(etosha$occ.covs$temp), max(etosha$occ.covs$temp), length.out = 100)\n)\n\n# Step 2: Convert this into a model matrix\nX.0 &lt;- model.matrix(~ tree + temp, data = fake)\n\n# Step 3: Predict occupancy from model matrix\npred_occ &lt;- predict(fit, X.0 = X.0, type = \"occupancy\")\n\n# Step 4: Transform back into logit values\nlogit_psi_samples &lt;- qlogis(pred_occ$psi.0.samples)\n\n# Step 5: Summarize the posteriors\nfake &lt;- data.frame(\n  temp = fake$temp,\n  mean.psi = apply(logit_psi_samples, 2, mean),\n  lower = apply(logit_psi_samples, 2, function(x) quantile(x, 0.025)),\n  upper = apply(logit_psi_samples, 2, function(x) quantile(x, 0.975))\n)\n\n# Step 6: Backtransform into probabilities\nfake &lt;- data.frame(\n  temp = fake$temp,\n  mean.psi = plogis(fake$mean.psi),\n  lower = plogis(fake$lower),\n  upper = plogis(fake$upper)\n)\n\n# Step 5: Plot\np2 &lt;- ggplot(fake, aes(x = temp, y = mean.psi)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  labs(x = \"Temperature\", y = \"Occupancy probability (ψ)\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n# Rain fall ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n\n# Step 1: Create a new data frame for temp (hold tree at median)\nfake &lt;- data.frame(\n  rain = seq(min(etosha$det.covs$rain), max(etosha$det.covs$rain), length.out = 100)\n)\n\n# Step 2: Convert this into a model matrix\nX.0 &lt;- model.matrix(~ rain, data = fake)\n\n# Step 3: Predict occupancy from model matrix\npred_det &lt;- predict(fit, X.0 = X.0, type = \"detection\")\n\n# Step 4: Transform back into logit values\nlogit_p_samples &lt;- qlogis(pred_det$p.0.samples)\n\n# Step 5: Summarize the posteriors\nfake &lt;- data.frame(\n  rain = fake$rain,\n  mean.p = apply(logit_p_samples, 2, mean),\n  lower = apply(logit_p_samples, 2, function(x) quantile(x, 0.025)),\n  upper = apply(logit_p_samples, 2, function(x) quantile(x, 0.975))\n)\n\n# Step 6: Backtransform into probabilities\nfake &lt;- data.frame(\n  rain = fake$rain,\n  mean.p = plogis(fake$mean.p),\n  lower = plogis(fake$lower),\n  upper = plogis(fake$upper)\n)\n\n# Step 5: Plot\np3 &lt;- ggplot(fake, aes(x = rain, y = mean.p)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  labs(x = \"Rainfall\", y = \"Detection probability (p)\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\ndesign &lt;- \"\nAB\nCC\n\"\n\np1 + p2 + p3 + plot_layout(design = design)"
  },
  {
    "objectID": "OccMods_WithCovariates.html#dataset-one-detection-history",
    "href": "OccMods_WithCovariates.html#dataset-one-detection-history",
    "title": "Including Covariates in Occupancy Models",
    "section": "Dataset one: Detection history",
    "text": "Dataset one: Detection history\nThis dataset records whether or not the species was detected during each survey at each site.\nCreate an Excel sheet where rows are sites (or cameras) and columns are survey periods. For example, if your sampling unit is a 12-hour period and you have three surveys per site, your dataset might look like:\n\n\nCode\nlibrary(DT)\n\ndh &lt;- data.frame(\n  \"1\" = rbinom(50, size = 1, prob = 0.3),\n  \"2\" = rbinom(50, size = 1, prob = 0.5),\n  \"3\" = rbinom(50, size = 1, prob = 0.4),\n  check.names = FALSE\n)\n\ndatatable(dh, options = list(pageLength = 10))\n\n\n\n\n\n\n\n\nRow 1: Site 1 detections for surveys 1, 2, 3\n\nRow 2: Site 2 detections for surveys 1, 2, 3\n\netc.\n\nEach cell should be a 1 (detection) or 0 (no detection)."
  },
  {
    "objectID": "OccMods_WithCovariates.html#dataset-two-site-level-covariates",
    "href": "OccMods_WithCovariates.html#dataset-two-site-level-covariates",
    "title": "Including Covariates in Occupancy Models",
    "section": "Dataset two: Site-level covariates",
    "text": "Dataset two: Site-level covariates\nThis dataset contains all of your site covariates, variables that do not change from one survey to the next. These should be organised in the same order as your detection history dataset.\n\nThis is crucial. If site 1 is the first row in your detection history, it must also be the first row here. Check and double-check that the site ordering is consistent across files.\n\nEach covariate should be a column. For example:\n\nDistance to nearest road\n\nDistance to nearest release pen\n\nHabitat type\n\n\n\nCode\ndf &lt;- data.frame(\n  road = round(rgamma(50, 1, 2), digits = 3),\n  pen = round(rgamma(50, 2, 3), digits = 1),\n  habitat = sample(size = 50, x = c(\"Forest\", \"Field\"), prob = c(0.5, 0.5), replace = TRUE)\n)\n\ndatatable(df, options = list(pageLength = 10))"
  },
  {
    "objectID": "OccMods_WithCovariates.html#dataset-three-and-beyond-survey-level-covariates",
    "href": "OccMods_WithCovariates.html#dataset-three-and-beyond-survey-level-covariates",
    "title": "Including Covariates in Occupancy Models",
    "section": "Dataset three (and beyond): Survey-level covariates",
    "text": "Dataset three (and beyond): Survey-level covariates\nEach survey covariate needs its own Excel file. These files are structured like your detection history:\n\nRows are sites\n\nColumns are surveys\n\nThis setup allows each covariate to vary by site and by survey, which is essential when fitting more realistic detection models.\nFor example, you might have a dataset that records average light pollution on each survey:\n\n\nCode\ndf &lt;- data.frame(\n  \"1\" = rnorm(50, 0, 1),\n  \"2\" = rnorm(50, 1, 2),\n  \"3\" = rnorm(50, -1, 1),\n  check.names = FALSE\n)\n\ndatatable(df, options = list(pageLength = 10))\n\n\n\n\n\n\n\nBut if you were also collecting data on if a field was being ploughed or not you may have:\n\n\nCode\ndf &lt;- data.frame(\n  \"1\" = sample(size = 50, x = c(\"Plough\", \"No plough\"), prob = c(0.5, 0.5), replace = TRUE), \n  \"2\" = sample(size = 50, x = c(\"Plough\", \"No plough\"), prob = c(0.1, 0.9), replace = TRUE), \n  \"3\" = sample(size = 50, x = c(\"Plough\", \"No plough\"), prob = c(0.8, 0.2), replace = TRUE), \n  check.names = FALSE\n)\n\ndatatable(df, options = list(pageLength = 10))"
  },
  {
    "objectID": "Occupancy_Models.html",
    "href": "Occupancy_Models.html",
    "title": "Introduction to Occupancy Models",
    "section": "",
    "text": "A fundamental question in ecology is; Where are species present, and why are they present there?\n\nZitong, in your case this may be; Why are roe deer present at site A but not site B? Is it because site A is closer to roads? If a site is close to a road what’s the probability roe deer are present? Is that very different to sites that are far away from a road?\nWhat about features of the habitat? Are roe deer more likely if a site is forested versus agricultural?\n\n\nJoe, in your case this may be; Why are pheasants present at site A but not site B? Is it related to how far away the nearest known release pen is? What does the relationship look like? If there is no relationship, what would that mean for policy?\nWhat about the habitat makes pheasants more likely to occupy that site?\n\nTo act as motivation for this intro to occupancy models, let’s say we’re interested in understanding why are elephants present in the north of Etosha nature reserve in Namibia, but not in the south of the reserve? (As a side note, I’d recommend going there on holiday if you ever get the chance). Are elephants present in some parts of Etosha due to water availability? Is it related to food? Could predators play a role?\nTo find out, we need stats. As useful as gut feelings and intuition are, it’s not enough. Remember, statistics is how we get the evidence to answer our research question using data.\nThe best statistical tool available for answering these types of questions are “occupancy modeling”. These were originally developed by Daryl MacKenzie et al in 2002 who wanted to deal with a subtle issue when trying to find animals (more on this in a second). The broad concept underlying MacKenzie’s occupancy modelling framework built on ideas that had already been developed for estimating survival of individual animals, called the Cormack-Jolly-Seber, or CJS, model. As a cool note; Cormack and Jolly independently developed this model while they worked in Aberdeen University - a vital and internationally renowned model was developed right here in bloody Aberdeen! In my entire time as an undergraduate student, then as a PhD in Aberdeen no one ever told me this (even though I was taught these methods in Aberdeen)! I only found out at a conference in France years after leaving Aberdeen. As a school, we should be far more proud of this than we are.\nThe problem that occupancy models solve is subtle, yet absolutely crucial. Occupancy models estimate the probability a species occupies a site, and also the probability that you see them.\nAssume I want to determine where elephants are present in Etosha nature reserve in Namibia. I need data to answer this, so I decide to do ten elephant surveys in Etosha. Specifically, I go to visit these ten sites that are nicely spread throughout the park. At each site, whenever I spot an elephant I note down that elephants are present at that location. When I don’t see any elephants I record that elephants are absent at that location. Simple, right?\nLet’s imagine we collect data across ten sites. Here’s what our dataset might look like:\n\n\nCode\nset.seed(1988)\netosha &lt;- data.frame(\n  site = 1:10,\n  elephants = rbinom(n = 10, size = 1, prob = 0.4)\n)\netosha\n\n\n   site elephants\n1     1         0\n2     2         0\n3     3         0\n4     4         0\n5     5         0\n6     6         0\n7     7         0\n8     8         1\n9     9         1\n10   10         0\n\n\nWhen elephants is 1, it means elephants were seen at that site; when it’s 0, they were not. So, 1 indicates presence, and 0 indicates absence; at least for now.\nTo figure out the average probability that any one of my ten sites are occupied, I can run a Bernoulli Generalised Linear Model. The Bernoulli distribution (named after Jacob Bernoulli) generates values of 1 or 0, which is perfect for our binary presence–absence data.\nThe model would be:\n\\[\ny_i \\sim Bernoulli(p_i) \\\\\n\\]\n\\[\nlogit(p_i) = \\beta_0\n\\]\nLet’s go through these equations slowly:\n\n\\(y\\) is our observation (elephants in the etosha dataset)\n\\(i\\) is the index, here being which site the data was collected from\n\\(\\sim\\) means “generated according to” (or “our data is the same as would be generated by the following distribution”)\n\\(Bernoulli\\) is a type of distribution that will generate either 0 or 1\n\\(p\\) is the probability of success (i.e. there is \\(p\\) probability that we see an elephant). We can’t possibly know what \\(p\\) is when we collect the data, so we need to figure it out with statistics. (The \\(i\\) means each site could have a different probability - but we’re not doing that yet)\n\\(logit\\) is the link function to ensure that \\(p\\) remains between 0% and 100%. Specifically, it’s a little bit of maths: \\(log(\\frac{p}{1-p})\\), which is the natural log of the probability to succeed (\\(p\\)) divided by the probability to fail (\\(1-p\\)). More on this in a second.\n\\(\\beta_0\\) is the intercept. Since we haven’t included any covariates (yet), it captures the average logit value of elephant presence across all sites. When converted back from the logit scale, this will give us the average probability of elephant presence.\n\nTo understand how the logit link transforms probabilities, have a look at this figure. It maps the full range of probabilities (0% to 100%) to the logit scale.\n\n\nCode\np &lt;- seq(from = 0, to = 1, by = 0.001)\nlogit &lt;- log(p/(1-p))\ndat &lt;- data.frame(p, logit)\nlibrary(ggplot2)\nggplot(dat) +\n  geom_line(aes(x = logit, y = p)) +\n  labs(x = \"Logit value that\\nelephants are present\",\n       y = \"Probability that\\nelephants are present\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\nNotice how as the probability approaches 0%, it never actually drops below it, even as the logit value keeps decreasing? And the same happens when it gets close to 100%. And that’s exactly what we want. The fact that logit values can range from \\(-\\infty\\) to \\(+\\infty\\) makes model fitting much easier: it turns a bounded probability into an unbounded scale, allows linear relationships with predictors, and ensures valid probabilities when converting predictions back. So even if our model estimates a logit value of 999 billion, it still maps to a probability of 100%. This ensures the natural bounds of probability, which is between 0% and 100%, are always respected.\nIf you want a bit more detail and intuition, here is an old lecture recording that covers the logit link function:\n\n\nWith that in mind, here’s how we’d fit our model in R. (Click Code to reveal the code.)\n\n\nCode\nmod &lt;- glm(elephants ~ 1,\n           data = etosha,\n           family = binomial(link = \"logit\"))\n\n\nThis estimates \\(\\beta_0\\), the average probability of elephant presence, but on the logit scale. We can see what it has been estimated as by looking at the summary of the model:\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nglm(formula = elephants ~ 1, family = binomial(link = \"logit\"), \n    data = etosha)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -1.3863     0.7906  -1.754   0.0795 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10.008  on 9  degrees of freedom\nResidual deviance: 10.008  on 9  degrees of freedom\nAIC: 12.008\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe estimate for (Intercept) (or \\(\\beta_0\\)) is -1.3863. Now that we have an estimate for \\(\\beta_0\\), we can plug it into our model:\n\\[\ny_i \\sim Bernoulli(p_i) \\\\\n\\]\n\\[\nlogit(p_i) = -1.3863\n\\]\nTo interpret this on the probability scale, we apply the inverse logit transformation using plogis() in R:\n\n\nCode\nplogis(-1.3863)\n\n\n[1] 0.1999991\n\n\nThis gives us a probability of approximately 0.2, or 20%, meaning there’s a 20% chance a site is occupied by elephants.\nOr does it?"
  },
  {
    "objectID": "Occupancy_Models.html#state-model",
    "href": "Occupancy_Models.html#state-model",
    "title": "Introduction to Occupancy Models",
    "section": "State model",
    "text": "State model\nThe first part of an occupancy model is the state model, so called because it estimates the true state of each site: is the species present or absent?\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\n\\]\n\\[\nlogit(\\psi_i) = \\beta_0\n\\]\nThis model probably looks similar. That’s because like the first model we spoke about further above, it’s also a \\(Bernoulli\\) GLM. But they differ in two important ways:\n\n\\(z\\) is the true presence or absence of elephants in site \\(i\\) (this isn’t \\(y\\))\n\\(\\psi\\) is the probability to be present\n\nThat’s nice and all but just changing the labels doesn’t fix imperfect detection. To solve that, we need a second model: the observation model."
  },
  {
    "objectID": "Occupancy_Models.html#observation-model",
    "href": "Occupancy_Models.html#observation-model",
    "title": "Introduction to Occupancy Models",
    "section": "Observation model",
    "text": "Observation model\nThe second GLM (which I’ll call the observation model) also looks remarkably similar:\n\\[\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n\\]\n\\[\nlogit(p_{i,j}) = \\alpha_0\n\\]\nIt’s another \\(Bernoulli\\) GLM but with some crucial differences.\n\n\\(y\\) is now the detection or not of an elephant. This is worth highlighting - \\(y\\) is not the presence or absence of elephants, it’s the detection of elephants if they’re present!\n\\(j\\) represents which survey \\(y\\) was collected in, which inherently means we have multiple surveys, not just one like in our first GLM example above where \\(i\\) was a single (of ten) sites. This has implications for how we collect data, which we’ll come back to.\n\\(p\\) is the probability to detect an elephant at site \\(i\\) in survey \\(j\\) (e.g. what is the probability to detect an elephant in site 1 in the third survey?)\n\nImportantly, the probability to detect elephants (\\(p\\)) is multiplied by \\(z\\). What’s \\(z\\)? Well that’s the true occupancy state of that site from the state model. It’s this multiplication that allows the two models to “speak” to each other, and how we deal with imperfect detection; it ensures the observation process depends on whether the species is actually present.\nIf there are elephants in a site (\\(z = 1\\)), then \\(p \\times z = p \\times 1 = p\\). If elephants are absent from a site (\\(z = 0\\)), then \\(p \\times z = p \\times 0 = 0\\). This means you cannot detect elephants if they aren’t there. That’s blindingly obvious… It’s so dumb that most people don’t realise you need to specify it (I’ve heard people say: “surely any model can figure this out?”). But this stupidly simple logic is missing from our starting GLM! And a lot of research asking about where species are use a model equivalent to the very first GLM on this page.\n\nKeep in mind that we don’t know \\(z\\). That’s the true occupancy state, a latent variable. Latent just means it’s unobserved: we can’t measure \\(z\\) directly in the field, but we can estimate it using the data we collect, much like we do with parameters.\n\nThat’s how occupancy models “know” that you can only detect elephants if they’re present, and that if they’re absent, you won’t detect them. It seems so obvious, yet this logic is missing from many models that treat non-detections as true absences.\nAnd that’s the beauty of occupancy models. They handle the messy reality of ecological data (but can also be used in e.g. medicine), and that’s why they’re one of my favorite types of analysis."
  }
]