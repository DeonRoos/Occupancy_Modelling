[
  {
    "objectID": "Bayesian_Occupancy_Models.html",
    "href": "Bayesian_Occupancy_Models.html",
    "title": "Bayesian Algorithms",
    "section": "",
    "text": "In the previous page, we went through the underlying theory of Bayesian statistics; how our prior knowledge is updated using real data through Bayes’ Theorem. Now, we’re going to move into the practical application of these ideas using the spOccupancy package. My goal here is to help you understand how Bayesian occupancy models are fit, how to monitor convergence, and how to interpret uncertainty through credible intervals.\nWe’ll again work with the Etosha elephant dataset. Importantly, we are not yet including spatial autocorrelation. We’ll add that in the next section.\n\n\nCode\nlibrary(spOccupancy)\n\n# These are needed to produce the website\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggthemes)\n\n\n\nHow do we estimate a posterior?\nIn the previous page we spoke about posteriors and how these are a combination of our prior belief and our data. But how does that actually happen? Well, the answer is pretty simple but took hundreds of years for statisticians to figure out. Markov Chain Monte Carlo is an algorithm that was developed by scientists at Los Alamos in the 1950s working on the Manhattan Project (yes that Manhattan Project). The algorithm is called Markov Chain Monte Carlo (or MCMC) for two reasons. A Markov Chain describes a sequence of states, where the probability to move from one state to the next state depends only on what the current state is.\nFor example, imagine you are walking through a series of rooms. Where you go next depends only on the room you’re currently in. You can’t instantly teleport to the other side of the building! Nor does it depend on the path you have taken to get to your current room.\n\n\nWhat does MCMC look like?\nBelow is an animation of a simple MCMC algorithm to help give an intuition for what’s happening behind the scenes.\nFor this, we have the following simple linear model:\n\\[y_i \\sim Normal(\\mu_i, \\sigma^2)\\]\n\\[\\mu_i = \\beta_0 + \\beta_1 \\times x_i \\]\nSo our objective here is to figure out \\(\\beta_0\\) and \\(\\beta_1\\). To do so, we’ll set up an MCMC using one “chain”. A chain is a Markov Chain - it’s the black dot and orange line you can see in the animation below. This chain is trying to find the “True value” location marked with a teal X. At each step the black dot asks “if I take a step in a random direction, will I be closer to X or further away?”. If it’s closer, then it takes the step. If it’s further away, then it’s less likely to take the step but it’s not impossible. This seems like a bad choice. Why move somewhere if it’s further away from X? The answer is a bit nuanced, but the concise explanation is that sometimes there might be areas of the “parameter space” (the parameter space is all possible values of \\(\\beta_0\\) and \\(\\beta_1\\)) which “work” quite well despite not being the true value. If we didn’t have the behaviour, where the chain might take a step even though it’s worse, then we might get stuck in one of these “bad” areas (technically, these “bad” areas are called “local minima”).\nHere’s our MCMC in action:\n\n\nCode\ntrue_param &lt;- c(2, -1)\ntarget_density &lt;- function(x, y) {\n  exp(-0.5 * ((x - true_param[1])^2 / 1^2 + (y - true_param[2])^2 / 0.5^2))\n}\n\nset.seed(123)\nn_iter &lt;- 100\nx &lt;- y &lt;- 0\nsamples &lt;- tibble(x = x, y = y, iteration = 1)\n\nfor (i in 2:n_iter) {\n  x_prop &lt;- rnorm(1, x, 0.4)\n  y_prop &lt;- rnorm(1, y, 0.4)\n  accept_ratio &lt;- target_density(x_prop, y_prop) / target_density(x, y)\n  \n  if (runif(1) &lt; accept_ratio) {\n    x &lt;- x_prop\n    y &lt;- y_prop\n  }\n  samples &lt;- samples |&gt; add_row(x = x, y = y, iteration = i)\n}\n\nggplot(samples, aes(x = x, y = y)) +\n  geom_path(color = \"#FF5733\", linewidth = 0.7) +\n  geom_point(aes(x = x, y = y), color = \"black\", size = 2) +\n  geom_point(aes(x = true_param[1], y = true_param[2]), \n             color = \"#00A68A\", size = 3, shape = 4, stroke = 2) +\n  annotate(\"text\", x = true_param[1] + 0.1, y = true_param[2],\n           label = \"True value\", color = \"#00A68A\", hjust = 0) +\n  transition_reveal(iteration) +\n  coord_fixed() +\n  labs(\n    title = \"MCMC Chain Step: {frame_along}\",\n    x = bquote(beta[0]),\n    y = bquote(beta[1])\n  ) +\n  theme_bw()\n\n\n\nHere’s where the really clever bit comes in. At each iteration, if we record the parameter value it tried, and store it, when we build it up the end result is our posterior! This is what made Bayesian statistics possible! We don’t need to use any crazy (and often impossible) maths to figure out the posterior, we just have MCMC walk around and the end result is an insanely good reflection of the posterior!\n\n\nCode\nsamples_long &lt;- samples |&gt;\n  pivot_longer(cols = c(x, y), names_to = \"parameter\", values_to = \"value\")\n\nsamples_long &lt;- samples_long |&gt;\n  mutate(true_value = if_else(parameter == \"x\", true_param[1], true_param[2]))\n\nsamples_long_cumulative &lt;- samples_long |&gt;\n  group_by(parameter) |&gt;\n  group_split() |&gt;\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    map_dfr(1:max(df$iteration), function(i) {\n      df |&gt;\n        filter(iteration &lt;= i) |&gt;\n        mutate(frame = i, parameter = param)\n    })\n  })\n\nsamples_long_cumulative &lt;- samples_long_cumulative %&gt;%\n  mutate(parameter_label = case_when(\n    parameter == \"x\" ~ \"beta[0]\",\n    parameter == \"y\" ~ \"beta[1]\"\n  ))\n\nggplot(samples_long_cumulative, aes(x = value)) +\n  geom_histogram(binwidth = 0.4, fill = \"#FF5733\", color = \"white\", boundary = 0) +\n  geom_vline(aes(xintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", ncol = 1, labeller = label_parsed) +\n  transition_manual(frame) +\n  labs(title = \"Cumulative Posterior up to Iteration {current_frame}\",\n       x = \"Parameter Value\", y = \"Frequency\") +\n  theme_bw()\n\n\n\nAnother way to show what the MCMC was up to is using something called “traceplots”. These are relatively simple but actually quite powerful for determining if we trust the model output. We’ll come back to this in a bit, but for now, we can show the MCMC exploring the parameter space using these traceplots.\n\n\nCode\nsamples_long_cumulative_trace &lt;- samples_long |&gt;\n  group_by(parameter) |&gt;\n  group_split() |&gt;\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    map_dfr(1:max(df$iteration), function(i) {\n      df |&gt;\n        filter(iteration &lt;= i) |&gt;\n        mutate(frame = i, parameter = param)\n    })\n  })\n\nsamples_long_cumulative_trace &lt;- samples_long_cumulative_trace %&gt;%\n  mutate(parameter_label = case_when(\n    parameter == \"x\" ~ \"beta[0]\",\n    parameter == \"y\" ~ \"beta[1]\"\n  ))\n\nggplot(samples_long_cumulative_trace, aes(x = iteration, y = value)) +\n  geom_line(color = \"#FF5733\", linewidth = 0.8) +\n  geom_hline(aes(yintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", ncol = 1, labeller = label_parsed) +\n  transition_manual(frame) +\n  labs(title = \"Traceplot up to Iteration {current_frame}\",\n       x = \"Iteration\", y = \"Parameter Value\") +\n  theme_bw()\n\n\n\n\n\nImproving our MCMC\nWe’ve made a good start but we can improve this quite a bit. Firstly, limiting the algorithm to 100 iterations doesn’t give it many opportunities to find the true value. In general, you often give MCMC thousands of iterations, rather than a paltry 100. So first improvement is to increase the number of iterations (you may remember we did this when we were going through the occupancy theory pages - now you know why!).\nSecondly, we’re using one MCMC chain. Why not more? Afterall, if we have say four chains, then if all four agree that they’re close to the true value that would give us more comfort. If they find different “True values”, well, then it seems likely that we haven’t actually found it.\nLet’s implement our improvement and see what our plots now look like:\n\n\nCode\ntrue_param &lt;- c(2, -1)\n\ntarget_density &lt;- function(x, y) {\n  exp(-0.5 * ((x - true_param[1])^2 / 1^2 + (y - true_param[2])^2 / 0.5^2))\n}\n\nset.seed(123)\nn_iter &lt;- 1000\nn_chains &lt;- 4\n\nchains_list &lt;- map_dfr(1:n_chains, function(chain_id) {\n  x &lt;- y &lt;- 0  # Start at (0, 0)\n  samples &lt;- tibble(x = x, y = y, iteration = 1, chain = chain_id)\n  \n  for (i in 2:n_iter) {\n    x_prop &lt;- rnorm(1, x, 0.4)\n    y_prop &lt;- rnorm(1, y, 0.4)\n    accept_ratio &lt;- target_density(x_prop, y_prop) / target_density(x, y)\n    \n    if (runif(1) &lt; accept_ratio) {\n      x &lt;- x_prop\n      y &lt;- y_prop\n    }\n    \n    samples &lt;- samples %&gt;% add_row(x = x, y = y, iteration = i, chain = chain_id)\n  }\n  samples\n})\n\nggplot(chains_list, aes(x = x, y = y, group = chain, color = as.factor(chain))) +\n  geom_path(linewidth = 0.7) +\n  geom_point(aes(x = x, y = y), size = 1.5) +\n  geom_point(aes(x = true_param[1], y = true_param[2]), \n             color = \"white\", size = 4, shape = 4, stroke = 2, inherit.aes = FALSE) +\n  annotate(\"text\", x = true_param[1] + 0.1, y = true_param[2],\n           label = \"True value\", color = \"white\", hjust = 0) +\n  transition_reveal(along = iteration) +\n  coord_fixed() +\n  scale_color_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"MCMC Chains Step: {round(frame_along, digits = 0)}\",\n    x = bquote(beta[0]),\n    y = bquote(beta[1])) +\n  theme_bw()\n\n\n\n\n\nCode\nsamples_long &lt;- chains_list %&gt;%\n  pivot_longer(cols = c(x, y), names_to = \"parameter\", values_to = \"value\") %&gt;%\n  mutate(true_value = if_else(parameter == \"x\", true_param[1], true_param[2]),\n         parameter_label = case_when(\n           parameter == \"x\" ~ \"beta[0]\",\n           parameter == \"y\" ~ \"beta[1]\"\n         ))\n\n# Build cumulative data\nsamples_long_cumulative &lt;- samples_long %&gt;%\n  group_by(parameter, chain) %&gt;%\n  group_split() %&gt;%\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    chain_id &lt;- unique(df$chain)\n    map_dfr(1:max(df$iteration), function(i) {\n      df %&gt;%\n        filter(iteration &lt;= i) %&gt;%\n        mutate(frame = i, parameter = param, chain = chain_id)\n    })\n  })\n\nggplot(samples_long_cumulative, aes(x = value, fill = as.factor(chain))) +\n  geom_histogram(binwidth = 0.4, color = \"white\", boundary = 0, \n                 position = position_dodge(), alpha = 0.6) +\n  geom_vline(aes(xintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", labeller = label_parsed, ncol = 1) +\n  transition_manual(frame) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"Cumulative Posterior up to Iteration {current_frame}\",\n       x = \"Parameter Value\", y = \"Frequency\") +\n  theme_minimal()\n\nggplot(samples_long_cumulative, aes(x = iteration, y = value, color = as.factor(chain))) +\n  geom_line(linewidth = 0.7) +\n  geom_hline(aes(yintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_y\", labeller = label_parsed, ncol = 1) +\n  transition_manual(frame) +\n  scale_color_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"Traceplot up to Iteration {current_frame}\",\n       x = \"Iteration\", y = \"Parameter Value\") +\n  theme_minimal()\n\n\n\n\nI have a bit more confidence in our posteriors now:\n\nAll chains seem to agree on the same value (they have “converged” to the same answer)\nThe posteriors look reasonably well estimated (they’ll close to the true value - but keep in mind that with real analysis we don’t know what the truth is)\nThe traceplots resemble “hairy caterpillars”, which matches the Markov Chain idea that the next value depends only on the current value (it doesn’t “remember” older values).\n\nThe final thing I want to bring your attention to are the first hundred or so iterations. Generally, these iterations can be pretty wild, fluctuating massively. If you think about it, that’s kind of fair enough. We’re starting each MCMC chain at random locations, so it’s fair if things are a bit wobbly at the start. So what we do with these early iterations is to simply ignore them. These are called “burn in” iterations, as in, we’re letting the engine warm up, so these are just used to get up to speed. Mostly arbitrarily, we typically ignore the first 10% of iterations, so where we’ve used 1000 iterations, we’d generally ignore the first 100.\n\n\nFitting the occupancy model using MCMC\nLet’s now revisit our Bayesian model. As a reminder, here’s what it looked like:\n\\[z_i \\sim Bernoulli(\\psi_i)\\\\\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\nWhich we can translate into the following code. But pay attention to the n.chains, n.samples and n.burn arguments. This is where we specify how many chains we want (n.chains), how many iterations we want (n.samples, note an iteration can also be called a sample), and how many of the first iterations we want to ignore (n.burn).\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  n.chains = 4,     # 4 chains just like in our simple example\n  n.samples = 2000, # 2000 iterations for each chain\n  n.burn = 200,     # We ignore the first 200 iterations to give MCMC a chance to get it's feet\n  \n  verbose = FALSE   # This just says don't spit out details while fitting \n  # (normally I would leave verbose = TRUE so I can keep track of the model while it's fitting)\n  )\n\n\nWe can now check how our model worked. The code is pretty simple. The only tricky thing is to specify if you want beta or alpha. Importantly, beta here refers to the occupancy parameters, and alpha refers to the detection parameters.\nSo here are the traceplots and posteriors for \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) in:\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\n\nCode\nplot(fit, 'beta') # Occupancy parameters.\n\n\n\n\n\nAnd here are the traceplots and posteriors for \\(\\alpha_0\\) and \\(\\alpha_1\\) in:\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\n\n\nCode\nplot(fit, 'alpha') # Detection parameters.\n\n\n\n\n\nAnd we can also get a more numeric summary of the model using summary(). Importantly, the information you see below is the same as the data in the figures above. It’s just summarised! So the Mean is just the mean of the posteriors above!\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0183\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0536 1.0657  0.1991  1.9727 4.2706 1.0151 315\ntree         0.2259 1.2968 -2.1384  0.1544 2.9006 1.0797 278\ntemp        -1.1362 1.1440 -3.2916 -1.1758 1.3257 1.0396 272\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8539 0.2623 -2.3783 -1.8533 -1.3435 1.0018 1142\nrain         0.1730 0.2207 -0.2558  0.1719  0.6087 1.0015 2994\n\n\n\n\nCredible intervals\nNotice the 2.5% and 97.5% in the tables above? These are the credible intervals that I briefly mentioned in the Occupancy Models: Covariates page. Technically, these are just the quantiles of the posterior. Or, phrased alternatively, 95% of all iterations are within this interval. And keep in mind, that a Bayesian credible interval is not the same as a frequentist confidence interval. For our purposes, these intervals represent a 95% probability to contain the true value (based on the data we collected and the model we fit)!\n\n\nCode\nbeta_0_samples &lt;- fit$beta.samples[,1]\n\nbeta_0_df &lt;- tibble(beta_0 = beta_0_samples)\n\nsummary_stats &lt;- beta_0_df %&gt;%\n  summarise(\n    mean = mean(beta_0),\n    median = median(beta_0),\n    lower = quantile(beta_0, 0.025),\n    upper = quantile(beta_0, 0.975)\n  )\n\nggplot(beta_0_df, aes(x = beta_0)) +\n  geom_histogram(fill = \"#00A68A\", colour = \"white\", alpha = 0.5, bins = 50) +\n  \n  # Vertical lines\n  geom_vline(xintercept = summary_stats$mean, linetype = \"solid\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$median, linetype = \"dashed\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$lower, linetype = \"dotted\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$upper, linetype = \"dotted\", color = \"black\", linewidth = 1) +\n\n  # curved arrows\n  geom_curve(aes(x = summary_stats$mean + 0.5, y = 400, \n                 xend = summary_stats$mean, yend = 350), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = 0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$median - 0.5, y = 300, \n                 xend = summary_stats$median, yend = 250), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = -0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$lower - 0.5, y = 100, \n                 xend = summary_stats$lower, yend = 75), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = -0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$upper + 0.5, y = 100, \n                 xend = summary_stats$upper, yend = 75), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = 0.3, color = \"black\") +\n\n  annotate(\"text\", x = summary_stats$mean + 0.5, y = 400, label = \"Mean\", hjust = 0, size = 5) +\n  annotate(\"text\", x = summary_stats$median - 0.5, y = 300, label = \"Median\", hjust = 1, size = 5) +\n  annotate(\"text\", x = summary_stats$lower - 0.5, y = 100, label = \"2.5% CI\", hjust = 1, size = 5) +\n  annotate(\"text\", x = summary_stats$upper + 0.5, y = 100, label = \"97.5% CI\", hjust = 0, size = 5) +\n\n  labs(\n    x = expression(paste(beta[0], \" or '(Intercept)'\")),\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nFrom our summary table, the mean for the \\(\\beta_0\\) or (Intercept) posterior was estimated as 2.0536, the median (called 50% in the summary table) was 1.9727, and the 95% credible intervals were 0.1991 and 4.2706, just like we see in the figure.\nThis might seem trivial but having credible intervals be so simple, and the interpretation having a useful meaning is one of the big selling points of Bayesian analysis in my mind. It’s not something to trivialise.\n\n\nSpecifying priors\nSo far, we’ve let the spOccupancy package handle setting the priors for us behind the scenes. But we can actually specify our own priors if we want to.\nIn the background, spOccupancy uses something called Pólya-Gamma data augmentation, and under the hood, this method assumes \\(Normal\\) priors for both the occupancy and detection parameters (including their intercepts).\nIf you don’t specify anything, spOccupancy will set:\n\nThe “hypermean” (mean of \\(Normal\\) distribution for all priors) = 0\nThe “hypervariance” (variance \\(Normal\\) distribution for all priors) = 2.72\n\nThis corresponds to a relatively flat prior on the probability scale (the 0–1 scale for occupancy or detection probabilities). In other words, by default, the prior doesn’t strongly pull your estimates toward any particular value; it lets the data mostly speak for itself.\nBut we can set these explicitly if we want to! Here’s what that looks like:\n\n\nCode\n# Specify priors for detection (alpha) and occupancy (beta)\npriors &lt;- list(\n  alpha.normal = list(mean = 0, var = 2.72),  # Detection priors\n  beta.normal = list(mean = 0, var = 2.72)    # Occupancy priors\n)\n\n\nThe above code is basically doing what spOccupancy does by default. All priors will get \\(Normal(0,2.72)\\) in both the detection and occupancy sub models. We can either give a single number (like 0 and 2.72 above), and it will apply to all parameters, or you can give a vector if you want different priors for different parameters.\nHere’s how we’d give different priors for each parameter:\n\n\nCode\n# Different priors for each occupancy parameter\noccurrence_priors &lt;- list(\n  mean = c(0, 0.5, -0.5),    # One mean for each occupancy parameter\n  var = c(2.72, 1, 2)        # One variance for each occupancy parameter\n)\n\n# Different priors for each detection parameter\ndetection_priors &lt;- list(\n  mean = c(0, 0.2),          # One mean for each detection parameter\n  var = c(0.5, 1)           # One variance for each detection parameter\n)\n\n# Combine into the priors list\npriors &lt;- list(\n  beta.normal = occurrence_priors,\n  alpha.normal = detection_priors\n)\n\n\nThis would translate into this model:\n\\[z_i \\sim Bernoulli(\\psi_i)\\\\\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[\\beta_0 \\sim Normal(0, 2.72)\\]\n\\[\\beta_1 \\sim Normal(0.5, 1)\\]\n\\[\\beta_2 \\sim Normal(-0.5, 2)\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\n\\[\\alpha_0 \\sim Normal(0, 0.5)\\]\n\\[\\alpha_1 \\sim Normal(0.2, 1)\\]\nNow, to be very, very clear; I am choosing these priors completely at random just for demonstration. I have no reason, in this case, to think that a reasonable prior for \\(\\beta_1\\) is \\(Normal(0.5, 1)\\).\nBut we can always fit this model to see what happens:\n\n\nCode\nfit_priors &lt;- PGOcc(\n  occ.formula = ~ tree + temp,\n  det.formula = ~ rain,\n  data = etosha,\n  priors = priors,     # We add our priors here\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE\n)\n\n\nHaving fit, we can see what the posteriors look like:\n\n\nCode\nplot(fit_priors, 'beta') # Occupancy parameters.\n\n\n\n\n\nCode\nplot(fit_priors, 'alpha') # Detection parameters.\n\n\n\n\n\nAnd pull up the summary:\n\n\nCode\nsummary(fit_priors)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    priors = priors, n.samples = 2000, verbose = FALSE, n.burn = 200, \n    n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0158\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.5053 0.9644 -0.2024  1.4383 3.5766 1.0514 304\ntree         0.5140 0.9042 -1.2174  0.4798 2.3414 1.0165 412\ntemp        -1.2473 0.9496 -3.0085 -1.2664 0.7709 1.0097 365\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.6451 0.2634 -2.1407 -1.6515 -1.0959 1.0112 1004\nrain         0.1595 0.2133 -0.2483  0.1565  0.6006 1.0048 3385\n\n\nIf we compare this with the original model, where we left the priors at their defaults we can see that different priors can lead to different posteriors. Keep in mind how Bayesian statistics work - the posterior is a combination of our data and prior beliefs. We’ve changed our prior belief and our posteriors have changed as a result.\nNow in this case, the difference in posteriors are pretty minimal. It might not always be. And in truth, we would want to think a bit more carefully about the priors. Don’t be fooled here. Just because my randomly chosen priors are different from the default ones doesn’t mean the default model is necessarily better. The best option is to choose priors you think are reasonable. If you have different prior beliefs (e.g. I have some informative priors but also some uninformative priors) I can run the model with both, and see how much of a difference it makes. This is something called “prior sensitivity analysis” and is a fairly useful tool to have in your back pocket when you’re not sure about your priors.\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0183\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0536 1.0657  0.1991  1.9727 4.2706 1.0151 315\ntree         0.2259 1.2968 -2.1384  0.1544 2.9006 1.0797 278\ntemp        -1.1362 1.1440 -3.2916 -1.1758 1.3257 1.0396 272\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8539 0.2623 -2.3783 -1.8533 -1.3435 1.0018 1142\nrain         0.1730 0.2207 -0.2558  0.1719  0.6087 1.0015 2994\n\n\n\n\nFin\nWith that, you should be good to run a Bayesian occupancy model. Something you can add to your CV and have employers fawn over you. Even better if you understand it, so that when they ask you about it you can have a conversation!\nThe only part left in the analysis is including “spatial autocorrelation”. We’ll cover that in the next page.\n\n\nA subtle point: Priors and the link function\nWhen we choose priors in a Bayesian model, it’s really important to remember what scale those priors live on.\nIn our occupancy model, we specify priors for the parameters on the logit scale, not directly on the probability (0–1) scale.\nFor example, if we write:\n\\[logit(p_i) = \\beta_0 + \\beta_1 \\times x_i\\]\nthen \\(\\beta_0\\) and \\(\\beta_1\\) are in logit space.\nRemember, the logit function stretches the 0–1 probability scale onto the whole real line:\n\nProbabilities near 0.5 correspond to logits near 0.\nProbabilities near 0 or 1 correspond to logits of -\\(\\inf\\) and +∞.\n\nThis means that a \\(Normal\\) prior with mean 0 and large variance on the logit scale is not flat on the probability scale! Even “uninformative” \\(Normal\\) priors on the logit scale can actually imply very strong beliefs on the probability scale.\nTo build some intuition, we’ll do a little prior predictive simulation:\n\nWe’ll randomly draw values for \\(\\beta_0\\) and \\(\\beta_1\\) from a \\(Normal(0, 2.72)\\) prior.\nWe’ll simulate the relationship between \\(x\\) and \\(p(x)\\) by plugging those \\(\\beta_0\\) and \\(\\beta_1\\) values into the logit equation.\nWe’ll repeat this 100 times to show many possible relationships.\n\n\n\nCode\nset.seed(123)\n\nn_draws &lt;- 100\nx_seq &lt;- seq(-3, 3, length.out = 100)\n\nbeta_0_draws &lt;- rnorm(n_draws, mean = 0, sd = sqrt(2.72))\nbeta_1_draws &lt;- rnorm(n_draws, mean = 0, sd = sqrt(2.72))\n\nprior_simulations &lt;- map2_dfr(\n  beta_0_draws, beta_1_draws,\n  .f = function(b0, b1) {\n    tibble(\n      x = x_seq,\n      logit_p = b0 + b1 * x,\n      p = plogis(logit_p)\n    )\n  },\n  .id = \"draw\"\n)\n# Plot\nggplot(prior_simulations, aes(x = x, y = p, group = draw)) +\n  geom_line(alpha = 0.2, color = \"#FF5733\") +\n  theme_minimal() +\n  labs(\n    title = \"Prior Predictive Simulation\",\n    x = \"Covariate (x)\",\n    y = \"Probability (p)\"\n  )\n\n\n\n\n\nEach orange line is a possible relationship between \\(x\\) and \\(p(x)\\) given the priors we chose. Notice that some lines are almost flat at 0 or 1? While others are very steep, flipping from 0 to 1 over a narrow range of \\(x\\)? Even though the prior on \\(\\beta_0\\) and \\(\\beta_1\\) was centered at 0 with large variance, the resulting priors on \\(p\\) are not uniform or “neutral.”\nIf I were being hyper cautious, I might be worried these priors are pushing the model towards the extreme flipping behaviour. In some cases that might be good, in others it might be bad.\nThe broader points I am making here are:\n\nDon’t stress too much about priors. If you have a lot of data your prior will often not be terribly important.\nBut give a little thought as to what a sensible prior would be, especially when working with link functions.\n\nIf you’re in doubt speak with me! I think having a discussion about your priors would be an excellent use of one of our meetings (hint, hint)."
  },
  {
    "objectID": "Bayesian_Statistics.html",
    "href": "Bayesian_Statistics.html",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "",
    "text": "Most of the statistics you’ve probably learned so far has been based in what’s called “frequentist” statistics. The analysis you’ll be using for occupancy models (at least this particular implementation) will be using something called “Bayesian” statistics (pronounced “Bay-zee-’n”). These are the two main frameworks currently used to learn something from data (unless you want to start splitting philosophical hairs). Within this document we’re going to start with what Bayesian statistics is, how to move from frequentist to Bayesian statistics, and how a Bayesian model works. We’ll leave occupancy models to the side for the time being."
  },
  {
    "objectID": "Bayesian_Statistics.html#your-prior-belief-before-seeing-the-data",
    "href": "Bayesian_Statistics.html#your-prior-belief-before-seeing-the-data",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "Your Prior Belief (before seeing the data)",
    "text": "Your Prior Belief (before seeing the data)\nYou start off thinking you have a 50% chance of passing. That doesn’t mean you’re certain, it’s just your best guess. You might be unsure, so you’re open to the idea that it could be lower or higher. Your prior might look like this:\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\ntheta &lt;- seq(0, 1, length.out = 1000)\n\nprior &lt;- dbeta(theta, 20, 20)\n\ndata.frame(theta, prior) |&gt;\n  ggplot(aes(x = theta, y = prior)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Chance of Passing\", y = \"Belief Density\") +\n  theme_minimal()"
  },
  {
    "objectID": "Bayesian_Statistics.html#the-data-100-students-passed",
    "href": "Bayesian_Statistics.html#the-data-100-students-passed",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "The Data (100 students passed)",
    "text": "The Data (100 students passed)\nNow you find out that 100 students have taken the thesis before you, and all of them passed. That’s really strong evidence that the pass rate is high, probably very near to 100%.\nWe can visualise how likely the observed data is for different values of the pass rate, this is the likelihood.\n\n\nCode\nlikelihood &lt;- theta^100\nlikelihood &lt;- likelihood / max(likelihood)\n\ndata.frame(theta, likelihood) |&gt;\n  ggplot(aes(x = theta, y = likelihood)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Chance of Passing\", y = \"Likelihood (scaled)\") +\n  theme_minimal()\n\n\n\n\n\nIn Bayesian terms, this evidence updates your belief."
  },
  {
    "objectID": "Bayesian_Statistics.html#the-posterior-updated-belief",
    "href": "Bayesian_Statistics.html#the-posterior-updated-belief",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "The Posterior (updated belief)",
    "text": "The Posterior (updated belief)\nYour updated belief, the posterior, combines your initial guess of roughly 50% with the strong evidence we have from 100 students. The resulting curve now shifts to the right — it’s taller near 85%–90% but still not at 100%, because you haven’t thrown out your original uncertainty.\n\n\nCode\nposterior &lt;- dbeta(theta, 120, 20)\n\nposterior_df &lt;- data.frame(\n  theta = rep(theta, 3),\n  density = c(prior, likelihood, posterior),\n  belief = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(theta))\n)\n\nposterior_df |&gt;\n  ggplot(aes(x = theta, y = density, color = belief)) +\n  geom_line(size = 1.2) +\n  labs(x = \"Chance of Passing\", y = \"Belief / Likelihood (scaled)\", colour = \"\") +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"steelblue\", \"darkgreen\", \"firebrick\"))\n\n\n\n\n\nImportantly, in Bayesian statistics, we don’t have a single value for our parameter estimate (or posterior). Instead we have a full distribution of values. This becomes an insanely valuable property that can be exploited in many different ways, which we’ll get back to later on.\nFor now, your posterior is more-or-less the multiplication of your prior belief with the likelihood (\\(P(Data | Parameters)\\times P(Parameters)\\)). In this case, giving you a happy middle ground of ca. 85% chance to pass.\nThis is how Bayesian statistics works in practice:\n\nStart with a belief (or hypothesis).\nSee some evidence (i.e. collect some data).\nUpdate your belief accordingly.\n\nIn this case, seeing 100 successful students shifts your belief quite a lot, but not all the way to 100%, because your prior belief still matters! You know yourself after all! You’d be silly to just ignore that.\nIn short, this is kinda the scientific method, isn’t it? (pls don’t tell any frequentists that I said that…)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Occupancy Modelling",
    "section": "",
    "text": "This site is for you both and introduces the fundamentals and advanced topics in occupancy modelling that you’ll need to do your honours project. I’ve written the material to be accessible for you both, in terms of your honours projects, but also to help boost your skillset for when you finish you undergraduate.\nAs always, if there is anything in the site that is either wrong or confusing, just let me know. If it helps, I would be amazed if you understood everything that’s covered, so it’s my prior belief that parts will need to be explained in person.\n\n\nTo get the most out of this material, I recommend the following order:\n\nOccupancy Models: The basics\nLearn the basics of occupancy models and how they help us infer species presence/absence from imperfect detection.\nOccupancy Models: Covariates\nExplore how environmental or survey covariates can be incorporated to improve model accuracy and interpretation.\nBayesian models: The concept\nDive into Bayesian methods for occupancy modelling, allowing more flexibility and robust uncertainty estimation.\n\nEnjoy! (Maybe?)"
  },
  {
    "objectID": "index.html#suggested-order",
    "href": "index.html#suggested-order",
    "title": "Welcome to Occupancy Modelling",
    "section": "",
    "text": "To get the most out of this material, I recommend the following order:\n\nOccupancy Models: The basics\nLearn the basics of occupancy models and how they help us infer species presence/absence from imperfect detection.\nOccupancy Models: Covariates\nExplore how environmental or survey covariates can be incorporated to improve model accuracy and interpretation.\nBayesian models: The concept\nDive into Bayesian methods for occupancy modelling, allowing more flexibility and robust uncertainty estimation.\n\nEnjoy! (Maybe?)"
  },
  {
    "objectID": "OccMods_WithCovariates.html",
    "href": "OccMods_WithCovariates.html",
    "title": "Including Covariates in Occupancy Models",
    "section": "",
    "text": "The R packages\nAs before, this code uses spOccupancy, ggplot2 and patchwork for the analysis and visualisations.\n\n\nCode\nlibrary(spOccupancy)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n\n\n\nThe theory\nIn the previous document we went through a simple example to get a sense of how occupancy models work. We’re now going to begin the process of slowly increasing the complexity. To start, we’re going to need to include covariates (often called explanatory variables or independent variables or a wide variety of other names - it’s a mess).\nLet’s revisit our simple occupancy model:\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\n\\]\n\\[\nlogit(\\psi_i) = \\beta_0\\\\\n\\]\n\\[\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n\\]\n\\[\nlogit(p_{i,j}) = \\alpha_0\n\\]\nwhere the first two formula represent the state model (i.e. are our species present or absent from site \\(i\\)), and the last two form the detection model.\nWhat we’re going to do in this section is to add in covariates to these equations, starting with the detection model.\nThe first thing to note is that \\(y\\) is indexed by both \\(i\\) and \\(j\\), where \\(i\\) was the site and \\(j\\) was the survey. That \\(j\\) is important because it allows us to specify covariates that vary not just by site (e.g. site 1 has 20 trees while site 2 has 10 trees) but by survey as well (e.g. on survey 1 in site 1, the temperature was 20 degrees, 15 degrees in survey 2 and 5 degrees in survey 3). This means that anything that was different in one survey to the next can be accounted for and included in the model (so long as we can measure it).\nSpecifically, these “survey varying covariates” are features that we think may have made us (or whoever or whatever collected the data) to be more or less effective. Using cameras? Well maybe the presence of fog has a big impact on how likely we are to detect elephants. Doing surveys yourself? Well maybe the hour of day that you did the survey had a big impact. The point is, we can include these variables that deal with differences in detection probability.\nI’ll simulate a new dataset (with a bit more data for us to work with - 64 sites surveyed 3 times each) to show this off. We can say that the detection covariate in this case is rainfall, but note that the values for rainfall will be centred on zero (don’t worry about this).\nHere’s what the data looks like:\n\n\nCode\nset.seed(1234)\ndat &lt;- simOcc(J.x = 8, \n              J.y = 8, \n              n.rep = rep(3, times = 8 * 8), \n              beta = c(1), \n              alpha = c(-2, 0.5))\nobs &lt;- dat$y\ndet_cov &lt;- dat$X.p[,,2]\ndf &lt;- data.frame(\n  survey = rep(1:3, each = 64),\n  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),\n  y = c(obs[,1], obs[,2], obs[,3])\n)\n\np1 &lt;- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = cov)) +\n  geom_jitter(aes(x = factor(survey), y = cov),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Rainfall\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = cov),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Rainfall\", y = \"Elephant detection\") +\n  theme_minimal()\n\np1 + p2 + plot_annotation(tag_levels = \"A\", tag_suffix = \")\")\n\n\n\n\n\nNotice anything? In A) we can see that in each survey, there’s a lot of variation across the 64 sites in how much rainfall there was; maybe peaking in survey 2. In B), it looks like it might be more likely that we detect elephants more often when there’s more rainfall.\nAre you sure there’s an influence? If so, how strong is it? Exactly how strong is it?\nThere’s no way you can answer those questions. That’s where we need stats. So let’s add in rainfall to our model. I’ll recycle the code from the previous document and include rainfall. Here’s how we do that.\n\n\nData preparation\nJust like in the previous document, we need to include our different datasets into a list. If you’re still not sure what a list is in R, think of it like a folder on your computer. You can add lots of different files to a folder, but they’re all “tied” together by being within the same folder. That’s the same as a list in R.\nHere, I’m going to do something seemingly strange. I’m going to create a list and include this into our etosha list. The reason is that we might have more than one detection covariate, so having a list, though redundant here, will make adding additional variables easier in the future.\n\n\nCode\n# Note you wouldn't need to do the dat$X.p[,,2] bit\n# That's just because the data is simulated.\ndet.covs &lt;- list(rain = dat$X.p[,,2])\n\netosha &lt;- list(\n  y = dat$y,\n  det.covs = det.covs\n)\n\n\nAnd we’re good to go on to the modelling.\n\n\nFitting the model\nA few things to note in the R code;\n\nocc.formula = ~ 1 is the equivalent to \\(logit(\\psi_{i,j}) = \\beta_0\\)\n\nI.e. an intercept only occupancy model, meaning we are not including any covariates for the probability that a site is occupied.\n\ndet.formula = ~ rain is the equivalent to \\(logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i\\)\n\nI.e. we have both an intercept and slope given we have included rain in the model.\n\nWe specify that all data is contained in the list (i.e. “folder”) called etosha.\nThe remainder of the arguments (e.g. n.chains) can be ignored for now.\n\n\n\nCode\nfit &lt;- PGOcc(\n  # The state model (i.e. what % that elephants are present?)\n  # ~ 1 means we want an intercept only model (no covariates)\n  occ.formula = ~ 1, \n  # The observation model (i.e. what % that we see elephants if present?)\n  det.formula = ~ rain, \n  # Our carefully formatted dataset\n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n\n\n\n\nInterpreting\nHaving now fit the model to the data, we can see what we’ve learnt:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.017\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n\n\nCompared to the model in the previous document we now have additional information for Detection (logit scale); we have both an (Intercept) and rain. These are \\(\\alpha_0\\) and \\(\\alpha_1\\) from our detection model \\(logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i\\). If we really wanted to, we could now replace the parameter labels (e.g. the \\(\\alpha\\)s and \\(\\beta\\)s) with their now estimated values which would look like (rounding the estimates to two decimal points arbitrarily):\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\\] $$\n\\[\nlogit(\\psi_i) = 1.88\\\\\n\\]\n\\[\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n\\]\n\\[\nlogit(p_{i,j}) = -1.89 + 0.69 \\times Rain_i\n\\]\nWith this, we can swap out \\(Rain\\) for any value that we might be interested in, to see how our detection probability changes. For example, let’s see what happens when \\(Rain = 1\\).\n\n\nCode\n-1.89 + 0.69 * 1\n\n\n[1] -1.2\n\n\nOur logit value is -1.2. How do we get that into probabilities that we can actually understand? Backtransform out of logit using plogis():\n\n\nCode\nplogis(-1.89 + 0.69 * 1)\n\n\n[1] 0.2314752\n\n\nAnd we get a ca. 23% chance to detect an elephant when \\(Rain = 1\\). What about when \\(Rain = 0\\)? Well, we can do that easily enough now that we know thew general steps:\n\n\nCode\nplogis(-1.89 + 0.69 * 0)\n\n\n[1] 0.1312445\n\n\nWhen \\(Rain = 0\\), we predict a ca. 13% chance to detect elephants.\nThis approach, whereby we make a prediction for a specific value of \\(Rain\\) can be extended into making multiple predictions at once, such that we can then draw a line through them. Here’s how we’d do that.\n\n\nPlot predicted relationships\nWe start by creating a sequence of \\(Rain\\) values, rather than doing one a time. Here we use the seq() function to create a sequence, which will range from the minimum \\(Rain\\) value to the maximum within our dataset. The number of values that we want in this sequence is specified as 20 but we can choose any value here - we just need enough that the line is drawn “accurately”.\n\n\nCode\nrain &lt;- seq(from = min(det.covs$rain),\n            to = max(det.covs$rain),\n            length.out = 20)\n\nrain\n\n\n [1] -2.71815687 -2.44127881 -2.16440076 -1.88752271 -1.61064465 -1.33376660\n [7] -1.05688855 -0.78001049 -0.50313244 -0.22625439  0.05062367  0.32750172\n[13]  0.60437977  0.88125783  1.15813588  1.43501393  1.71189199  1.98877004\n[19]  2.26564809  2.54252615\n\n\nNow we have our values, we don’t want to manually enter each value into our equation. Instead we can use the fact that R works with vectors (i.e. columns of data) to do this quickly and easily:\n\n\nCode\npred &lt;- plogis(-1.89 + 0.69 * rain)\npred\n\n\n [1] 0.02263134 0.02726568 0.03281714 0.03945308 0.04736516 0.05677017\n [7] 0.06790956 0.08104689 0.09646267 0.11444547 0.13527876 0.15922259\n[13] 0.18649040 0.21722152 0.25145143 0.28908330 0.32986526 0.37337882\n[19] 0.41904309 0.46613767\n\n\nNow for each value of rain, we have the predicted probability of detecting an elephant. Useful but you wouldn’t want to throw these two columns at your audience/reader and expect them to make sense of it. It’d be better if we include these in a figure.\nTo do so, we’ll combine both columns into a single dataset and plot using ggplot2:\n\n\nCode\ndf &lt;- data.frame(\n  pred,\n  rain\n)\n\nggplot(df) +\n  geom_line(aes(x = rain, y = pred))\n\n\n\n\n\nFrom this figure it now appears much more intuitive that increasing rain makes elephants easier to detect. We can do a little “tidying” of the figure to make it more visually pleasing:\n\n\nCode\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n\n\n\n\n\n\n\nUncertainty\nThe above figure is a good start but we’re missing any measure of uncertainty. If we were doing frequentist statistics, we would use 95% confidence intervals but these are exclusively frequentist. There are no 95% confidence intervals when we use the Bayesian statistical framework. Instead we have credible intervals. I’ll explain the Bayesian framework in a subsequent workflow but for now here is the formal definition of a credible interval:\n\nThere is a 95% probability that the True parameter value lies within the interval range, given the data and model.\n\nThis is in contrast with the frequentist confidence interval whose formal definition is so bizarre and unintuitive that it’s barely useful. 95% credible intervals are useful and work exactly the way people think frequentist intervals work.\nBut how do we include these in the figure? Well, the model summary makes it easy to find the values:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.017\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n\n\nThey’re the 2.5% and 97.5% values in our summary table. So all we need to do is repeat our predicions using these values to get out measure of uncertainty to include in the figure. Importantly, if you remember back to BI3010 where you had to multiple standard error by 1.96, we do not need to do that here. That’s frequentist nonsense - we’re Bayesian now.\nLet’s do just that:\n\n\nCode\ndf$low &lt;- plogis(-2.4610 + 0.2328 * df$rain)\ndf$upp &lt;- plogis(-1.2939 + 1.1801 * df$rain)\ndf\n\n\n         pred        rain        low        upp\n1  0.02263134 -2.71815687 0.04336427 0.01096960\n2  0.02726568 -2.44127881 0.04611831 0.01514457\n3  0.03281714 -2.16440076 0.04903828 0.02087495\n4  0.03945308 -1.88752271 0.05213304 0.02871039\n5  0.04736516 -1.61064465 0.05541172 0.03936862\n6  0.05677017 -1.33376660 0.05888379 0.05376451\n7  0.06790956 -1.05688855 0.06255900 0.07302436\n8  0.08104689 -0.78001049 0.06644741 0.09846565\n9  0.09646267 -0.50313244 0.07055932 0.13151304\n10 0.11444547 -0.22625439 0.07490526 0.17351714\n11 0.13527876  0.05062367 0.07949599 0.22545433\n12 0.15922259  0.32750172 0.08434241 0.28752905\n13 0.18649040  0.60437977 0.08945559 0.35877811\n14 0.21722152  0.88125783 0.09484663 0.43685701\n15 0.25145143  1.15813588 0.10052670 0.51819600\n16 0.28908330  1.43501393 0.10650691 0.59858193\n17 0.32986526  1.71189199 0.11279825 0.67399363\n18 0.37337882  1.98877004 0.11941156 0.74135968\n19 0.41904309  2.26564809 0.12635738 0.79895748\n20 0.46613767  2.54252615 0.13364590 0.84638633\n\n\nAnd we can add in our uncertainty using geom_ribbon():\n\n\nCode\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  geom_ribbon(aes(x = rain, ymin = low, ymax = upp),\n              alpha = 0.3) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n\n\n\n\n\nWith that, we have a publication ready figure.\n\n\nMultiple covariates\nLet’s increase the complexity a bit and have the simulation include multiple covariates. We’ll say that tree height and average temperature affect whether or not a site is occupied (elephants will like tall trees and cooler locations). We’ll still have rain affect our detection probability, as above.\nTo create the figures below we need to do some tweaks to the data. The df object I create has one row per survey, with three surveys per site. Our occupancy covariates, tree and temp have just one value; these do not change from one survey to the next. If a tree is 3 m tall in survey one, then it’ll still be 3 m tall in surveys two and three.\nA note here is that the covariates are still centered on zero. That’s just the way the data is simulated but the data you collect does not need to be the same (so ignore the fact that we will have trees that are -1 m tall - the general idea doesn’t change).\n\n\nCode\nset.seed(1234)\ndat &lt;- simOcc(J.x = 8, \n              J.y = 8, \n              n.rep = rep(3, times = 8 * 8), \n              beta = c(1, -0.2, 0.3), \n              alpha = c(-2, 0.5))\nobs &lt;- dat$y\ntemp &lt;- dat$X[,2]\ntree &lt;- dat$X[,3]\ndet_cov &lt;- dat$X.p[,,2]\ndf &lt;- data.frame(\n  survey = rep(1:3, each = 64),\n  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),\n  tree = rep(tree, times = 3),\n  temp = rep(temp, times = 3),\n  y = c(obs[,1], obs[,2], obs[,3])\n)\n\np1 &lt;- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = temp)) +\n  geom_jitter(aes(x = factor(survey), y = temp),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Temperature\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = temp),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Temperature\", y = \"Elephant detection\") +\n  theme_minimal()\n\np3 &lt;- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = tree)) +\n  geom_jitter(aes(x = factor(survey), y = tree),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Tree\") +\n  theme_minimal()\n\np4 &lt;- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = tree),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Tree\", y = \"Elephant detection\") +\n  theme_minimal()\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\nThere are two things worth highlighting from the above figures. Firstly, the boxplots on the right are identical for each of the three surveys (the points are jittered so are randomly placed but trust me that these are identical). Secondly, it becomes quite hard to see any clear pattern in the two right hand figures. Keep in mind that the zeros here are actively misleading us. Some of the zeros are genuine in that there were no elephants there and tree and temp likely caused that (which we only know because the data is simulated) but the other zeros are false negatives; The elephants were there, we just didn’t see them.\nThis is why we need a model to figure out what the relationships are. We can no longer trust our eyes to see the pattern.\nSo let’s get our data organised such that we can fit our model.\n\n\nData preparation\nAs before, we provide our detection covariates as a list but XXX\n\n\nCode\ndet.covs &lt;- list(rain = dat$X.p[,,2])\nocc.covs &lt;- data.frame(tree = tree, temp = temp)\netosha &lt;- list(\n  y = dat$y,\n  det.covs = det.covs,\n  occ.covs = occ.covs\n)\n\n\n\n\nFitting the model\nThe core model we’re going to fit is:\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\nlogit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\nlogit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\n\\]\nThe main thing that I want to highlight here, other than having included \\(Tree\\) and \\(Temp\\) as effecting occupancy probability (\\(\\psi\\)), is that the subscripts are different.\nIn the occupancy model (\\(logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\)) the variables are subscript by \\(i\\), indicating that we have one value of \\(Tree\\) or \\(Rain\\) for each site \\(i\\).\nBut in the detection model (\\(logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\)) \\(Rain\\) is subscript by both \\(i\\) and \\(j\\). That’s because we have a different \\(Rain\\) value for each survey. This would be something we record every time we visit the site (or extract from some online source for each day).\nKeep this in mind when you’re collecting your own data. Variables that you think affect occupancy will have one value per site. Variables that you think affect detection will (generally) have one value per survey.\nTo fit this is relatively straight forward. For the occupancy model, we write occ.formula = ~ tree + temp, and for the detection model we write det.formula = ~ rain, and let the machinery do it’s thing.\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n\n\nOnce it’s run, we can check the summary() to see what the parameters were estimated as:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.019\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.9424 1.0250  0.1512  1.8670 4.0917 1.0187 347\ntree         0.3904 1.3242 -2.0614  0.3232 3.0704 1.0196 267\ntemp        -1.2186 1.0959 -3.2786 -1.2776 1.1046 1.0246 267\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8408 0.2706 -2.3623 -1.8447 -1.2875 1.0066 1117\nrain         0.1666 0.2196 -0.2592  0.1611  0.6073 1.0038 3054\n\n\nWe see that tree has a positive effect on occupancy and temp has a negative effect. This is how the simulation was carried out, but the values do not match up particularly well. Specifically, in the simulation tree was set to 0.3 but has been estimated as 0.2, while temp was set to -0.2 but has been estimated as -1.2.\nSo we’ve got the general relationships reasonably well estimated but it’s not particularly accurate. This is where having credible intervals is useful. Keep in mind that credible intervals are, correctly, interpreted as having a 95% chance of containing the “True” value (the True value here is what each parameter was set to in the simulation). For both tree and temp the True parameter value is indeed within the 95% credible intervals! Both 95% CI are wide, but that’s good here! The model isn’t entirely sure what the values are (partly because of sample size and the effects being subtle) and that is being conveyed to us! We’re not deluding ourselves into thinking we have a perfect understanding when we don’t!\nThere are some other warning signs that the model might not be performing especially well. The first is that the Rhat values are getting uncomfortably large for the Occurence parameters. My personal rule of thumb is Rhat values close to 1.05 is where I get worried. None of our parameters are at 1.05 but they’re close enough that I’m paying attention to them and want to see if I can fix the problem.\nThe other warning sign is the ESS for the Occurence model are all “low”. At least noticeably lower than the Detection model parameters. Again, the rule of thumb here is that we want hundreds or thousands of “Effective Sample Size”, so we’re technically OK but I’m still concerned.\nNow, to be clear, we ignored these issues with the first model we ran in this document, we here we’re going to see if we can’t resolve this problem.\n\n\nResolving issues\nTo fully appreciate the solution we’ll attempt requires a deeper understanding of Bayesian statistics. But for now, we’re not going to cover this. Instead, I’ll simply say that the “machinery” of Bayesian statistics revolves around giving a number of algorithms (called “chains”) a number of guesses (called “iterations”) to try and figure out the most likely values for our parameters.\nThe two metrics we used above, Rhat and ESS, both monitor these chains and iterations and let us know if they are not in agreement. When Rhat values get high, and ESS values get low, it can suggests we simply need to give the algorithms more iterations to figure out the best values.\nInevitably, the details are more complex than I can convey in two paragraphs but the general idea is there. If either number looks a bit worrying, it’s relatively cheap and easy to just rerun the model with more iterations and see if that solves the problem.\nLet’s do just that by increasing the number of iterations to 3000 per chain:\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # We're using four chains/algorithms\n  n.chains = 4,\n  # We allow 3000 guesses (increased from 2000)\n  n.samples = 3000,\n  # We ignore the first 300 (increased from 200)\n  # We ignore them because we assume the algorithms are not particularly reliable\n  # in the first ca. 10% of guesses\n  n.burn = 300,\n  verbose = FALSE)\n\n\nOnce run, we can check how well they ran:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 3000, verbose = FALSE, n.burn = 300, n.chains = 4)\n\nSamples per Chain: 3000\nBurn-in: 300\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 10800\nRun Time (min): 0.027\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0655 1.0246  0.1977  2.0138 4.2114 1.0010 529\ntree         0.3721 1.3145 -2.0763  0.3028 3.0319 1.0662 375\ntemp        -1.2063 1.1214 -3.3214 -1.2363 1.1050 1.0220 422\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8530 0.2644 -2.3777 -1.8521 -1.3301 1.0020 1914\nrain         0.1715 0.2210 -0.2506  0.1682  0.6072 1.0004 4390\n\n\nNow when we look at the Rhat and ESS values, we doing better. Still not perfect but enough for me to be happy that by these two metrics alone there’s nothing to suggest the machinery is struggling and that the parameters are being estimated as robustly as possible.\n\nImportantly, these are not the only metrics or tools available to us. We’ll check out the other options in a later document.\n\nAs far as we can tell, for now, we can trust this model and move on to plotting the predicted relationships.\n\n\nPlot predicted relationships\nPreviously, we made the predictions ourselves by “hand”. This time, we’re going to use the predict() function to do this for us to make life easier on ourselves. Importantly, the process is exactly the same;\n\nCreate a “fake” dataset with the covariate values we want to predict for.\nApply this fake data to our equation, with the now estimated parameter values, to generate the predictions.\nConvert from logit values to probabilities.\nPlot the predicted probabilities against the fake covariate values to show the estimated relationship.\n\n\n\nCode\nX.0 &lt;- cbind(\n  1,\n  temp = seq(from = min(occ.covs$temp), \n             to = max(occ.covs$temp),\n             length.out = 50),\n  tree = 0\n)\n\n#predict(fit, X.0)\n\n\n\n\nHow should I store my data?\nIn order to be able to run the model, we need the data to be in a particular order"
  },
  {
    "objectID": "Occupancy_Models.html",
    "href": "Occupancy_Models.html",
    "title": "Introduction to Occupancy Models",
    "section": "",
    "text": "Where are species present and why are they present?\nA fundamental question in ecology is: where are species present, and why? For example, why are elephants present in the north of Etosha nature reserve in Namibia, but not in the south of the reserve? Is it due to water availability? Food? Could predators play a role?\nThe best tool available for answering these types of questions is occupancy modeling, originally developed by Daryl MacKenzie et al in 2002. This modelling framework built on concepts already developed for estimating survival of individual animals, called the Cormack-Jolly-Seber model (note that Cormack and Jolly independently developed this model while they worked in Aberdeen University - a vital and internationally renowned model was developed right here in bloody Aberdeen!)\nThe problem that occupancy models solve is subtle but insanely crucial.\nAssume I want to determine where elephants are present in Etosha nature reserve in Namibia. I do a bunch of surveys where I go to various sites throughout the park. Whenever I spot an elephant I note down that elephants are present at that location. When I don’t see any elephants I also record that. My dataset might look something like:\n\n\nCode\nset.seed(1988)\netosha &lt;- data.frame(\n  site = 1:10,\n  elephants = rbinom(n = 10, size = 1, prob = 0.4)\n)\netosha\n\n\n   site elephants\n1     1         0\n2     2         0\n3     3         0\n4     4         0\n5     5         0\n6     6         0\n7     7         0\n8     8         1\n9     9         1\n10   10         0\n\n\nWhen elephants is 1, then I saw them at that site, and when elephants is 0, then I didn’t see them.\nTo figure out the probability that a site is occupied, I can run a Bernoulli Generalised Linear Model. The Bernoulli distribution (named after Jacob Bernoulli) is a type of distribution that will generate (or expect) values of 1 or 0; perfect here because our data can only be 1 or 0.\nThe model would be:\n\\[\ny_i \\sim Bernoulli(p_i) \\\\\n\\]\n\\[\nlogit(p_i) = \\beta_0\n\\]\nwhere:\n\n\\(y\\) is our observation (elephants in the etosha dataset)\n\\(i\\) is the index, here being which site the data was collected from\n\\(\\sim\\) means “generated according to” (or “our data is the same as would be generated by the following distribution”)\n\\(Bernoulli\\) is a type of distribution that will generate either 0 or 1\n\\(p\\) is the probability of success (i.e. there is \\(p\\) probability that we see an elephant). We can’t possibly know what \\(p\\) is when we collect the data, so we need to figure it out with statistics. (The \\(i\\) means each site could have a different probability - but we’re not doing that yet)\n\\(logit\\) is the link function to ensure that \\(p\\) remains between 0% and 100%. Specifically, it’s a little bit of maths: \\(log(\\frac{p}{1-p})\\), which is the natural log of the probability to succeed (\\(p\\)) divided by the probability to fail (\\(1-p\\)).\n\\(\\beta_0\\) is the intercept which here, given we have nothing else in this part of the model, means the average probability to see an elephant.\n\nHere’s how we’d run that model in R (click the Show button to see the code):\n\n\nCode\nmod &lt;- glm(elephants ~ 1,\n           data = etosha,\n           family = binomial)\n\n\nWhich returns the estimate of \\(\\beta_0\\) (or average probability to detect elephants on the link function scale, i.e. it’s a logit value):\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nglm(formula = elephants ~ 1, family = binomial, data = etosha)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -1.3863     0.7906  -1.754   0.0795 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10.008  on 9  degrees of freedom\nResidual deviance: 10.008  on 9  degrees of freedom\nAIC: 12.008\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe estimate for (Intercept) (or \\(\\beta_0\\)) is -1.3863, which we can convert into a probability by doing the inverse logit (R has a nice way to do this using the function plogis()):\n\n\nCode\nplogis(-1.3863)\n\n\n[1] 0.1999991\n\n\nWe now have our estimate as a proportion (which we can multiply by 100 to get to percentage), so we can say there’s a roughly 20% chance that an elephant occupies a site in Etosha.\nOr can we?\n\n\nImperfect detection\nThis 20% estimate is actually the product of two probabilities. For us to detect an elephant two things need to happen.\n\nThe elephant must obviously be there, with some probability. We can call this probability \\(\\psi\\) (called “psi”, pronounced like “sigh”).\nI have to see the elephant. This will never have a 100% chance no matter how good I am at spotting elephants. We can call this probability \\(p\\).\n\nSo all of the 1’s in our etosha dataset are the result of succeeding in both of these probabilities. However, all of the 0s are the result of failing either probability; either there were no elephants (\\(1-\\psi\\)), or there were you just didn’t see them (\\(\\psi \\times 1-p\\)). Fundamentally, the problem is that elephants represents \\(\\psi \\times p\\), and not just \\(\\psi\\) like we want.\nThere’s no shortage of reasons why you might not see an elephant despite it being there. It can be as simple and dumb as the elephant was behind a bush when I checked that site. This reflects something called “imperfect detection”, i.e. just cause it’s there doesn’t mean we’re going to see it.\nImperfect detection is not an issue for the 1’s in our data - when we see an elephant. If you see an elephant, you know it’s there. That’s easy and obvious. The complication created by imperfect detections happens when we don’t detect an elephant; when elephants is 0. What does that mean? Are elephants actually absent from that site or are they present but I didn’t see them?\nThat’s the problem! The 0’s have multiple meanings. It’s not as simple as “if I don’t see an elephant, there are not elephants there”.\nGenerally, when people say they ran an SDM (a “Species Distribution Model”), what they mean is, they ran something akin to the GLM above. Specifically, they would treat the elephants column in our etosha dataset as being a faithful measure of elephants being either present or absent, and not elephants are present and detected or just absent.\nThey ignore imperfect detection; they ignore the fact that 0’s have multiple meanings.\nOccupancy models do not ignore this, and that’s why they’re such a powerful tool. Here’s how they do it.\n\n\nOccupancy models\nThe first part of the occupancy model (which I’ll call the “state model” because it’s trying to determine the “state” of a site - either occupied or not) looks remarkably similar to the GLM above:\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\nlogit(\\psi_i) = \\beta_0\n\\]\nThey look similar, because they’re both \\(Bernoulli\\) GLMs! But they differ in two important ways,\n\n\\(z\\) is the true presence or absence of elephants in site \\(i\\)\n\\(\\psi\\) is the probability to be present\n\nOk, so the labels have changed, but how does that magically solve the problem of imperfect detection? Well, if we left it there it wouldn’t be solved. We need something that will deal with impefect detection; a second GLM.\nThe second GLM (which I’ll call the “observation model”) also looks remarkably similar:\n\\[\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\nlogit(p_{i,j}) = \\alpha_0\n\\]\nIt’s yet another \\(Bernoulli\\) GLM but with some really important changes.\n\n\\(y\\) is now the detection or not of an elephant\n\\(j\\) is survey, which means we have multiple surveys, not just one like in our first GLM example above\n\\(p\\) is the probability to detect an elephant at site \\(i\\) in survey \\(j\\) (e.g. what is the probability to detect an elephant in site 1 in the third survey?)\n\nImportantly, the probability to detect elephants (\\(p\\)) is multiplied by \\(z\\). What’s \\(z\\)? Well that’s the true occupancy state of that site from the first GLM. It’s this multiplication that allows the two models to “speak” to each other.\nIf there are elephants in a site (\\(z = 1\\)), then \\(p \\times 1 = p\\). If elephants are absent from a site (\\(z = 0\\)), then \\(p \\times 0 = 0\\). This means you cannot detect elephants if they aren’t there. That’s blindingly obvious… But this stupidly simple logic is missing from our starting GLM!\nKeep in mind that we don’t know \\(z\\) - that’s the “true” occupancy state of a site. It’s something called a “latent variable”, meaning we never actually measure or record this in the field. Instead, much like with parameters, we use the data we have collected to estimate this variable for each site that we have data from.\nThat’s how occupancy model knows that you can only detect elephants if they’re present, and if they aren’t present then you can’t see them! That’s the beauty of occupancy models and why they are far, far superior to traditional SDMs!\n\n\nThe robust design and closure\nOk, great, we’ve got a clever modelling framework but how does the above help us resolve if we go to a site and don’t see an elephant? How do we distinguish between a “true negative” (i.e. we don’t see elephants because there aren’t any elephants) versus a “false negative” (i.e. we don’t see elephants but they were there)?\nThis is where the subscript \\(j\\) in the observation model becomes important. Imagine we visit a site once and we see no elephants. With no additional information you have absolutely no way to determine if it was occupied or not. You just know you didn’t see any elephants.\nBut imagine I went back to that same site the next day. This time I do see elephants. We learn a few things from this. First, we know elephants are present at the site. Secondly, we learn that the first survey must have been a false negative - we just didn’t see elephants despite them being there.\nThis introduces a key assumption in occupancy models. For my above statement to be “true” and make sense, then I have to assume elephants were always present, and they didn’t just happen to move into the site on the second day.\nThis assumption is called the “assumption of closure”. This is often poorly understood, where people interpret it as meaning the site is physically “closed” - as in there is some kind of “fence” that prevents animals from moving. This is not correct. “Closure” here, means “demographically closed”, i.e. if the species was present in survey one, then it is present in survey two and survey three, survey four, and so on, until you stop surveying.\n\nThis imposes two important considerations for field work. We need to survey sites on multiple occasions. Meaning, we can’t have a “one-and-done” approach to surveying; we need to leave cameras in situ for more than one sampling period (though it’s for us to define what our sampling period is - it could be an hour, a day, a week - so long as we can assume that if the species is present at the first survey, then it will be present at the last survey). Secondly, these sampling occasions cannot extend over such a long period of time that it becomes increasingly hard, or unreasonable, to assume the species could not have gone locally extinct or (re)colonised the area.\n\nIn practice, a minimum of three survey occasions is required (e.g., three days, three hours, or three defined sampling periods). But additionally, you don’t want to monitor the same site for so long the species could go locally extinct (obviously dependent on the species you’re working with). If such local extinctions and recolonisations are likely at a site, then you can extend from the single season occupancy models (the type of occupancy model that we’re talking about here) into multi-season occupancy models.\nThis form of sampling (repeatedly sampling the same site a minimum of three times) is called “the robust design” and is very often used in more “robust” (i.e. responsible or trustworthy) research.\n\n\nThe data\nI’m going to simulate data for an occupancy model to give you an idea of how you data should be organised. To do so, I’ll use the R package that you’ll eventually use in your own analysis, spOccupancy (short for spatial occupancy - we’ll get to the spatial part later).\n\n\nCode\nlibrary(spOccupancy)\nset.seed(1234)\ndat &lt;- simOcc(J.x = 5, J.y = 2, n.rep = rep(3, times = 10), beta = c(1), alpha = c(0))\nobs &lt;- dat$y\n\n\nOur dataset will look slightly different. Since the robust design requires monitoring each site multiple times (typically three or more), we cannot use a single column for detections as we did in the etosha example. Instead, here would data will look like:\n\n\nCode\nobs\n\n\n      [,1] [,2] [,3]\n [1,]    0    1    0\n [2,]    0    0    1\n [3,]    0    0    0\n [4,]    0    0    0\n [5,]    0    1    1\n [6,]    1    1    0\n [7,]    1    0    1\n [8,]    0    0    0\n [9,]    0    1    0\n[10,]    0    1    1\n\n\nEach row is one of our ten sites, and each column is one of our three sampling periods. Notice anything? In site 3, or [3,], and sites 4 and 8, we never see elephants. Without any further information, we don’t know if this is because elephants weren’t in those sites, or they were present but we didn’t see. Same problem as before.\nHowever, we clearly have sites where elephants are present. In sites 1, 2, 5, 6, 7, 9, and 10, we see elephants at least once, so we know for sure that they are present there. But! We can also use these detections to help us figure out what’s likely going on in those sites where we never saw elephants.\nTo see how that works, let’s focus on just site 5, or [5,]. Over the three days that we surveyed, we didn’t see any elephants on day 1, we then saw them on day 2, then again on day 3. Are elephants present at this site? Yes, clearly. We see them at least once but there’s additional information we learn from having used a robust design (surveying the same site multiple times).\nIgnore occupancy models for a moment. For the detection history of site 5 (not detected, detected, detected, or 011), we might guess that our detection probability is about 66% given we have two detections out of three surveys. If so, then we can think of this as being the equivalent to us having a 66% chance of detecting elephants on any survey (importantly, if they’re present).\nArmed with this 66%, we can figure out how likely it is that elephants were actually absent from site 3 and 4. Let’s assume elephants were present at site 3. What’s the probability that we would fail to detect them? Well, if there’s a 66% chance to detect elephants, then failure to detect is \\(100\\% -66\\% = 34\\%\\) (or \\(1-0.66=0.34\\)). So a 34% chance not to detect an elephant even if it’s there. What about two days in a row? \\(0.34 \\times 0.34 = 0.12\\), or \\(0.34^2 = 0.12\\), - about 12% chance to not detect an elephant two days in a row if it was there. Three days? \\(0.34^3 = 0.04\\), or 4% chance that, if there was an elephant in site 5 that we’d fail to detect it, three days in a row.\nGiven a measly 4% chance to completely miss elephants on all three days, it seems more likely that we didn’t fail to detect, but that there was no elephant there. Since this is a simulation, we can verify our estimate by checking the true occupancy status of site 3.\n\n\nCode\ndat$z[3]\n\n\n[1] 0\n\n\nWe see that elephants were actually not present in site 5 (above should read [1] 0 which is absent). Our guess was correct - it was more likely that they weren’t there and we literally could not see them, rather than they were there and we just got very unlucky.\nThat’s kind of how occupancy models work, except that rather than using a single site to estimate detection probability, we use all data. Here’s how we do the analysis more formally.\n\n\nData preparation\nBefore getting to running a fancy * Bayesian * model, we need to get our dataset organised in a way that works for spOccupancy. Contrary to linear models and GLMs, we don’t provide a simple data.frame object. Instead, we provide a list, which itself can, and eventually will, contain multiple data sets.\nFor the starting model we’ll fit below, we only need our detection history “matrix” to be added to this list (which I’ll call etosha). That detection history matrix is the data we saw above (shown again below). So for now, we’re basically taking our data set and adding it to a list. A bit of a pain, given it’s a seemingly unnecessary step, but it’s needed for the more complex versions of the model.\n\n\nCode\ndat$y\n\n\n      [,1] [,2] [,3]\n [1,]    0    1    0\n [2,]    0    0    1\n [3,]    0    0    0\n [4,]    0    0    0\n [5,]    0    1    1\n [6,]    1    1    0\n [7,]    1    0    1\n [8,]    0    0    0\n [9,]    0    1    0\n[10,]    0    1    1\n\n\nThe code to create the etosha list is actually relatively straightforward. The function list() will create the list, and we can just add our detection history (currently stored as dat$y) and have it be called y (for the reason that we call it \\(y\\) in our equations above). If you want to see the code, press the Show button to the right.\n\n\nCode\netosha &lt;- list(\n  y = dat$y\n)\n\n\n\n\nFitting the model\nBelow we’re going to fit the simplest version of an occupancy model that we can, specifically:\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\nlogit(\\psi_i) = \\beta_0\\\\\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\nlogit(p_{i,j}) = \\alpha_0\n\\]\nThis version of the model only has two average probabilities. One for the probability that elephants occupy a site (\\(\\beta_0\\) which is fit on the \\(logit\\) link function), and another to detect elephants if they are present (\\(\\alpha_0\\), also fit on the \\(logit\\) link function).\nThe model is not fit using the so-called “frequentist framework”. Instead, it uses the Bayesian framework. I’ll gloss over what these are and what the differences are, other than to say that maybe 95% (for all the NHST lovers) of science uses the frequentist framework. Not by coincidence, it’s also the framework that is generally taught to students (hence why most people use it).\nThe code below uses the PGOcc() function to fit a simple single species, single season occupancy model with no explanatory variables. Click the Show button to see the code.\n\n\nCode\nfit &lt;- PGOcc(\n  # The state model (i.e. what % that elephants are present?)\n  # ~ 1 means we want an intercept only model (no covariates)\n  occ.formula = ~ 1, \n  # The observation model (i.e. what % that we see elephants if present?)\n  # ~ 1 means the same as above - intercept only\n  det.formula = ~1, \n  # Our carefully formatted dataset\n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n\n\nHaving fit the model, we can inspect the parameter estimates:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~1, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0025\n\nOccurrence (logit scale): \n              Mean     SD    2.5%    50%  97.5%   Rhat  ESS\n(Intercept) 1.4601 0.9914 -0.1904 1.3426 3.7274 1.0065 1576\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat  ESS\n(Intercept) -0.1952 0.4643 -1.0869 -0.2025 0.7266 1.0039 3274\n\n\nThere’s a lot of information in this summary but we only really care about a few things.\nOccurrence (logit scale)\n\nThis is the state model (i.e. what’s the probability that an elephant is present in any site?)\nThe (Intercept) row contains all of the information on the… intercept. Because we only have an intercept, we only have an intercept row.\nMean is the mean of the “posterior”. Crudely, this is Bayesian terminology for the “parameter” (it’s a bit more complicated than that, but we’re leaving the Bayesian stuff for later).\nSD is the standard deviation of the posterior.\n2.5% is the lower 95% credible interval (this is not a confidence interval but you can think of them as being the same for now).\n50% is the median of the posterior.\n97.5% is the upper 95% credible interval.\nRhat will be entirely new to you. This is a measure of whether or not there were any problems in figuring out the posterior. If the Rhat value is close to 1, then it suggests that it could not find any problems (note this does not mean there are no problems, just that it couldn’t find any). If the value goes above, say 1.1, then it suggests the model is not “convinced” that it’s found the best posterior estimate. In that case, you would need to make some tweaks to the model.\nESS stands for “Effective Sample Size”. This is another new Bayesian thing that I’ll gloss over the details of but for now, we want this value to be in the multiple hundreds or thousands. Having “too low” an ESS suggests problems that may require tweaks to the model.\n\nIn our case, all looks good. The Rhats are generally very close to 1 and the ESS are all in the thousands.\n\n\nInterpreting the results\nSo what do our results mean? For the state model (that spOccupancy calls Occurence), our mean is estimated as 1.4601 but we would surely expect this to be a probability, right? Keep in mind, we’re fitting fancy Bernoulli GLMs, and with our Bernoulli GLMs we’ve used the logit link. So the 1.4601 value is on the logit link scale.\nWe need to convert this logit value to probabilities ourselves using the same plogis() function that we used before:\n\n\nCode\nplogis(1.4601)\n\n\n[1] 0.811548\n\n\nSo we have a roughly 80% chance to have elephants in any of our sites. If you have a look at our actual data set, you’ll see we don’t have detections in 80% of the sites. We have detections in 70% of the sites. That’s where the detection part of the model is helping us out - we don’t have 100% detection probability, so we should find that we have more sites occupied than we would expect by default.\nWhat was our detection probability? The mean of the intercept is estimated (as a logit value) at -0.1952. We know how to back transform this into probabilities:\n\n\nCode\nplogis(-0.1952)\n\n\n[1] 0.4513544\n\n\nThe model has estimated that we have roughly 45% chance of seeing an elephant if it’s actually there. If we do have a 45% detection probability, then the chance of not seeing an elephant in sites 3, 4 or 8 is \\(0.45^3 = 0.09 = 9\\%\\). Not huge, but not trivial either.\nMost importantly, we learn that detection probability is not 100%. That alone is important. The consequence of that is to show, quite clearly, why the Bernoulli GLM approach we used at the start is not appropriate. Because it ignored imperfect detections, it’s unreliable. Our occupancy model is not.\n\n\nWhat next?\nThere are three big elements this document has not covered.\nThe first is Bayesian statistics. Summarised crudely, Bayesian statistics allows statisticians to include their (or others) understanding of the world into the model. What does this mean specifically? Well, we might have had some idea of what a sensible range of probabilities that elephants were present in any of our sites. Maybe we think 20% to 90% is sensible, i.e. we don’t think it’s possible that there aren’t any elephants present or all sites are occupied by elephants. These “guesses” are formally called “priors”, and these are the hallmarks of Bayesian statistics.\nThe second is including covariates. Just like in the modelling you did in BI3010, we can add anything that we think is playing a role in either detection of elephants or why elephants are there. Do you think rainfall influences how likely people are to detect elephants? Well, include rainfall in the detection model. Like I say, in exactly the same way as you did in BI3010!\nThe last one is spatial autocorrelation. Imagine one site is elephant utopia. It has everything an elephant could ever want. This site is almost certainly occupied. What about a site right next to it? This next door site is no utopia, so in isolation we would expect a lower occupancy probability, but because it’s right next to utopia, it’s probably more likely to be occupied than the local features of the site would suggest. This is spatial autocorrelation. The occupancy (or whatever it is we’re measuring in space) of one site influences the occupancy of sites depending on how close they are to each other in space. If we don’t account for spatial autocorrelation, then we might think there’s some feature of the non-utopian elephant site that is more attractive to elephants than it actually is; leading to flawed inference."
  }
]