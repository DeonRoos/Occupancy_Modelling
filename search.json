[
  {
    "objectID": "Occupancy_Models.html",
    "href": "Occupancy_Models.html",
    "title": "Introduction to Occupancy Models",
    "section": "",
    "text": "A fundamental question in ecology is; Where are species present, and why are they present there?\n\nZitong, in your case this may be; Why are roe deer present at site A but not site B? Is it because site A is closer to roads? If a site is close to a road what’s the probability roe deer are present? Is that very different to sites that are far away from a road?\nWhat about features of the habitat? Are roe deer more likely if a site is forested versus agricultural?\n\n\nJoe, in your case this may be; Why are pheasants present at site A but not site B? Is it related to how far away the nearest known release pen is? What does the relationship look like? If there is no relationship, what would that mean for policy?\nWhat about the habitat makes pheasants more likely to occupy that site?\n\nTo act as motivation for this intro to occupancy models, let’s say we’re interested in understanding why are elephants present in the north of Etosha nature reserve in Namibia, but not in the south of the reserve? (As a side note, I’d recommend going there on holiday if you ever get the chance). Are elephants present in some parts of Etosha due to water availability? Is it related to food? Could predators play a role?\nTo find out, we needs stats. As useful as gut feelings and intuition are, it’s not enough. Remember, statistics is how we get the evidence to answer our research question using data.\nThe best statistical tool available for answering these types of questions are “occupancy modeling”. These were originally developed by Daryl MacKenzie et al in 2002 who wanted to deal with a subtle issue when trying to find animals (more on this in a second). The broad concept underlying MacKenzie’s occupancy modelling framework built on ideas that had already been developed for estimating survival of individual animals, called the Cormack-Jolly-Seber, or CJS, model. As a cool note; Cormack and Jolly independently developed this model while they worked in Aberdeen University - a vital and internationally renowned model was developed right here in bloody Aberdeen! In my entire time as an undergradute student in Aberdeen no one ever told me this, even though I was using these models during my PhD! I only found out when I was at a conference in France. We, as a school, should be far more proud of this than we are.\nThe problem that occupancy models solve is subtle but insanely crucial.\nAssume I want to determine where elephants are present in Etosha nature reserve in Namibia. I need data to answer this, so I decide to do ten elephant surveys in Etosha. Specifically, I go to visit these ten sites that are nicely spread throughout the park. At each site, whenever I spot an elephant I note down that elephants are present at that location. When I don’t see any elephants I record that elephants are absent at that location. Simple, right?\nMy data set for these ten sites and surveys might look something like:\n\n\nCode\nset.seed(1988)\netosha &lt;- data.frame(\n  site = 1:10,\n  elephants = rbinom(n = 10, size = 1, prob = 0.4)\n)\netosha\n\n\n   site elephants\n1     1         0\n2     2         0\n3     3         0\n4     4         0\n5     5         0\n6     6         0\n7     7         0\n8     8         1\n9     9         1\n10   10         0\n\n\nThe only thing to note here is that when elephants is 1, then that means I saw them at that site. When elephants is 0, then I didn’t see them. 1 means Present, 0 means Absent (at least for now).\nTo figure out the average probability that any one of my ten sites are occupied, I can run a Bernoulli Generalised Linear Model. The Bernoulli distribution (named after Jacob Bernoulli) is a type of distribution that will generate (or expect) values of 1 or 0; perfect here because our data can only be 1 or 0.\nThe model would be:\n\\[\ny_i \\sim Bernoulli(p_i) \\\\\n\\]\n\\[\nlogit(p_i) = \\beta_0\n\\]\nLet’s go through these equations slowly:\n\n\\(y\\) is our observation (elephants in the etosha dataset)\n\\(i\\) is the index, here being which site the data was collected from\n\\(\\sim\\) means “generated according to” (or “our data is the same as would be generated by the following distribution”)\n\\(Bernoulli\\) is a type of distribution that will generate either 0 or 1\n\\(p\\) is the probability of success (i.e. there is \\(p\\) probability that we see an elephant). We can’t possibly know what \\(p\\) is when we collect the data, so we need to figure it out with statistics. (The \\(i\\) means each site could have a different probability - but we’re not doing that yet)\n\\(\\beta_0\\) is the intercept which here, given we have nothing else in this part of the model, means the average probability to see an elephant.\n\\(logit\\) is the link function to ensure that \\(p\\) remains between 0% and 100%. Specifically, it’s a little bit of maths: \\(log(\\frac{p}{1-p})\\), which is the natural log of the probability to succeed (\\(p\\)) divided by the probability to fail (\\(1-p\\)).\n\nTo get a sense of how the \\(logit\\) link function works, here’s a figure where the different potential probability that elephants are present is on the y-axis. Remember, we don’t know what this probability is but we do know that at minimum it’s 0% and at most it’s 100%. The x-axis shows the corresponding logit value (if we take each probability and feed it into \\(log(\\frac{p}{1-p})\\)).\n\n\nCode\np &lt;- seq(from = 0, to = 1, by = 0.001)\nlogit &lt;- log(p/(1-p))\ndat &lt;- data.frame(p, logit)\nlibrary(ggplot2)\nggplot(dat) +\n  geom_line(aes(x = logit, y = p)) +\n  labs(x = \"Logit value that\\nelephants are present\",\n       y = \"Probability that\\nelephants are present\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNotice how as the probability gets close to 0%, it never actually goes below 0%, even as the logit value keeps decreasing? And the same happens when it gets close to 100%. That’s exactly what we want! Having logit values that can go from \\(-\\infty\\) to \\(+\\infty\\) is great for model fitting (for reasons we don’t need to go into here) but importantly, even if our model thinks the logit value is 999 billion, then that’s still just 100%. The “natural bounds” of the data is respected (the bounds here are 0% and 100%).\nTaking that into account, here’s how we’d run that model in R (click the Show button to see the code):\n\n\nCode\nmod &lt;- glm(elephants ~ 1,\n           data = etosha,\n           family = binomial(link = \"logit\"))\n\n\nWhich returns the estimate of \\(\\beta_0\\) (or average probability to detect elephants on the link function scale, i.e. it’s a logit value):\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nglm(formula = elephants ~ 1, family = binomial(link = \"logit\"), \n    data = etosha)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -1.3863     0.7906  -1.754   0.0795 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10.008  on 9  degrees of freedom\nResidual deviance: 10.008  on 9  degrees of freedom\nAIC: 12.008\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe estimate for (Intercept) (or \\(\\beta_0\\)) is -1.3863. We can always plug that back into our equation if we wanted to, now that we know what \\(\\beta_0\\) is:\n\\[\ny_i \\sim Bernoulli(p_i) \\\\\n\\]\n\\[\nlogit(p_i) = -1.3863\n\\]\nWe can go a bit further though, where we convert this intercept (-1.3863) that’s on the logit value into a probability by doing the inverse logit (R has a nice way to do this using the function plogis()):\n\n\nCode\nplogis(-1.3863)\n\n\n[1] 0.1999991\n\n\nWe now have our estimate as a proportion (which we can multiply by 100 to get to percentage), to get us to 20%. This probability tells us that there’s a roughly 20% chance that an elephant occupies a site in Etosha.\nOr does it?"
  },
  {
    "objectID": "Occupancy_Models.html#state-model",
    "href": "Occupancy_Models.html#state-model",
    "title": "Introduction to Occupancy Models",
    "section": "State model",
    "text": "State model\nThe first part of the occupancy model (which I’ll call the state model, so called because we’re trying to determine the state of a site - are the species present or absent) looks remarkably similar to the GLM above:\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\nlogit(\\psi_i) = \\beta_0\n\\]\nThey look similar, because they’re both \\(Bernoulli\\) GLMs! But they differ in two important ways,\n\n\\(z\\) is the true presence or absence of elephants in site \\(i\\)\n\\(\\psi\\) is the probability to be present\n\nOk, so the labels have changed, but how does that magically solve the problem of imperfect detection? Well, if we left it there this new model wouldn’t solve anything. We need something that will deal with impefect detection; a second GLM."
  },
  {
    "objectID": "Occupancy_Models.html#observation-model",
    "href": "Occupancy_Models.html#observation-model",
    "title": "Introduction to Occupancy Models",
    "section": "Observation model",
    "text": "Observation model\nThe second GLM (which I’ll call the observation model) also looks remarkably similar:\n\\[\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\nlogit(p_{i,j}) = \\alpha_0\n\\]\nIt’s yet another \\(Bernoulli\\) GLM but with some really important changes.\n\n\\(y\\) is now the detection or not of an elephant. This is worth highlighting - \\(y\\) is not the presence or absence of elephants, it’s the detection of elephants if they’re present!\n\\(j\\) is survey, which means we have multiple surveys, not just one like in our first GLM example above where \\(i\\) was a single (of ten) sites. This has implications for how we collect data, which we’ll come back to.\n\\(p\\) is the probability to detect an elephant at site \\(i\\) in survey \\(j\\) (e.g. what is the probability to detect an elephant in site 1 in the third survey?)\n\nImportantly, the probability to detect elephants (\\(p\\)) is multiplied by \\(z\\). What’s \\(z\\)? Well that’s the true occupancy state of that site from the state model. It’s this multiplication that allows the two models to “speak” to each other, and how we deal with imperfect detection.\nIf there are elephants in a site (\\(z = 1\\)), then \\(p \\times z = p \\times 1 = p\\). If elephants are absent from a site (\\(z = 0\\)), then \\(p \\times z = p \\times 0 = 0\\). This means you cannot detect elephants if they aren’t there. That’s blindingly obvious… It’s so dumb that most people don’t realise you need to specify it (I’ve heard people say: “surely any model can figure this out?”). But this stupidly simple logic is missing from our starting GLM! And a lot of research asking about where species are use a model equivalent to the very first GLM on this page.\n\nKeep in mind that we don’t know \\(z\\) - that’s the “true” occupancy state of a site. It’s something called a latent variable, meaning a “colum of data” that can’t be measured or recorded in the field. Instead, much like with parameters, we use the data we have collected to estimate this latent variable for each site that we have data from.\n\nThat’s how occupancy model knows that you can only detect elephants if they’re present, and if they aren’t present then you can’t see them! That’s the beauty of occupancy models and why they are one of my favorite types of analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Occupancy Modelling",
    "section": "",
    "text": "This site is for you both and introduces the fundamentals and advanced topics in occupancy modelling that you’ll need to do your honours project. I’ve written the material to be accessible for you both, in terms of your honours projects, but also to help boost your skillset for when you finish you undergraduate.\nAs always, if there is anything in the site that is either wrong or confusing, just let me know. If it helps, I would be amazed if you understood everything that’s covered, so it’s my prior belief that parts will need to be explained in person.\n\n\nTo get the most out of this material, I recommend the following order:\n\nOccupancy Models: The basics\nLearn the basic theory of occupancy models and how they help us infer species presence/absence from imperfect detection.\nOccupancy Models: Covariates\nLearn how site and survey covariates can be incorporated to improve our biological understanding but also improve the accuracy of our inference.\nBayesian models: The concept\nLearn the basic theory of the Bayesian statistical framework; what’s a prior versus a posterior?\nFitting Bayesian Models\nCombine occupancy models with the Bayesian framework to fit Bayesian occupancy models.\nTo be added - Spatial autocorrelation\nLearn the basic theory of spatial autocorrelation\nTo be added - Occupancy models with Spatial autocorrelation\nCombine occupancy models, the Bayesian framework, and spatial autocorrelation to fit Bayesian occupancy models with spatial autocorrelation.\n\nBe aware that this site is very much an on going work in progress. Some text may be unfinished, some code may be a bit janky. Let me know if you spot anything that needs some work.\nEnjoy! (Maybe?)"
  },
  {
    "objectID": "index.html#suggested-order",
    "href": "index.html#suggested-order",
    "title": "Welcome to Occupancy Modelling",
    "section": "",
    "text": "To get the most out of this material, I recommend the following order:\n\nOccupancy Models: The basics\nLearn the basic theory of occupancy models and how they help us infer species presence/absence from imperfect detection.\nOccupancy Models: Covariates\nLearn how site and survey covariates can be incorporated to improve our biological understanding but also improve the accuracy of our inference.\nBayesian models: The concept\nLearn the basic theory of the Bayesian statistical framework; what’s a prior versus a posterior?\nFitting Bayesian Models\nCombine occupancy models with the Bayesian framework to fit Bayesian occupancy models.\nTo be added - Spatial autocorrelation\nLearn the basic theory of spatial autocorrelation\nTo be added - Occupancy models with Spatial autocorrelation\nCombine occupancy models, the Bayesian framework, and spatial autocorrelation to fit Bayesian occupancy models with spatial autocorrelation.\n\nBe aware that this site is very much an on going work in progress. Some text may be unfinished, some code may be a bit janky. Let me know if you spot anything that needs some work.\nEnjoy! (Maybe?)"
  },
  {
    "objectID": "Bayesian_Occupancy_Models.html",
    "href": "Bayesian_Occupancy_Models.html",
    "title": "Bayesian Algorithms",
    "section": "",
    "text": "In the previous page, we went through the underlying theory of Bayesian statistics; how our prior knowledge is updated using real data through Bayes’ Theorem. Now, we’re going to move into the practical application of these ideas using the spOccupancy package. My goal here is to help you understand how Bayesian occupancy models are fit, how to monitor convergence, and how to interpret uncertainty through credible intervals.\nWe’ll again work with the Etosha elephant dataset. Importantly, we are not yet including spatial autocorrelation. We’ll add that in the next section.\n\n\nCode\nlibrary(spOccupancy)\n\n# These are needed to produce the website\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggthemes)\n\n\n\nHow do we estimate a posterior?\nIn the previous page we spoke about posteriors and how these are a combination of our prior belief and our data. But how does that actually happen? Well, the answer is pretty simple but took hundreds of years for statisticians to figure out. Markov Chain Monte Carlo is an algorithm that was developed by scientists at Los Alamos in the 1950s working on the Manhattan Project (yes that Manhattan Project). The algorithm is called Markov Chain Monte Carlo (or MCMC) for two reasons. A Markov Chain describes a sequence of states, where the probability to move from one state to the next state depends only on what the current state is.\nFor example, imagine you are walking through a series of rooms. Where you go next depends only on the room you’re currently in. You can’t instantly teleport to the other side of the building! Nor does it depend on the path you have taken to get to your current room.\n\n\nWhat does MCMC look like?\nBelow is an animation of a simple MCMC algorithm to help give an intuition for what’s happening behind the scenes.\nFor this, we have the following simple linear model:\n\\[y_i \\sim Normal(\\mu_i, \\sigma^2)\\]\n\\[\\mu_i = \\beta_0 + \\beta_1 \\times x_i \\]\nSo our objective here is to figure out \\(\\beta_0\\) and \\(\\beta_1\\). To do so, we’ll set up an MCMC using one “chain”. A chain is a Markov Chain - it’s the black dot and orange line you can see in the animation below. This chain is trying to find the “True value” location marked with a teal X. At each step the black dot asks “if I take a step in a random direction, will I be closer to X or further away?”. If it’s closer, then it takes the step. If it’s further away, then it’s less likely to take the step but it’s not impossible. This seems like a bad choice. Why move somewhere if it’s further away from X? The answer is a bit nuanced, but the concise explanation is that sometimes there might be areas of the “parameter space” (the parameter space is all possible values of \\(\\beta_0\\) and \\(\\beta_1\\)) which “work” quite well despite not being the true value. If we didn’t have the behaviour, where the chain might take a step even though it’s worse, then we might get stuck in one of these “bad” areas (technically, these “bad” areas are called “local minima”).\nHere’s our MCMC in action:\n\n\nCode\ntrue_param &lt;- c(2, -1)\ntarget_density &lt;- function(x, y) {\n  exp(-0.5 * ((x - true_param[1])^2 / 1^2 + (y - true_param[2])^2 / 0.5^2))\n}\n\nset.seed(123)\nn_iter &lt;- 100\nx &lt;- y &lt;- 0\nsamples &lt;- tibble(x = x, y = y, iteration = 1)\n\nfor (i in 2:n_iter) {\n  x_prop &lt;- rnorm(1, x, 0.4)\n  y_prop &lt;- rnorm(1, y, 0.4)\n  accept_ratio &lt;- target_density(x_prop, y_prop) / target_density(x, y)\n  \n  if (runif(1) &lt; accept_ratio) {\n    x &lt;- x_prop\n    y &lt;- y_prop\n  }\n  samples &lt;- samples |&gt; add_row(x = x, y = y, iteration = i)\n}\n\nggplot(samples, aes(x = x, y = y)) +\n  geom_path(color = \"#FF5733\", linewidth = 0.7) +\n  geom_point(aes(x = x, y = y), color = \"black\", size = 2) +\n  geom_point(aes(x = true_param[1], y = true_param[2]), \n             color = \"#00A68A\", size = 3, shape = 4, stroke = 2) +\n  annotate(\"text\", x = true_param[1] + 0.1, y = true_param[2],\n           label = \"True value\", color = \"#00A68A\", hjust = 0) +\n  transition_reveal(iteration) +\n  coord_fixed() +\n  labs(\n    title = \"MCMC Chain Step: {frame_along}\",\n    x = bquote(beta[0]),\n    y = bquote(beta[1])\n  ) +\n  theme_bw()\n\n\n\nHere’s where the really clever bit comes in. At each iteration, if we record the parameter value it tried, and store it, when we build it up the end result is our posterior! This is what made Bayesian statistics possible! We don’t need to use any crazy (and often impossible) maths to figure out the posterior, we just have MCMC walk around and the end result is an insanely good reflection of the posterior!\n\n\nCode\nsamples_long &lt;- samples |&gt;\n  pivot_longer(cols = c(x, y), names_to = \"parameter\", values_to = \"value\")\n\nsamples_long &lt;- samples_long |&gt;\n  mutate(true_value = if_else(parameter == \"x\", true_param[1], true_param[2]))\n\nsamples_long_cumulative &lt;- samples_long |&gt;\n  group_by(parameter) |&gt;\n  group_split() |&gt;\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    map_dfr(1:max(df$iteration), function(i) {\n      df |&gt;\n        filter(iteration &lt;= i) |&gt;\n        mutate(frame = i, parameter = param)\n    })\n  })\n\nsamples_long_cumulative &lt;- samples_long_cumulative %&gt;%\n  mutate(parameter_label = case_when(\n    parameter == \"x\" ~ \"beta[0]\",\n    parameter == \"y\" ~ \"beta[1]\"\n  ))\n\nggplot(samples_long_cumulative, aes(x = value)) +\n  geom_histogram(binwidth = 0.4, fill = \"#FF5733\", color = \"white\", boundary = 0) +\n  geom_vline(aes(xintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", ncol = 1, labeller = label_parsed) +\n  transition_manual(frame) +\n  labs(title = \"Cumulative Posterior up to Iteration {current_frame}\",\n       x = \"Parameter Value\", y = \"Frequency\") +\n  theme_bw()\n\n\n\nAnother way to show what the MCMC was up to is using something called “traceplots”. These are relatively simple but actually quite powerful for determining if we trust the model output. We’ll come back to this in a bit, but for now, we can show the MCMC exploring the parameter space using these traceplots.\n\n\nCode\nsamples_long_cumulative_trace &lt;- samples_long |&gt;\n  group_by(parameter) |&gt;\n  group_split() |&gt;\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    map_dfr(1:max(df$iteration), function(i) {\n      df |&gt;\n        filter(iteration &lt;= i) |&gt;\n        mutate(frame = i, parameter = param)\n    })\n  })\n\nsamples_long_cumulative_trace &lt;- samples_long_cumulative_trace %&gt;%\n  mutate(parameter_label = case_when(\n    parameter == \"x\" ~ \"beta[0]\",\n    parameter == \"y\" ~ \"beta[1]\"\n  ))\n\nggplot(samples_long_cumulative_trace, aes(x = iteration, y = value)) +\n  geom_line(color = \"#FF5733\", linewidth = 0.8) +\n  geom_hline(aes(yintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", ncol = 1, labeller = label_parsed) +\n  transition_manual(frame) +\n  labs(title = \"Traceplot up to Iteration {current_frame}\",\n       x = \"Iteration\", y = \"Parameter Value\") +\n  theme_bw()\n\n\n\n\n\nImproving our MCMC\nWe’ve made a good start but we can improve this quite a bit. Firstly, limiting the algorithm to 100 iterations doesn’t give it many opportunities to find the true value. In general, you often give MCMC thousands of iterations, rather than a paltry 100. So first improvement is to increase the number of iterations (you may remember we did this when we were going through the occupancy theory pages - now you know why!).\nSecondly, we’re using one MCMC chain. Why not more? Afterall, if we have say four chains, then if all four agree that they’re close to the true value that would give us more comfort. If they find different “True values”, well, then it seems likely that we haven’t actually found it.\nLet’s implement our improvement and see what our plots now look like:\n\n\nCode\ntrue_param &lt;- c(2, -1)\n\ntarget_density &lt;- function(x, y) {\n  exp(-0.5 * ((x - true_param[1])^2 / 1^2 + (y - true_param[2])^2 / 0.5^2))\n}\n\nset.seed(123)\nn_iter &lt;- 1000\nn_chains &lt;- 4\n\nchains_list &lt;- map_dfr(1:n_chains, function(chain_id) {\n  x &lt;- y &lt;- 0  # Start at (0, 0)\n  samples &lt;- tibble(x = x, y = y, iteration = 1, chain = chain_id)\n  \n  for (i in 2:n_iter) {\n    x_prop &lt;- rnorm(1, x, 0.4)\n    y_prop &lt;- rnorm(1, y, 0.4)\n    accept_ratio &lt;- target_density(x_prop, y_prop) / target_density(x, y)\n    \n    if (runif(1) &lt; accept_ratio) {\n      x &lt;- x_prop\n      y &lt;- y_prop\n    }\n    \n    samples &lt;- samples %&gt;% add_row(x = x, y = y, iteration = i, chain = chain_id)\n  }\n  samples\n})\n\nggplot(chains_list, aes(x = x, y = y, group = chain, color = as.factor(chain))) +\n  geom_path(linewidth = 0.7) +\n  geom_point(aes(x = x, y = y), size = 1.5) +\n  geom_point(aes(x = true_param[1], y = true_param[2]), \n             color = \"white\", size = 4, shape = 4, stroke = 2, inherit.aes = FALSE) +\n  annotate(\"text\", x = true_param[1] + 0.1, y = true_param[2],\n           label = \"True value\", color = \"white\", hjust = 0) +\n  transition_reveal(along = iteration) +\n  coord_fixed() +\n  scale_color_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"MCMC Chains Step: {round(frame_along, digits = 0)}\",\n    x = bquote(beta[0]),\n    y = bquote(beta[1])) +\n  theme_bw()\n\n\n\n\n\nCode\nsamples_long &lt;- chains_list %&gt;%\n  pivot_longer(cols = c(x, y), names_to = \"parameter\", values_to = \"value\") %&gt;%\n  mutate(true_value = if_else(parameter == \"x\", true_param[1], true_param[2]),\n         parameter_label = case_when(\n           parameter == \"x\" ~ \"beta[0]\",\n           parameter == \"y\" ~ \"beta[1]\"\n         ))\n\n# Build cumulative data\nsamples_long_cumulative &lt;- samples_long %&gt;%\n  group_by(parameter, chain) %&gt;%\n  group_split() %&gt;%\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    chain_id &lt;- unique(df$chain)\n    map_dfr(1:max(df$iteration), function(i) {\n      df %&gt;%\n        filter(iteration &lt;= i) %&gt;%\n        mutate(frame = i, parameter = param, chain = chain_id)\n    })\n  })\n\nggplot(samples_long_cumulative, aes(x = value, fill = as.factor(chain))) +\n  geom_histogram(binwidth = 0.4, color = \"white\", boundary = 0, \n                 position = position_dodge(), alpha = 0.6) +\n  geom_vline(aes(xintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", labeller = label_parsed, ncol = 1) +\n  transition_manual(frame) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"Cumulative Posterior up to Iteration {current_frame}\",\n       x = \"Parameter Value\", y = \"Frequency\") +\n  theme_minimal()\n\nggplot(samples_long_cumulative, aes(x = iteration, y = value, color = as.factor(chain))) +\n  geom_line(linewidth = 0.7) +\n  geom_hline(aes(yintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_y\", labeller = label_parsed, ncol = 1) +\n  transition_manual(frame) +\n  scale_color_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"Traceplot up to Iteration {current_frame}\",\n       x = \"Iteration\", y = \"Parameter Value\") +\n  theme_minimal()\n\n\n\n\nI have a bit more confidence in our posteriors now:\n\nAll chains seem to agree on the same value (they have “converged” to the same answer)\nThe posteriors look reasonably well estimated (they’ll close to the true value - but keep in mind that with real analysis we don’t know what the truth is)\nThe traceplots resemble “hairy caterpillars”, which matches the Markov Chain idea that the next value depends only on the current value (it doesn’t “remember” older values).\n\nThe final thing I want to bring your attention to are the first hundred or so iterations. Generally, these iterations can be pretty wild, fluctuating massively. If you think about it, that’s kind of fair enough. We’re starting each MCMC chain at random locations, so it’s fair if things are a bit wobbly at the start. So what we do with these early iterations is to simply ignore them. These are called “burn in” iterations, as in, we’re letting the engine warm up, so these are just used to get up to speed. Mostly arbitrarily, we typically ignore the first 10% of iterations, so where we’ve used 1000 iterations, we’d generally ignore the first 100.\n\n\nFitting the occupancy model using MCMC\nLet’s now revisit our Bayesian model. As a reminder, here’s what it looked like:\n\\[z_i \\sim Bernoulli(\\psi_i)\\\\\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\nWhich we can translate into the following code. But pay attention to the n.chains, n.samples and n.burn arguments. This is where we specify how many chains we want (n.chains), how many iterations we want (n.samples, note an iteration can also be called a sample), and how many of the first iterations we want to ignore (n.burn).\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  n.chains = 4,     # 4 chains just like in our simple example\n  n.samples = 2000, # 2000 iterations for each chain\n  n.burn = 200,     # We ignore the first 200 iterations to give MCMC a chance to get it's feet\n  \n  verbose = FALSE   # This just says don't spit out details while fitting \n  # (normally I would leave verbose = TRUE so I can keep track of the model while it's fitting)\n  )\n\n\nWe can now check how our model worked. The code is pretty simple. The only tricky thing is to specify if you want beta or alpha. Importantly, beta here refers to the occupancy parameters, and alpha refers to the detection parameters.\nSo here are the traceplots and posteriors for \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) in:\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\n\nCode\nplot(fit, 'beta') # Occupancy parameters.\n\n\n\n\n\n\n\n\n\nAnd here are the traceplots and posteriors for \\(\\alpha_0\\) and \\(\\alpha_1\\) in:\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\n\n\nCode\nplot(fit, 'alpha') # Detection parameters.\n\n\n\n\n\n\n\n\n\nAnd we can also get a more numeric summary of the model using summary(). Importantly, the information you see below is the same as the data in the figures above. It’s just summarised! So the Mean is just the mean of the posteriors above!\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0167\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0536 1.0657  0.1991  1.9727 4.2706 1.0151 315\ntree         0.2259 1.2968 -2.1384  0.1544 2.9006 1.0797 278\ntemp        -1.1362 1.1440 -3.2916 -1.1758 1.3257 1.0396 272\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8539 0.2623 -2.3783 -1.8533 -1.3435 1.0018 1142\nrain         0.1730 0.2207 -0.2558  0.1719  0.6087 1.0015 2994\n\n\n\n\nCredible intervals\nNotice the 2.5% and 97.5% in the tables above? These are the credible intervals that I briefly mentioned in the Occupancy Models: Covariates page. Technically, these are just the quantiles of the posterior. Or, phrased alternatively, 95% of all iterations are within this interval. And keep in mind, that a Bayesian credible interval is not the same as a frequentist confidence interval. For our purposes, these intervals represent a 95% probability to contain the true value (based on the data we collected and the model we fit)!\n\n\nCode\nbeta_0_samples &lt;- fit$beta.samples[,1]\n\nbeta_0_df &lt;- tibble(beta_0 = beta_0_samples)\n\nsummary_stats &lt;- beta_0_df %&gt;%\n  summarise(\n    mean = mean(beta_0),\n    median = median(beta_0),\n    lower = quantile(beta_0, 0.025),\n    upper = quantile(beta_0, 0.975)\n  )\n\nggplot(beta_0_df, aes(x = beta_0)) +\n  geom_histogram(fill = \"#00A68A\", colour = \"white\", alpha = 0.5, bins = 50) +\n  \n  # Vertical lines\n  geom_vline(xintercept = summary_stats$mean, linetype = \"solid\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$median, linetype = \"dashed\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$lower, linetype = \"dotted\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$upper, linetype = \"dotted\", color = \"black\", linewidth = 1) +\n\n  # curved arrows\n  geom_curve(aes(x = summary_stats$mean + 0.5, y = 400, \n                 xend = summary_stats$mean, yend = 350), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = 0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$median - 0.5, y = 300, \n                 xend = summary_stats$median, yend = 250), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = -0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$lower - 0.5, y = 100, \n                 xend = summary_stats$lower, yend = 75), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = -0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$upper + 0.5, y = 100, \n                 xend = summary_stats$upper, yend = 75), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = 0.3, color = \"black\") +\n\n  annotate(\"text\", x = summary_stats$mean + 0.5, y = 400, label = \"Mean\", hjust = 0, size = 5) +\n  annotate(\"text\", x = summary_stats$median - 0.5, y = 300, label = \"Median\", hjust = 1, size = 5) +\n  annotate(\"text\", x = summary_stats$lower - 0.5, y = 100, label = \"2.5% CI\", hjust = 1, size = 5) +\n  annotate(\"text\", x = summary_stats$upper + 0.5, y = 100, label = \"97.5% CI\", hjust = 0, size = 5) +\n\n  labs(\n    x = expression(paste(beta[0], \" or '(Intercept)'\")),\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom our summary table, the mean for the \\(\\beta_0\\) or (Intercept) posterior was estimated as 2.0536, the median (called 50% in the summary table) was 1.9727, and the 95% credible intervals were 0.1991 and 4.2706, just like we see in the figure.\nThis might seem trivial but having credible intervals be so simple, and the interpretation having a useful meaning is one of the big selling points of Bayesian analysis in my mind. It’s not something to trivialise.\n\n\nSpecifying priors\nSo far, we’ve let the spOccupancy package handle setting the priors for us behind the scenes. But we can actually specify our own priors if we want to.\nIn the background, spOccupancy uses something called Pólya-Gamma data augmentation, and under the hood, this method assumes \\(Normal\\) priors for both the occupancy and detection parameters (including their intercepts).\nIf you don’t specify anything, spOccupancy will set:\n\nThe “hypermean” (mean of \\(Normal\\) distribution for all priors) = 0\nThe “hypervariance” (variance \\(Normal\\) distribution for all priors) = 2.72\n\nThis corresponds to a relatively flat prior on the probability scale (the 0–1 scale for occupancy or detection probabilities). In other words, by default, the prior doesn’t strongly pull your estimates toward any particular value; it lets the data mostly speak for itself.\nBut we can set these explicitly if we want to! Here’s what that looks like:\n\n\nCode\n# Specify priors for detection (alpha) and occupancy (beta)\npriors &lt;- list(\n  alpha.normal = list(mean = 0, var = 2.72),  # Detection priors\n  beta.normal = list(mean = 0, var = 2.72)    # Occupancy priors\n)\n\n\nThe above code is basically doing what spOccupancy does by default. All priors will get \\(Normal(0,2.72)\\) in both the detection and occupancy sub models. We can either give a single number (like 0 and 2.72 above), and it will apply to all parameters, or you can give a vector if you want different priors for different parameters.\nHere’s how we’d give different priors for each parameter:\n\n\nCode\n# Different priors for each occupancy parameter\noccurrence_priors &lt;- list(\n  mean = c(0, 0.5, -0.5),    # One mean for each occupancy parameter\n  var = c(2.72, 1, 2)        # One variance for each occupancy parameter\n)\n\n# Different priors for each detection parameter\ndetection_priors &lt;- list(\n  mean = c(0, 0.2),          # One mean for each detection parameter\n  var = c(0.5, 1)           # One variance for each detection parameter\n)\n\n# Combine into the priors list\npriors &lt;- list(\n  beta.normal = occurrence_priors,\n  alpha.normal = detection_priors\n)\n\n\nThis would translate into this model:\n\\[z_i \\sim Bernoulli(\\psi_i)\\\\\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[\\beta_0 \\sim Normal(0, 2.72)\\]\n\\[\\beta_1 \\sim Normal(0.5, 1)\\]\n\\[\\beta_2 \\sim Normal(-0.5, 2)\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\n\\[\\alpha_0 \\sim Normal(0, 0.5)\\]\n\\[\\alpha_1 \\sim Normal(0.2, 1)\\]\nNow, to be very, very clear; I am choosing these priors completely at random just for demonstration. I have no reason, in this case, to think that a reasonable prior for \\(\\beta_1\\) is \\(Normal(0.5, 1)\\).\nBut we can always fit this model to see what happens:\n\n\nCode\nfit_priors &lt;- PGOcc(\n  occ.formula = ~ tree + temp,\n  det.formula = ~ rain,\n  data = etosha,\n  priors = priors,     # We add our priors here\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE\n)\n\n\nHaving fit, we can see what the posteriors look like:\n\n\nCode\nplot(fit_priors, 'beta') # Occupancy parameters.\n\n\n\n\n\n\n\n\n\nCode\nplot(fit_priors, 'alpha') # Detection parameters.\n\n\n\n\n\n\n\n\n\nAnd pull up the summary:\n\n\nCode\nsummary(fit_priors)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    priors = priors, n.samples = 2000, verbose = FALSE, n.burn = 200, \n    n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0152\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.5053 0.9644 -0.2024  1.4383 3.5766 1.0514 304\ntree         0.5140 0.9042 -1.2174  0.4798 2.3414 1.0165 412\ntemp        -1.2473 0.9496 -3.0085 -1.2664 0.7709 1.0097 365\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.6451 0.2634 -2.1407 -1.6515 -1.0959 1.0112 1004\nrain         0.1595 0.2133 -0.2483  0.1565  0.6006 1.0048 3385\n\n\nIf we compare this with the original model, where we left the priors at their defaults we can see that different priors can lead to different posteriors. Keep in mind how Bayesian statistics work - the posterior is a combination of our data and prior beliefs. We’ve changed our prior belief and our posteriors have changed as a result.\nNow in this case, the difference in posteriors are pretty minimal. It might not always be. And in truth, we would want to think a bit more carefully about the priors. Don’t be fooled here. Just because my randomly chosen priors are different from the default ones doesn’t mean the default model is necessarily better. The best option is to choose priors you think are reasonable. If you have different prior beliefs (e.g. I have some informative priors but also some uninformative priors) I can run the model with both, and see how much of a difference it makes. This is something called “prior sensitivity analysis” and is a fairly useful tool to have in your back pocket when you’re not sure about your priors.\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0167\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0536 1.0657  0.1991  1.9727 4.2706 1.0151 315\ntree         0.2259 1.2968 -2.1384  0.1544 2.9006 1.0797 278\ntemp        -1.1362 1.1440 -3.2916 -1.1758 1.3257 1.0396 272\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8539 0.2623 -2.3783 -1.8533 -1.3435 1.0018 1142\nrain         0.1730 0.2207 -0.2558  0.1719  0.6087 1.0015 2994\n\n\n\n\nFin\nWith that, you should be good to run a Bayesian occupancy model. Something you can add to your CV and have employers fawn over you. Even better if you understand it, so that when they ask you about it you can have a conversation!\nThe only part left in the analysis is including “spatial autocorrelation”. We’ll cover that in the next page.\n\n\nA subtle point: Priors and the link function\nWhen we choose priors in a Bayesian model, it’s really important to remember what scale those priors live on.\nIn our occupancy model, we specify priors for the parameters on the logit scale, not directly on the probability (0–1) scale.\nFor example, if we write:\n\\[logit(p_i) = \\beta_0 + \\beta_1 \\times x_i\\]\nthen \\(\\beta_0\\) and \\(\\beta_1\\) are in logit space.\nRemember, the logit function stretches the 0–1 probability scale onto the whole real line:\n\nProbabilities near 0.5 correspond to logits near 0.\nProbabilities near 0 or 1 correspond to logits of \\(-\\infty\\) and \\(+\\infty\\).\n\nThis means that a \\(Normal\\) prior with mean 0 and large variance on the logit scale is not flat on the probability scale! Even “uninformative” \\(Normal\\) priors on the logit scale can actually imply very strong beliefs on the probability scale.\nTo build some intuition, we’ll do a little prior predictive simulation:\n\nWe’ll randomly draw values for \\(\\beta_0\\) and \\(\\beta_1\\) from a \\(Normal(0, 2.72)\\) prior.\nWe’ll simulate the relationship between \\(x\\) and \\(p(x)\\) by plugging those \\(\\beta_0\\) and \\(\\beta_1\\) values into the logit equation.\nWe’ll repeat this 100 times to show many possible relationships.\n\n\n\nCode\nset.seed(123)\n\nn_draws &lt;- 100\nx_seq &lt;- seq(-3, 3, length.out = 100)\n\nbeta_0_draws &lt;- rnorm(n_draws, mean = 0, sd = sqrt(2.72))\nbeta_1_draws &lt;- rnorm(n_draws, mean = 0, sd = sqrt(2.72))\n\nprior_simulations &lt;- map2_dfr(\n  beta_0_draws, beta_1_draws,\n  .f = function(b0, b1) {\n    tibble(\n      x = x_seq,\n      logit_p = b0 + b1 * x,\n      p = plogis(logit_p)\n    )\n  },\n  .id = \"draw\"\n)\n# Plot\nggplot(prior_simulations, aes(x = x, y = p, group = draw)) +\n  geom_line(alpha = 0.2, color = \"#FF5733\") +\n  theme_minimal() +\n  labs(\n    title = \"Prior Predictive Simulation\",\n    x = \"Covariate (x)\",\n    y = \"Probability (p)\"\n  )\n\n\n\n\n\n\n\n\n\nEach orange line is a possible relationship between \\(x\\) and \\(p(x)\\) given the priors we chose. Notice that some lines are almost flat at 0 or 1? While others are very steep, flipping from 0 to 1 over a narrow range of \\(x\\)? Even though the prior on \\(\\beta_0\\) and \\(\\beta_1\\) was centered at 0 with large variance, the resulting priors on \\(p\\) are not uniform or “neutral.”\nIf I were being hyper cautious, I might be worried these priors are pushing the model towards the extreme flipping behaviour. In some cases that might be good, in others it might be bad.\nThe broader points I am making here are:\n\nDon’t stress too much about priors. If you have a lot of data your prior will often not be terribly important.\nBut give a little thought as to what a sensible prior would be, especially when working with link functions.\n\nIf you’re in doubt speak with me! I think having a discussion about your priors would be an excellent use of one of our meetings (hint, hint)."
  },
  {
    "objectID": "Bayesian_Statistics.html",
    "href": "Bayesian_Statistics.html",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "",
    "text": "Most of the statistics you’ve probably learned so far has been based in what’s called “frequentist” statistics. The analysis you’ll be using for occupancy models (at least this particular implementation) will be using something called “Bayesian” statistics (pronounced “Bay-zee-’n”). These are the two main frameworks currently used to learn something from data (unless you want to start splitting philosophical hairs). Within this document we’re going to start with what Bayesian statistics is, how to move from frequentist to Bayesian statistics, and how a Bayesian model works. We’ll leave occupancy models to the side for the time being."
  },
  {
    "objectID": "Bayesian_Statistics.html#your-prior-belief-before-seeing-the-data",
    "href": "Bayesian_Statistics.html#your-prior-belief-before-seeing-the-data",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "Your Prior Belief (before seeing the data)",
    "text": "Your Prior Belief (before seeing the data)\nYou start off thinking you have a 50% chance of passing. That doesn’t mean you’re certain, it’s just your best guess. You might be unsure, so you’re open to the idea that it could be lower or higher. Your prior might look like this:\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\ntheta &lt;- seq(0, 1, length.out = 1000)\n\nprior &lt;- dbeta(theta, 20, 20)\n\ndata.frame(theta, prior) |&gt;\n  ggplot(aes(x = theta, y = prior)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Chance of Passing\", y = \"Belief Density\") +\n  theme_minimal()"
  },
  {
    "objectID": "Bayesian_Statistics.html#the-data-100-students-passed",
    "href": "Bayesian_Statistics.html#the-data-100-students-passed",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "The Data (100 students passed)",
    "text": "The Data (100 students passed)\nNow you find out that 100 students have taken the thesis before you, and all of them passed. That’s really strong evidence that the pass rate is high, probably very near to 100%.\nWe can visualise how likely the observed data is for different values of the pass rate, this is the likelihood.\n\n\nCode\nlikelihood &lt;- theta^100\nlikelihood &lt;- likelihood / max(likelihood)\n\ndata.frame(theta, likelihood) |&gt;\n  ggplot(aes(x = theta, y = likelihood)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Chance of Passing\", y = \"Likelihood (scaled)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIn Bayesian terms, this evidence updates your belief."
  },
  {
    "objectID": "Bayesian_Statistics.html#the-posterior-updated-belief",
    "href": "Bayesian_Statistics.html#the-posterior-updated-belief",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "The Posterior (updated belief)",
    "text": "The Posterior (updated belief)\nYour updated belief, the posterior, combines your initial guess of roughly 50% with the strong evidence we have from 100 students. The resulting curve now shifts to the right — it’s taller near 85%–90% but still not at 100%, because you haven’t thrown out your original uncertainty.\n\n\nCode\nposterior &lt;- dbeta(theta, 120, 20)\n\nposterior_df &lt;- data.frame(\n  theta = rep(theta, 3),\n  density = c(prior, likelihood, posterior),\n  belief = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(theta))\n)\n\nposterior_df |&gt;\n  ggplot(aes(x = theta, y = density, color = belief)) +\n  geom_line(size = 1.2) +\n  labs(x = \"Chance of Passing\", y = \"Belief / Likelihood (scaled)\", colour = \"\") +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"steelblue\", \"darkgreen\", \"firebrick\"))\n\n\n\n\n\n\n\n\n\nImportantly, in Bayesian statistics, we don’t have a single value for our parameter estimate (or posterior). Instead we have a full distribution of values. This becomes an insanely valuable property that can be exploited in many different ways, which we’ll get back to later on.\nFor now, your posterior is more-or-less the multiplication of your prior belief with the likelihood (\\(P(Data | Parameters)\\times P(Parameters)\\)). In this case, giving you a happy middle ground of ca. 85% chance to pass.\nThis is how Bayesian statistics works in practice:\n\nStart with a belief (or hypothesis).\nSee some evidence (i.e. collect some data).\nUpdate your belief accordingly.\n\nIn this case, seeing 100 successful students shifts your belief quite a lot, but not all the way to 100%, because your prior belief still matters! You know yourself after all! You’d be silly to just ignore that.\nIn short, this is kinda the scientific method, isn’t it? (pls don’t tell any frequentists that I said that…)"
  },
  {
    "objectID": "OccMods_WithCovariates.html",
    "href": "OccMods_WithCovariates.html",
    "title": "Including Covariates in Occupancy Models",
    "section": "",
    "text": "The R packages\nAs before, this code uses spOccupancy, ggplot2 and patchwork for the analysis and visualisations.\n\n\nCode\nlibrary(spOccupancy)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n\n\n\nThe theory\nIn the previous document we went through a simple example to get a sense of how occupancy models work. We’re now going to begin the process of slowly increasing the complexity. To start, we’re going to need to include covariates (often called explanatory variables or independent variables or a wide variety of other names - it’s a mess).\nLet’s revisit our simple occupancy model:\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\n\\]\n\\[\nlogit(\\psi_i) = \\beta_0\\\\\n\\]\n\\[\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n\\]\n\\[\nlogit(p_{i,j}) = \\alpha_0\n\\]\nwhere the first two formula represent the state model (i.e. are our species present or absent from site \\(i\\)), and the last two form the detection model.\nWhat we’re going to do in this section is to add in covariates to these equations, starting with the detection model.\nThe first thing to note is that \\(y\\) is indexed by both \\(i\\) and \\(j\\), where \\(i\\) was the site and \\(j\\) was the survey. That \\(j\\) is important because it allows us to specify covariates that vary not just by site (e.g. site 1 has 20 trees while site 2 has 10 trees) but by survey as well (e.g. on survey 1 in site 1, the temperature was 20 degrees, 15 degrees in survey 2 and 5 degrees in survey 3). This means that anything that was different in one survey to the next can be accounted for and included in the model (so long as we can measure it).\nSpecifically, these “survey varying covariates” are features that we think may have made us (or whoever or whatever collected the data) to be more or less effective. Using cameras? Well maybe the presence of fog has a big impact on how likely we are to detect elephants. Doing surveys yourself? Well maybe the hour of day that you did the survey had a big impact. The point is, we can include these variables that deal with differences in detection probability.\nI’ll simulate a new dataset (with a bit more data for us to work with - 64 sites surveyed 3 times each) to show this off. We can say that the detection covariate in this case is rainfall, but note that the values for rainfall will be centred on zero (don’t worry about this).\nHere’s what the data looks like:\n\n\nCode\nset.seed(1234)\ndat &lt;- simOcc(J.x = 8, \n              J.y = 8, \n              n.rep = rep(3, times = 8 * 8), \n              beta = c(1), \n              alpha = c(-2, 0.5))\nobs &lt;- dat$y\ndet_cov &lt;- dat$X.p[,,2]\ndf &lt;- data.frame(\n  survey = rep(1:3, each = 64),\n  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),\n  y = c(obs[,1], obs[,2], obs[,3])\n)\n\np1 &lt;- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = cov)) +\n  geom_jitter(aes(x = factor(survey), y = cov),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Rainfall\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = cov),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Rainfall\", y = \"Elephant detection\") +\n  theme_minimal()\n\np1 + p2 + plot_annotation(tag_levels = \"A\", tag_suffix = \")\")\n\n\n\n\n\n\n\n\n\nNotice anything? In A) we can see that in each survey, there’s a lot of variation across the 64 sites in how much rainfall there was; maybe peaking in survey 2. In B), it looks like it might be more likely that we detect elephants more often when there’s more rainfall.\nAre you sure there’s an influence? If so, how strong is it? Exactly how strong is it?\nThere’s no way you can answer those questions. That’s where we need stats. So let’s add in rainfall to our model. I’ll recycle the code from the previous document and include rainfall. Here’s how we do that.\n\n\nData preparation\nJust like in the previous document, we need to include our different datasets into a list. If you’re still not sure what a list is in R, think of it like a folder on your computer. You can add lots of different files to a folder, but they’re all “tied” together by being within the same folder. That’s the same as a list in R.\nHere, I’m going to do something seemingly strange. I’m going to create a list and include this into our etosha list. The reason is that we might have more than one detection covariate, so having a list, though redundant here, will make adding additional variables easier in the future.\n\n\nCode\n# Note you wouldn't need to do the dat$X.p[,,2] bit\n# That's just because the data is simulated.\ndet.covs &lt;- list(rain = dat$X.p[,,2])\n\netosha &lt;- list(\n  y = dat$y,\n  det.covs = det.covs\n)\n\n\nAnd we’re good to go on to the modelling.\n\n\nFitting the model\nA few things to note in the R code;\n\nocc.formula = ~ 1 is the equivalent to \\(logit(\\psi_{i,j}) = \\beta_0\\)\n\nI.e. an intercept only occupancy model, meaning we are not including any covariates for the probability that a site is occupied.\n\ndet.formula = ~ rain is the equivalent to \\(logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i\\)\n\nI.e. we have both an intercept and slope given we have included rain in the model.\n\nWe specify that all data is contained in the list (i.e. “folder”) called etosha.\nThe remainder of the arguments (e.g. n.chains) can be ignored for now.\n\n\n\nCode\nfit &lt;- PGOcc(\n  # The state model (i.e. what % that elephants are present?)\n  # ~ 1 means we want an intercept only model (no covariates)\n  occ.formula = ~ 1, \n  # The observation model (i.e. what % that we see elephants if present?)\n  det.formula = ~ rain, \n  # Our carefully formatted dataset\n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n\n\n\n\nInterpreting\nHaving now fit the model to the data, we can see what we’ve learnt:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0153\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n\n\nCompared to the model in the previous document we now have additional information for Detection (logit scale); we have both an (Intercept) and rain. These are \\(\\alpha_0\\) and \\(\\alpha_1\\) from our detection model \\(logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i\\). If we really wanted to, we could now replace the parameter labels (e.g. the \\(\\alpha\\)s and \\(\\beta\\)s) with their now estimated values which would look like (rounding the estimates to two decimal points arbitrarily):\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\\] $$\n\\[\nlogit(\\psi_i) = 1.88\\\\\n\\]\n\\[\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n\\]\n\\[\nlogit(p_{i,j}) = -1.89 + 0.69 \\times Rain_i\n\\]\nWith this, we can swap out \\(Rain\\) for any value that we might be interested in, to see how our detection probability changes. For example, let’s see what happens when \\(Rain = 1\\).\n\n\nCode\n-1.89 + 0.69 * 1\n\n\n[1] -1.2\n\n\nOur logit value is -1.2. How do we get that into probabilities that we can actually understand? Backtransform out of logit using plogis():\n\n\nCode\nplogis(-1.89 + 0.69 * 1)\n\n\n[1] 0.2314752\n\n\nAnd we get a ca. 23% chance to detect an elephant when \\(Rain = 1\\). What about when \\(Rain = 0\\)? Well, we can do that easily enough now that we know thew general steps:\n\n\nCode\nplogis(-1.89 + 0.69 * 0)\n\n\n[1] 0.1312445\n\n\nWhen \\(Rain = 0\\), we predict a ca. 13% chance to detect elephants.\nThis approach, whereby we make a prediction for a specific value of \\(Rain\\) can be extended into making multiple predictions at once, such that we can then draw a line through them. Here’s how we’d do that.\n\n\nPlot predicted relationships\nWe start by creating a sequence of \\(Rain\\) values, rather than doing one a time. Here we use the seq() function to create a sequence, which will range from the minimum \\(Rain\\) value to the maximum within our dataset. The number of values that we want in this sequence is specified as 20 but we can choose any value here - we just need enough that the line is drawn “accurately”.\n\n\nCode\nrain &lt;- seq(from = min(det.covs$rain),\n            to = max(det.covs$rain),\n            length.out = 20)\n\nrain\n\n\n [1] -2.71815687 -2.44127881 -2.16440076 -1.88752271 -1.61064465 -1.33376660\n [7] -1.05688855 -0.78001049 -0.50313244 -0.22625439  0.05062367  0.32750172\n[13]  0.60437977  0.88125783  1.15813588  1.43501393  1.71189199  1.98877004\n[19]  2.26564809  2.54252615\n\n\nNow we have our values, we don’t want to manually enter each value into our equation. Instead we can use the fact that R works with vectors (i.e. columns of data) to do this quickly and easily:\n\n\nCode\npred &lt;- plogis(-1.89 + 0.69 * rain)\npred\n\n\n [1] 0.02263134 0.02726568 0.03281714 0.03945308 0.04736516 0.05677017\n [7] 0.06790956 0.08104689 0.09646267 0.11444547 0.13527876 0.15922259\n[13] 0.18649040 0.21722152 0.25145143 0.28908330 0.32986526 0.37337882\n[19] 0.41904309 0.46613767\n\n\nNow for each value of rain, we have the predicted probability of detecting an elephant. Useful but you wouldn’t want to throw these two columns at your audience/reader and expect them to make sense of it. It’d be better if we include these in a figure.\nTo do so, we’ll combine both columns into a single dataset and plot using ggplot2:\n\n\nCode\ndf &lt;- data.frame(\n  pred,\n  rain\n)\n\nggplot(df) +\n  geom_line(aes(x = rain, y = pred))\n\n\n\n\n\n\n\n\n\nFrom this figure it now appears much more intuitive that increasing rain makes elephants easier to detect. We can do a little “tidying” of the figure to make it more visually pleasing:\n\n\nCode\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n\n\n\n\n\n\n\n\n\n\n\nUncertainty\nThe above figure is a good start but we’re missing any measure of uncertainty. If we were doing frequentist statistics, we would use 95% confidence intervals but these are exclusively frequentist. There are no 95% confidence intervals when we use the Bayesian statistical framework. Instead we have credible intervals. I’ll explain the Bayesian framework in a subsequent workflow but for now here is the formal definition of a credible interval:\n\nThere is a 95% probability that the True parameter value lies within the interval range, given the data and model.\n\nThis is in contrast with the frequentist confidence interval whose formal definition is so bizarre and unintuitive that it’s barely useful. 95% credible intervals are useful and work exactly the way people think frequentist intervals work.\nBut how do we include these in the figure? Well, the model summary makes it easy to find the values:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0153\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n\n\nThey’re the 2.5% and 97.5% values in our summary table. So all we need to do is repeat our predicions using these values to get out measure of uncertainty to include in the figure. Importantly, if you remember back to BI3010 where you had to multiple standard error by 1.96, we do not need to do that here. That’s frequentist nonsense - we’re Bayesian now.\nLet’s do just that:\n\n\nCode\ndf$low &lt;- plogis(-2.4610 + 0.2328 * df$rain)\ndf$upp &lt;- plogis(-1.2939 + 1.1801 * df$rain)\ndf\n\n\n         pred        rain        low        upp\n1  0.02263134 -2.71815687 0.04336427 0.01096960\n2  0.02726568 -2.44127881 0.04611831 0.01514457\n3  0.03281714 -2.16440076 0.04903828 0.02087495\n4  0.03945308 -1.88752271 0.05213304 0.02871039\n5  0.04736516 -1.61064465 0.05541172 0.03936862\n6  0.05677017 -1.33376660 0.05888379 0.05376451\n7  0.06790956 -1.05688855 0.06255900 0.07302436\n8  0.08104689 -0.78001049 0.06644741 0.09846565\n9  0.09646267 -0.50313244 0.07055932 0.13151304\n10 0.11444547 -0.22625439 0.07490526 0.17351714\n11 0.13527876  0.05062367 0.07949599 0.22545433\n12 0.15922259  0.32750172 0.08434241 0.28752905\n13 0.18649040  0.60437977 0.08945559 0.35877811\n14 0.21722152  0.88125783 0.09484663 0.43685701\n15 0.25145143  1.15813588 0.10052670 0.51819600\n16 0.28908330  1.43501393 0.10650691 0.59858193\n17 0.32986526  1.71189199 0.11279825 0.67399363\n18 0.37337882  1.98877004 0.11941156 0.74135968\n19 0.41904309  2.26564809 0.12635738 0.79895748\n20 0.46613767  2.54252615 0.13364590 0.84638633\n\n\nAnd we can add in our uncertainty using geom_ribbon():\n\n\nCode\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  geom_ribbon(aes(x = rain, ymin = low, ymax = upp),\n              alpha = 0.3) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n\n\n\n\n\n\n\n\n\nWith that, we have a publication ready figure.\n\n\nMultiple covariates\nLet’s increase the complexity a bit and have the simulation include multiple covariates. We’ll say that tree height and average temperature affect whether or not a site is occupied (elephants will like tall trees and cooler locations). We’ll still have rain affect our detection probability, as above.\nTo create the figures below we need to do some tweaks to the data. The df object I create has one row per survey, with three surveys per site. Our occupancy covariates, tree and temp have just one value; these do not change from one survey to the next. If a tree is 3 m tall in survey one, then it’ll still be 3 m tall in surveys two and three.\nA note here is that the covariates are still centered on zero. That’s just the way the data is simulated but the data you collect does not need to be the same (so ignore the fact that we will have trees that are -1 m tall - the general idea doesn’t change).\n\n\nCode\nset.seed(1234)\ndat &lt;- simOcc(J.x = 8, \n              J.y = 8, \n              n.rep = rep(3, times = 8 * 8), \n              beta = c(1, -0.2, 0.3), \n              alpha = c(-2, 0.5))\nobs &lt;- dat$y\ntemp &lt;- dat$X[,2]\ntree &lt;- dat$X[,3]\ndet_cov &lt;- dat$X.p[,,2]\ndf &lt;- data.frame(\n  survey = rep(1:3, each = 64),\n  cov = c(det_cov[,1], det_cov[,2], det_cov[,3]),\n  tree = rep(tree, times = 3),\n  temp = rep(temp, times = 3),\n  y = c(obs[,1], obs[,2], obs[,3])\n)\n\np1 &lt;- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = temp)) +\n  geom_jitter(aes(x = factor(survey), y = temp),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Temperature\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = temp),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Temperature\", y = \"Elephant detection\") +\n  theme_minimal()\n\np3 &lt;- ggplot(df) +\n  geom_boxplot(aes(x = factor(survey), y = tree)) +\n  geom_jitter(aes(x = factor(survey), y = tree),\n              width = 0.2, height = 0) +\n  labs(x = \"Survey\", y = \"Tree\") +\n  theme_minimal()\n\np4 &lt;- ggplot(df) +\n  geom_jitter(aes(y = factor(y), x = tree),\n              alpha = 0.4, width = 0, height = 0.1) +\n  labs(x = \"Tree\", y = \"Elephant detection\") +\n  theme_minimal()\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nThere are two things worth highlighting from the above figures. Firstly, the boxplots on the right are identical for each of the three surveys (the points are jittered so are randomly placed but trust me that these are identical). Secondly, it becomes quite hard to see any clear pattern in the two right hand figures. Keep in mind that the zeros here are actively misleading us. Some of the zeros are genuine in that there were no elephants there and tree and temp likely caused that (which we only know because the data is simulated) but the other zeros are false negatives; The elephants were there, we just didn’t see them.\nThis is why we need a model to figure out what the relationships are. We can no longer trust our eyes to see the pattern.\nSo let’s get our data organised such that we can fit our model.\n\n\nData preparation\nAs before, we provide our detection covariates as a list but XXX\n\n\nCode\ndet.covs &lt;- list(rain = dat$X.p[,,2])\nocc.covs &lt;- data.frame(tree = tree, temp = temp)\netosha &lt;- list(\n  y = dat$y,\n  det.covs = det.covs,\n  occ.covs = occ.covs\n)\n\n\n\n\nFitting the model\nThe core model we’re going to fit is:\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\nlogit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\nlogit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\n\\]\nThe main thing that I want to highlight here, other than having included \\(Tree\\) and \\(Temp\\) as effecting occupancy probability (\\(\\psi\\)), is that the subscripts are different.\nIn the occupancy model (\\(logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\)) the variables are subscript by \\(i\\), indicating that we have one value of \\(Tree\\) or \\(Rain\\) for each site \\(i\\).\nBut in the detection model (\\(logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\)) \\(Rain\\) is subscript by both \\(i\\) and \\(j\\). That’s because we have a different \\(Rain\\) value for each survey. This would be something we record every time we visit the site (or extract from some online source for each day).\nKeep this in mind when you’re collecting your own data. Variables that you think affect occupancy will have one value per site. Variables that you think affect detection will (generally) have one value per survey.\nTo fit this is relatively straight forward. For the occupancy model, we write occ.formula = ~ tree + temp, and for the detection model we write det.formula = ~ rain, and let the machinery do it’s thing.\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n\n\nOnce it’s run, we can check the summary() to see what the parameters were estimated as:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0172\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.9424 1.0250  0.1512  1.8670 4.0917 1.0187 347\ntree         0.3904 1.3242 -2.0614  0.3232 3.0704 1.0196 267\ntemp        -1.2186 1.0959 -3.2786 -1.2776 1.1046 1.0246 267\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8408 0.2706 -2.3623 -1.8447 -1.2875 1.0066 1117\nrain         0.1666 0.2196 -0.2592  0.1611  0.6073 1.0038 3054\n\n\nWe see that tree has a positive effect on occupancy and temp has a negative effect. This is how the simulation was carried out, but the values do not match up particularly well. Specifically, in the simulation tree was set to 0.3 but has been estimated as 0.2, while temp was set to -0.2 but has been estimated as -1.2.\nSo we’ve got the general relationships reasonably well estimated but it’s not particularly accurate. This is where having credible intervals is useful. Keep in mind that credible intervals are, correctly, interpreted as having a 95% chance of containing the “True” value (the True value here is what each parameter was set to in the simulation). For both tree and temp the True parameter value is indeed within the 95% credible intervals! Both 95% CI are wide, but that’s good here! The model isn’t entirely sure what the values are (partly because of sample size and the effects being subtle) and that is being conveyed to us! We’re not deluding ourselves into thinking we have a perfect understanding when we don’t!\nThere are some other warning signs that the model might not be performing especially well. The first is that the Rhat values are getting uncomfortably large for the Occurence parameters. My personal rule of thumb is Rhat values close to 1.05 is where I get worried. None of our parameters are at 1.05 but they’re close enough that I’m paying attention to them and want to see if I can fix the problem.\nThe other warning sign is the ESS for the Occurence model are all “low”. At least noticeably lower than the Detection model parameters. Again, the rule of thumb here is that we want hundreds or thousands of “Effective Sample Size”, so we’re technically OK but I’m still concerned.\nNow, to be clear, we ignored these issues with the first model we ran in this document, we here we’re going to see if we can’t resolve this problem.\n\n\nResolving issues\nTo fully appreciate the solution we’ll attempt requires a deeper understanding of Bayesian statistics. But for now, we’re not going to cover this. Instead, I’ll simply say that the “machinery” of Bayesian statistics revolves around giving a number of algorithms (called “chains”) a number of guesses (called “iterations”) to try and figure out the most likely values for our parameters.\nThe two metrics we used above, Rhat and ESS, both monitor these chains and iterations and let us know if they are not in agreement. When Rhat values get high, and ESS values get low, it can suggests we simply need to give the algorithms more iterations to figure out the best values.\nInevitably, the details are more complex than I can convey in two paragraphs but the general idea is there. If either number looks a bit worrying, it’s relatively cheap and easy to just rerun the model with more iterations and see if that solves the problem.\nLet’s do just that by increasing the number of iterations to 3000 per chain:\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # We're using four chains/algorithms\n  n.chains = 4,\n  # We allow 3000 guesses (increased from 2000)\n  n.samples = 3000,\n  # We ignore the first 300 (increased from 200)\n  # We ignore them because we assume the algorithms are not particularly reliable\n  # in the first ca. 10% of guesses\n  n.burn = 300,\n  verbose = FALSE)\n\n\nOnce run, we can check how well they ran:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 3000, verbose = FALSE, n.burn = 300, n.chains = 4)\n\nSamples per Chain: 3000\nBurn-in: 300\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 10800\nRun Time (min): 0.0253\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0655 1.0246  0.1977  2.0138 4.2114 1.0010 529\ntree         0.3721 1.3145 -2.0763  0.3028 3.0319 1.0662 375\ntemp        -1.2063 1.1214 -3.3214 -1.2363 1.1050 1.0220 422\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8530 0.2644 -2.3777 -1.8521 -1.3301 1.0020 1914\nrain         0.1715 0.2210 -0.2506  0.1682  0.6072 1.0004 4390\n\n\nNow when we look at the Rhat and ESS values, we doing better. Still not perfect but enough for me to be happy that by these two metrics alone there’s nothing to suggest the machinery is struggling and that the parameters are being estimated as robustly as possible.\n\nImportantly, these are not the only metrics or tools available to us. We’ll check out the other options in a later document.\n\nAs far as we can tell, for now, we can trust this model and move on to plotting the predicted relationships.\n\n\nPlot predicted relationships\nPreviously, we made the predictions ourselves by “hand”. This time, we’re going to use the predict() function to do this for us to make life easier on ourselves. Importantly, the process is exactly the same;\n\nCreate a “fake” dataset with the covariate values we want to predict for.\nApply this fake data to our equation, with the now estimated parameter values, to generate the predictions.\nConvert from logit values to probabilities.\nPlot the predicted probabilities against the fake covariate values to show the estimated relationship.\n\n\n\nCode\nX.0 &lt;- cbind(\n  1,\n  temp = seq(from = min(occ.covs$temp), \n             to = max(occ.covs$temp),\n             length.out = 50),\n  tree = 0\n)\n\n#predict(fit, X.0)\n\n\n\n\nHow should I store my data?\nIn order to be able to run the model, we need the data to be in a particular order"
  }
]