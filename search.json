[
  {
    "objectID": "Occupancy_Models.html",
    "href": "Occupancy_Models.html",
    "title": "Introduction to Occupancy Models",
    "section": "",
    "text": "A fundamental question in ecology is; Where are species present, and why are they present there?\n\nZitong, in your case this may be; Why are roe deer present at site A but not site B? Is it because site A is closer to roads? If a site is close to a road what’s the probability roe deer are present? Is that very different to sites that are far away from a road?\nWhat about features of the habitat? Are roe deer more likely if a site is forested versus agricultural?\n\n\nJoe, in your case this may be; Why are pheasants present at site A but not site B? Is it related to how far away the nearest known release pen is? What does the relationship look like? If there is no relationship, what would that mean for policy?\nWhat about the habitat makes pheasants more likely to occupy that site?\n\nTo act as motivation for this intro to occupancy models, let’s say we’re interested in understanding why are elephants present in the north of Etosha nature reserve in Namibia, but not in the south of the reserve? (As a side note, I’d recommend going there on holiday if you ever get the chance). Are elephants present in some parts of Etosha due to water availability? Is it related to food? Could predators play a role?\nTo find out, we need stats. As useful as gut feelings and intuition are, it’s not enough. Remember, statistics is how we get the evidence to answer our research question using data.\nThe best statistical tool available for answering these types of questions are “occupancy modeling”. These were originally developed by Daryl MacKenzie et al in 2002 who wanted to deal with a subtle issue when trying to find animals (more on this in a second). The broad concept underlying MacKenzie’s occupancy modelling framework built on ideas that had already been developed for estimating survival of individual animals, called the Cormack-Jolly-Seber, or CJS, model. As a cool note; Cormack and Jolly independently developed this model while they worked in Aberdeen University - a vital and internationally renowned model was developed right here in bloody Aberdeen! In my entire time as an undergraduate student, then as a PhD in Aberdeen no one ever told me this (even though I was taught these methods in Aberdeen)! I only found out at a conference in France years after leaving Aberdeen. As a school, we should be far more proud of this than we are.\nThe problem that occupancy models solve is subtle, yet absolutely crucial. Occupancy models estimate the probability a species occupies a site, and also the probability that you see them.\nAssume I want to determine where elephants are present in Etosha nature reserve in Namibia. I need data to answer this, so I decide to do ten elephant surveys in Etosha. Specifically, I go to visit these ten sites that are nicely spread throughout the park. At each site, whenever I spot an elephant I note down that elephants are present at that location. When I don’t see any elephants I record that elephants are absent at that location. Simple, right?\nLet’s imagine we collect data across ten sites. Here’s what our dataset might look like:\n\n\nCode\nset.seed(1988)\netosha &lt;- data.frame(\n  site = 1:10,\n  elephants = rbinom(n = 10, size = 1, prob = 0.4)\n)\netosha\n\n\n   site elephants\n1     1         0\n2     2         0\n3     3         0\n4     4         0\n5     5         0\n6     6         0\n7     7         0\n8     8         1\n9     9         1\n10   10         0\n\n\nWhen elephants is 1, it means elephants were seen at that site; when it’s 0, they were not. So, 1 indicates presence, and 0 indicates absence; at least for now.\nTo figure out the average probability that any one of my ten sites are occupied, I can run a Bernoulli Generalised Linear Model. The Bernoulli distribution (named after Jacob Bernoulli) generates values of 1 or 0, which is perfect for our binary presence–absence data.\nThe model would be:\n\\[\ny_i \\sim Bernoulli(p_i) \\\\\n\\]\n\\[\nlogit(p_i) = \\beta_0\n\\]\nLet’s go through these equations slowly:\n\n\\(y\\) is our observation (elephants in the etosha dataset)\n\\(i\\) is the index, here being which site the data was collected from\n\\(\\sim\\) means “generated according to” (or “our data is the same as would be generated by the following distribution”)\n\\(Bernoulli\\) is a type of distribution that will generate either 0 or 1\n\\(p\\) is the probability of success (i.e. there is \\(p\\) probability that we see an elephant). We can’t possibly know what \\(p\\) is when we collect the data, so we need to figure it out with statistics. (The \\(i\\) means each site could have a different probability - but we’re not doing that yet)\n\\(logit\\) is the link function to ensure that \\(p\\) remains between 0% and 100%. Specifically, it’s a little bit of maths: \\(log(\\frac{p}{1-p})\\), which is the natural log of the probability to succeed (\\(p\\)) divided by the probability to fail (\\(1-p\\)). More on this in a second.\n\\(\\beta_0\\) is the intercept. Since we haven’t included any covariates (yet), it captures the average logit value of elephant presence across all sites. When converted back from the logit scale, this will give us the average probability of elephant presence.\n\nTo understand how the logit link transforms probabilities, have a look at this figure. It maps the full range of probabilities (0% to 100%) to the logit scale.\n\n\nCode\np &lt;- seq(from = 0, to = 1, by = 0.001)\nlogit &lt;- log(p/(1-p))\ndat &lt;- data.frame(p, logit)\nlibrary(ggplot2)\nggplot(dat) +\n  geom_line(aes(x = logit, y = p)) +\n  labs(x = \"Logit value that\\nelephants are present\",\n       y = \"Probability that\\nelephants are present\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNotice how as the probability approaches 0%, it never actually drops below it, even as the logit value keeps decreasing? And the same happens when it gets close to 100%. And that’s exactly what we want. The fact that logit values can range from \\(-\\infty\\) to \\(+\\infty\\) makes model fitting much easier: it turns a bounded probability into an unbounded scale, allows linear relationships with predictors, and ensures valid probabilities when converting predictions back. So even if our model estimates a logit value of 999 billion, it still maps to a probability of 100%. This ensures the natural bounds of probability, which is between 0% and 100%, are always respected.\nWith that in mind, here’s how we’d fit this model in R. (Click Code to reveal the code.)\n\n\nCode\nmod &lt;- glm(elephants ~ 1,\n           data = etosha,\n           family = binomial(link = \"logit\"))\n\n\nThis estimates \\(\\beta_0\\), the average probability of elephant presence, but on the logit scale. We can see what it has been estimated as by looking at the summary of the model:\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nglm(formula = elephants ~ 1, family = binomial(link = \"logit\"), \n    data = etosha)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -1.3863     0.7906  -1.754   0.0795 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10.008  on 9  degrees of freedom\nResidual deviance: 10.008  on 9  degrees of freedom\nAIC: 12.008\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe estimate for (Intercept) (or \\(\\beta_0\\)) is -1.3863. Now that we have an estimate for \\(\\beta_0\\), we can plug it into our model:\n\\[\ny_i \\sim Bernoulli(p_i) \\\\\n\\]\n\\[\nlogit(p_i) = -1.3863\n\\]\nTo interpret this on the probability scale, we apply the inverse logit transformation using plogis() in R:\n\n\nCode\nplogis(-1.3863)\n\n\n[1] 0.1999991\n\n\nThis gives us a probability of approximately 0.2, or 20%, meaning there’s a 20% chance a site is occupied by elephants.\nOr does it?"
  },
  {
    "objectID": "Occupancy_Models.html#state-model",
    "href": "Occupancy_Models.html#state-model",
    "title": "Introduction to Occupancy Models",
    "section": "State model",
    "text": "State model\nThe first part of an occupancy model is the state model, so called because it estimates the true state of each site: is the species present or absent?\n\\[\nz_i \\sim Bernoulli(\\psi_i)\\\\\n\\]\n\\[\nlogit(\\psi_i) = \\beta_0\n\\]\nThis model probably looks similar. That’s because like the first model we spoke about further above, it’s also a \\(Bernoulli\\) GLM. But they differ in two important ways:\n\n\\(z\\) is the true presence or absence of elephants in site \\(i\\) (this isn’t \\(y\\))\n\\(\\psi\\) is the probability to be present\n\nThat’s nice and all but just changing the labels doesn’t fix imperfect detection. To solve that, we need a second model: the observation model."
  },
  {
    "objectID": "Occupancy_Models.html#observation-model",
    "href": "Occupancy_Models.html#observation-model",
    "title": "Introduction to Occupancy Models",
    "section": "Observation model",
    "text": "Observation model\nThe second GLM (which I’ll call the observation model) also looks remarkably similar:\n\\[\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\n\\]\n\\[\nlogit(p_{i,j}) = \\alpha_0\n\\]\nIt’s another \\(Bernoulli\\) GLM but with some crucial differences.\n\n\\(y\\) is now the detection or not of an elephant. This is worth highlighting - \\(y\\) is not the presence or absence of elephants, it’s the detection of elephants if they’re present!\n\\(j\\) represents which survey \\(y\\) was collected in, which inherently means we have multiple surveys, not just one like in our first GLM example above where \\(i\\) was a single (of ten) sites. This has implications for how we collect data, which we’ll come back to.\n\\(p\\) is the probability to detect an elephant at site \\(i\\) in survey \\(j\\) (e.g. what is the probability to detect an elephant in site 1 in the third survey?)\n\nImportantly, the probability to detect elephants (\\(p\\)) is multiplied by \\(z\\). What’s \\(z\\)? Well that’s the true occupancy state of that site from the state model. It’s this multiplication that allows the two models to “speak” to each other, and how we deal with imperfect detection; it ensures the observation process depends on whether the species is actually present.\nIf there are elephants in a site (\\(z = 1\\)), then \\(p \\times z = p \\times 1 = p\\). If elephants are absent from a site (\\(z = 0\\)), then \\(p \\times z = p \\times 0 = 0\\). This means you cannot detect elephants if they aren’t there. That’s blindingly obvious… It’s so dumb that most people don’t realise you need to specify it (I’ve heard people say: “surely any model can figure this out?”). But this stupidly simple logic is missing from our starting GLM! And a lot of research asking about where species are use a model equivalent to the very first GLM on this page.\n\nKeep in mind that we don’t know \\(z\\). That’s the true occupancy state, a latent variable. Latent just means it’s unobserved: we can’t measure \\(z\\) directly in the field, but we can estimate it using the data we collect, much like we do with parameters.\n\nThat’s how occupancy models “know” that you can only detect elephants if they’re present, and that if they’re absent, you won’t detect them. It seems so obvious, yet this logic is missing from many models that treat non-detections as true absences.\nAnd that’s the beauty of occupancy models. They handle the messy reality of ecological data (but can also be used in e.g. medicine), and that’s why they’re one of my favorite types of analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to your site",
    "section": "",
    "text": "This site was made for you both and introduces the fundamentals and advanced topics in occupancy modelling that you’ll need to do your honours project. I’ve tried to write the material to be accessible for you both, in terms of your honours projects, but also to help boost your skillset for when you finish you undergraduate.\nAs always, if there is anything in the site that is either wrong or confusing, just let me know. If it helps, I would be amazed if you understood everything that’s covered, so it’s my prior belief that parts will need to be explained in person.\n\n\nTo get the most out of this material, I recommend the following order:\n\nOccupancy Models: The basics\nLearn the basic theory of occupancy models and how they help us infer species presence or absence from imperfect detection.\nOccupancy Models: Covariates\nLearn how site and survey covariates can improve our biological understanding and enhance the accuracy of our inferences.\nBayesian models: The concept\nLearn the basic theory of the Bayesian statistical framework: what’s a prior, and what’s a posterior?\nFitting Bayesian Models\nCombine occupancy models with the Bayesian framework to fit models using the spOccupancy package.\nTo be added – Spatial autocorrelation\nLearn the basic theory behind spatial autocorrelation.\nTo be added – Occupancy models with spatial autocorrelation\nCombine occupancy models, the Bayesian framework, and spatial autocorrelation to fit more advanced models that account for spatial structure.\n\nBe aware that this site is very much an on going work in progress. Let me know if you spot anything that needs some work.\nEnjoy!"
  },
  {
    "objectID": "index.html#suggested-order",
    "href": "index.html#suggested-order",
    "title": "Welcome to your site",
    "section": "",
    "text": "To get the most out of this material, I recommend the following order:\n\nOccupancy Models: The basics\nLearn the basic theory of occupancy models and how they help us infer species presence or absence from imperfect detection.\nOccupancy Models: Covariates\nLearn how site and survey covariates can improve our biological understanding and enhance the accuracy of our inferences.\nBayesian models: The concept\nLearn the basic theory of the Bayesian statistical framework: what’s a prior, and what’s a posterior?\nFitting Bayesian Models\nCombine occupancy models with the Bayesian framework to fit models using the spOccupancy package.\nTo be added – Spatial autocorrelation\nLearn the basic theory behind spatial autocorrelation.\nTo be added – Occupancy models with spatial autocorrelation\nCombine occupancy models, the Bayesian framework, and spatial autocorrelation to fit more advanced models that account for spatial structure.\n\nBe aware that this site is very much an on going work in progress. Let me know if you spot anything that needs some work.\nEnjoy!"
  },
  {
    "objectID": "Bayesian_Occupancy_Models.html",
    "href": "Bayesian_Occupancy_Models.html",
    "title": "Bayesian Algorithms",
    "section": "",
    "text": "In the previous page, we went through the underlying theory of Bayesian statistics; how our prior knowledge is updated using real data through Bayes’ Theorem. Now, we’re going to move into the practical application of these ideas using the spOccupancy package. My goal here is to help you understand how Bayesian occupancy models are fit, how to monitor convergence, and how to interpret uncertainty through credible intervals.\nWe’ll again work with the Etosha elephant dataset. Importantly, we are not yet including spatial autocorrelation. We’ll add that in the next section.\n\n\nCode\nlibrary(spOccupancy)\n\n# These are needed to produce the website\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggthemes)\n\n\n\nHow do we estimate a posterior?\nIn the previous page we spoke about posteriors and how these are a combination of our prior belief and our data. But how does that actually happen? Well, the answer is pretty simple but took hundreds of years for statisticians to figure out. Markov Chain Monte Carlo is an algorithm that was developed by scientists at Los Alamos in the 1950s working on the Manhattan Project (yes that Manhattan Project). The algorithm is called Markov Chain Monte Carlo (or MCMC) for two reasons. A Markov Chain describes a sequence of states, where the probability to move from one state to the next state depends only on what the current state is.\nFor example, imagine you are walking through a series of rooms. Where you go next depends only on the room you’re currently in. You can’t instantly teleport to the other side of the building! Nor does it depend on the path you have taken to get to your current room.\n\n\nWhat does MCMC look like?\nBelow is an animation of a simple MCMC algorithm to help give an intuition for what’s happening behind the scenes.\nFor this, we have the following simple linear model:\n\\[y_i \\sim Normal(\\mu_i, \\sigma^2)\\]\n\\[\\mu_i = \\beta_0 + \\beta_1 \\times x_i \\]\nSo our objective here is to figure out \\(\\beta_0\\) and \\(\\beta_1\\). To do so, we’ll set up an MCMC using one “chain”. A chain is a Markov Chain - it’s the black dot and orange line you can see in the animation below. This chain is trying to find the “True value” location marked with a teal X. At each step the black dot asks “if I take a step in a random direction, will I be closer to X or further away?”. If it’s closer, then it takes the step. If it’s further away, then it’s less likely to take the step but it’s not impossible. This seems like a bad choice. Why move somewhere if it’s further away from X? The answer is a bit nuanced, but the concise explanation is that sometimes there might be areas of the “parameter space” (the parameter space is all possible values of \\(\\beta_0\\) and \\(\\beta_1\\)) which “work” quite well despite not being the true value. If we didn’t have the behaviour, where the chain might take a step even though it’s worse, then we might get stuck in one of these “bad” areas (technically, these “bad” areas are called “local minima”).\nHere’s our MCMC in action:\n\n\nCode\ntrue_param &lt;- c(2, -1)\ntarget_density &lt;- function(x, y) {\n  exp(-0.5 * ((x - true_param[1])^2 / 1^2 + (y - true_param[2])^2 / 0.5^2))\n}\n\nset.seed(123)\nn_iter &lt;- 100\nx &lt;- y &lt;- 0\nsamples &lt;- tibble(x = x, y = y, iteration = 1)\n\nfor (i in 2:n_iter) {\n  x_prop &lt;- rnorm(1, x, 0.4)\n  y_prop &lt;- rnorm(1, y, 0.4)\n  accept_ratio &lt;- target_density(x_prop, y_prop) / target_density(x, y)\n  \n  if (runif(1) &lt; accept_ratio) {\n    x &lt;- x_prop\n    y &lt;- y_prop\n  }\n  samples &lt;- samples |&gt; add_row(x = x, y = y, iteration = i)\n}\n\nggplot(samples, aes(x = x, y = y)) +\n  geom_path(color = \"#FF5733\", linewidth = 0.7) +\n  geom_point(aes(x = x, y = y), color = \"black\", size = 2) +\n  geom_point(aes(x = true_param[1], y = true_param[2]), \n             color = \"#00A68A\", size = 3, shape = 4, stroke = 2) +\n  annotate(\"text\", x = true_param[1] + 0.1, y = true_param[2],\n           label = \"True value\", color = \"#00A68A\", hjust = 0) +\n  transition_reveal(iteration) +\n  coord_fixed() +\n  labs(\n    title = \"MCMC Chain Step: {frame_along}\",\n    x = bquote(beta[0]),\n    y = bquote(beta[1])\n  ) +\n  theme_bw()\n\n\n\nHere’s where the really clever bit comes in. At each iteration, if we record the parameter value it tried, and store it, when we build it up the end result is our posterior! This is what made Bayesian statistics possible! We don’t need to use any crazy (and often impossible) maths to figure out the posterior, we just have MCMC walk around and the end result is an insanely good reflection of the posterior!\n\n\nCode\nsamples_long &lt;- samples |&gt;\n  pivot_longer(cols = c(x, y), names_to = \"parameter\", values_to = \"value\")\n\nsamples_long &lt;- samples_long |&gt;\n  mutate(true_value = if_else(parameter == \"x\", true_param[1], true_param[2]))\n\nsamples_long_cumulative &lt;- samples_long |&gt;\n  group_by(parameter) |&gt;\n  group_split() |&gt;\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    map_dfr(1:max(df$iteration), function(i) {\n      df |&gt;\n        filter(iteration &lt;= i) |&gt;\n        mutate(frame = i, parameter = param)\n    })\n  })\n\nsamples_long_cumulative &lt;- samples_long_cumulative %&gt;%\n  mutate(parameter_label = case_when(\n    parameter == \"x\" ~ \"beta[0]\",\n    parameter == \"y\" ~ \"beta[1]\"\n  ))\n\nggplot(samples_long_cumulative, aes(x = value)) +\n  geom_histogram(binwidth = 0.4, fill = \"#FF5733\", color = \"white\", boundary = 0) +\n  geom_vline(aes(xintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", ncol = 1, labeller = label_parsed) +\n  transition_manual(frame) +\n  labs(title = \"Cumulative Posterior up to Iteration {current_frame}\",\n       x = \"Parameter Value\", y = \"Frequency\") +\n  theme_bw()\n\n\n\nAnother way to show what the MCMC was up to is using something called “traceplots”. These are relatively simple but actually quite powerful for determining if we trust the model output. We’ll come back to this in a bit, but for now, we can show the MCMC exploring the parameter space using these traceplots.\n\n\nCode\nsamples_long_cumulative_trace &lt;- samples_long |&gt;\n  group_by(parameter) |&gt;\n  group_split() |&gt;\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    map_dfr(1:max(df$iteration), function(i) {\n      df |&gt;\n        filter(iteration &lt;= i) |&gt;\n        mutate(frame = i, parameter = param)\n    })\n  })\n\nsamples_long_cumulative_trace &lt;- samples_long_cumulative_trace %&gt;%\n  mutate(parameter_label = case_when(\n    parameter == \"x\" ~ \"beta[0]\",\n    parameter == \"y\" ~ \"beta[1]\"\n  ))\n\nggplot(samples_long_cumulative_trace, aes(x = iteration, y = value)) +\n  geom_line(color = \"#FF5733\", linewidth = 0.8) +\n  geom_hline(aes(yintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", ncol = 1, labeller = label_parsed) +\n  transition_manual(frame) +\n  labs(title = \"Traceplot up to Iteration {current_frame}\",\n       x = \"Iteration\", y = \"Parameter Value\") +\n  theme_bw()\n\n\n\n\n\nImproving our MCMC\nWe’ve made a good start but we can improve this quite a bit. Firstly, limiting the algorithm to 100 iterations doesn’t give it many opportunities to find the true value. In general, you often give MCMC thousands of iterations, rather than a paltry 100. So first improvement is to increase the number of iterations (you may remember we did this when we were going through the occupancy theory pages - now you know why!).\nSecondly, we’re using one MCMC chain. Why not more? Afterall, if we have say four chains, then if all four agree that they’re close to the true value that would give us more comfort. If they find different “True values”, well, then it seems likely that we haven’t actually found it.\nLet’s implement our improvement and see what our plots now look like:\n\n\nCode\ntrue_param &lt;- c(2, -1)\n\ntarget_density &lt;- function(x, y) {\n  exp(-0.5 * ((x - true_param[1])^2 / 1^2 + (y - true_param[2])^2 / 0.5^2))\n}\n\nset.seed(123)\nn_iter &lt;- 1000\nn_chains &lt;- 4\n\nchains_list &lt;- map_dfr(1:n_chains, function(chain_id) {\n  x &lt;- y &lt;- 0  # Start at (0, 0)\n  samples &lt;- tibble(x = x, y = y, iteration = 1, chain = chain_id)\n  \n  for (i in 2:n_iter) {\n    x_prop &lt;- rnorm(1, x, 0.4)\n    y_prop &lt;- rnorm(1, y, 0.4)\n    accept_ratio &lt;- target_density(x_prop, y_prop) / target_density(x, y)\n    \n    if (runif(1) &lt; accept_ratio) {\n      x &lt;- x_prop\n      y &lt;- y_prop\n    }\n    \n    samples &lt;- samples %&gt;% add_row(x = x, y = y, iteration = i, chain = chain_id)\n  }\n  samples\n})\n\nggplot(chains_list, aes(x = x, y = y, group = chain, color = as.factor(chain))) +\n  geom_path(linewidth = 0.7) +\n  geom_point(aes(x = x, y = y), size = 1.5) +\n  geom_point(aes(x = true_param[1], y = true_param[2]), \n             color = \"white\", size = 4, shape = 4, stroke = 2, inherit.aes = FALSE) +\n  annotate(\"text\", x = true_param[1] + 0.1, y = true_param[2],\n           label = \"True value\", color = \"white\", hjust = 0) +\n  transition_reveal(along = iteration) +\n  coord_fixed() +\n  scale_color_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"MCMC Chains Step: {round(frame_along, digits = 0)}\",\n    x = bquote(beta[0]),\n    y = bquote(beta[1])) +\n  theme_bw()\n\n\n\n\n\nCode\nsamples_long &lt;- chains_list %&gt;%\n  pivot_longer(cols = c(x, y), names_to = \"parameter\", values_to = \"value\") %&gt;%\n  mutate(true_value = if_else(parameter == \"x\", true_param[1], true_param[2]),\n         parameter_label = case_when(\n           parameter == \"x\" ~ \"beta[0]\",\n           parameter == \"y\" ~ \"beta[1]\"\n         ))\n\n# Build cumulative data\nsamples_long_cumulative &lt;- samples_long %&gt;%\n  group_by(parameter, chain) %&gt;%\n  group_split() %&gt;%\n  map_dfr(function(df) {\n    param &lt;- unique(df$parameter)\n    chain_id &lt;- unique(df$chain)\n    map_dfr(1:max(df$iteration), function(i) {\n      df %&gt;%\n        filter(iteration &lt;= i) %&gt;%\n        mutate(frame = i, parameter = param, chain = chain_id)\n    })\n  })\n\nggplot(samples_long_cumulative, aes(x = value, fill = as.factor(chain))) +\n  geom_histogram(binwidth = 0.4, color = \"white\", boundary = 0, \n                 position = position_dodge(), alpha = 0.6) +\n  geom_vline(aes(xintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_x\", labeller = label_parsed, ncol = 1) +\n  transition_manual(frame) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"Cumulative Posterior up to Iteration {current_frame}\",\n       x = \"Parameter Value\", y = \"Frequency\") +\n  theme_minimal()\n\nggplot(samples_long_cumulative, aes(x = iteration, y = value, color = as.factor(chain))) +\n  geom_line(linewidth = 0.7) +\n  geom_hline(aes(yintercept = true_value), linetype = \"dashed\", color = \"#00A68A\", linewidth = 1) +\n  facet_wrap(~parameter_label, scales = \"free_y\", labeller = label_parsed, ncol = 1) +\n  transition_manual(frame) +\n  scale_color_brewer(palette = \"Set1\", name = \"Chain\") +\n  labs(title = \"Traceplot up to Iteration {current_frame}\",\n       x = \"Iteration\", y = \"Parameter Value\") +\n  theme_minimal()\n\n\n\n\nI have a bit more confidence in our posteriors now:\n\nAll chains seem to agree on the same value (they have “converged” to the same answer)\nThe posteriors look reasonably well estimated (they’ll close to the true value - but keep in mind that with real analysis we don’t know what the truth is)\nThe traceplots resemble “hairy caterpillars”, which matches the Markov Chain idea that the next value depends only on the current value (it doesn’t “remember” older values).\n\nThe final thing I want to bring your attention to are the first hundred or so iterations. Generally, these iterations can be pretty wild, fluctuating massively. If you think about it, that’s kind of fair enough. We’re starting each MCMC chain at random locations, so it’s fair if things are a bit wobbly at the start. So what we do with these early iterations is to simply ignore them. These are called “burn in” iterations, as in, we’re letting the engine warm up, so these are just used to get up to speed. Mostly arbitrarily, we typically ignore the first 10% of iterations, so where we’ve used 1000 iterations, we’d generally ignore the first 100.\n\n\nFitting the occupancy model using MCMC\nLet’s now revisit our Bayesian model. As a reminder, here’s what it looked like:\n\\[z_i \\sim Bernoulli(\\psi_i)\\\\\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\nWhich we can translate into the following code. But pay attention to the n.chains, n.samples and n.burn arguments. This is where we specify how many chains we want (n.chains), how many iterations we want (n.samples, note an iteration can also be called a sample), and how many of the first iterations we want to ignore (n.burn).\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  n.chains = 4,     # 4 chains just like in our simple example\n  n.samples = 2000, # 2000 iterations for each chain\n  n.burn = 200,     # We ignore the first 200 iterations to give MCMC a chance to get it's feet\n  \n  verbose = FALSE   # This just says don't spit out details while fitting \n  # (normally I would leave verbose = TRUE so I can keep track of the model while it's fitting)\n  )\n\n\nWe can now check how our model worked. The code is pretty simple. The only tricky thing is to specify if you want beta or alpha. Importantly, beta here refers to the occupancy parameters, and alpha refers to the detection parameters.\nSo here are the traceplots and posteriors for \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\) in:\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\n\nCode\nplot(fit, 'beta') # Occupancy parameters.\n\n\n\n\n\n\n\n\n\nAnd here are the traceplots and posteriors for \\(\\alpha_0\\) and \\(\\alpha_1\\) in:\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\n\n\nCode\nplot(fit, 'alpha') # Detection parameters.\n\n\n\n\n\n\n\n\n\nAnd we can also get a more numeric summary of the model using summary(). Importantly, the information you see below is the same as the data in the figures above. It’s just summarised! So the Mean is just the mean of the posteriors above!\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0172\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0536 1.0657  0.1991  1.9727 4.2706 1.0151 315\ntree         0.2259 1.2968 -2.1384  0.1544 2.9006 1.0797 278\ntemp        -1.1362 1.1440 -3.2916 -1.1758 1.3257 1.0396 272\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8539 0.2623 -2.3783 -1.8533 -1.3435 1.0018 1142\nrain         0.1730 0.2207 -0.2558  0.1719  0.6087 1.0015 2994\n\n\n\n\nCredible intervals\nNotice the 2.5% and 97.5% in the tables above? These are the credible intervals that I briefly mentioned in the Occupancy Models: Covariates page. Technically, these are just the quantiles of the posterior. Or, phrased alternatively, 95% of all iterations are within this interval. And keep in mind, that a Bayesian credible interval is not the same as a frequentist confidence interval. For our purposes, these intervals represent a 95% probability to contain the true value (based on the data we collected and the model we fit)!\n\n\nCode\nbeta_0_samples &lt;- fit$beta.samples[,1]\n\nbeta_0_df &lt;- tibble(beta_0 = beta_0_samples)\n\nsummary_stats &lt;- beta_0_df %&gt;%\n  summarise(\n    mean = mean(beta_0),\n    median = median(beta_0),\n    lower = quantile(beta_0, 0.025),\n    upper = quantile(beta_0, 0.975)\n  )\n\nggplot(beta_0_df, aes(x = beta_0)) +\n  geom_histogram(fill = \"#00A68A\", colour = \"white\", alpha = 0.5, bins = 50) +\n  \n  # Vertical lines\n  geom_vline(xintercept = summary_stats$mean, linetype = \"solid\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$median, linetype = \"dashed\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$lower, linetype = \"dotted\", color = \"black\", linewidth = 1) +\n  geom_vline(xintercept = summary_stats$upper, linetype = \"dotted\", color = \"black\", linewidth = 1) +\n\n  # curved arrows\n  geom_curve(aes(x = summary_stats$mean + 0.5, y = 400, \n                 xend = summary_stats$mean, yend = 350), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = 0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$median - 0.5, y = 300, \n                 xend = summary_stats$median, yend = 250), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = -0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$lower - 0.5, y = 100, \n                 xend = summary_stats$lower, yend = 75), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = -0.3, color = \"black\") +\n  geom_curve(aes(x = summary_stats$upper + 0.5, y = 100, \n                 xend = summary_stats$upper, yend = 75), \n             arrow = arrow(length = unit(0.02, \"npc\")), curvature = 0.3, color = \"black\") +\n\n  annotate(\"text\", x = summary_stats$mean + 0.5, y = 400, label = \"Mean\", hjust = 0, size = 5) +\n  annotate(\"text\", x = summary_stats$median - 0.5, y = 300, label = \"Median\", hjust = 1, size = 5) +\n  annotate(\"text\", x = summary_stats$lower - 0.5, y = 100, label = \"2.5% CI\", hjust = 1, size = 5) +\n  annotate(\"text\", x = summary_stats$upper + 0.5, y = 100, label = \"97.5% CI\", hjust = 0, size = 5) +\n\n  labs(\n    x = expression(paste(beta[0], \" or '(Intercept)'\")),\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom our summary table, the mean for the \\(\\beta_0\\) or (Intercept) posterior was estimated as 2.0536, the median (called 50% in the summary table) was 1.9727, and the 95% credible intervals were 0.1991 and 4.2706, just like we see in the figure.\nThis might seem trivial but having credible intervals be so simple, and the interpretation having a useful meaning is one of the big selling points of Bayesian analysis in my mind. It’s not something to trivialise.\n\n\nSpecifying priors\nSo far, we’ve let the spOccupancy package handle setting the priors for us behind the scenes. But we can actually specify our own priors if we want to.\nIn the background, spOccupancy uses something called Pólya-Gamma data augmentation, and under the hood, this method assumes \\(Normal\\) priors for both the occupancy and detection parameters (including their intercepts).\nIf you don’t specify anything, spOccupancy will set:\n\nThe “hypermean” (mean of \\(Normal\\) distribution for all priors) = 0\nThe “hypervariance” (variance \\(Normal\\) distribution for all priors) = 2.72\n\nThis corresponds to a relatively flat prior on the probability scale (the 0–1 scale for occupancy or detection probabilities). In other words, by default, the prior doesn’t strongly pull your estimates toward any particular value; it lets the data mostly speak for itself.\nBut we can set these explicitly if we want to! Here’s what that looks like:\n\n\nCode\n# Specify priors for detection (alpha) and occupancy (beta)\npriors &lt;- list(\n  alpha.normal = list(mean = 0, var = 2.72),  # Detection priors\n  beta.normal = list(mean = 0, var = 2.72)    # Occupancy priors\n)\n\n\nThe above code is basically doing what spOccupancy does by default. All priors will get \\(Normal(0,2.72)\\) in both the detection and occupancy sub models. We can either give a single number (like 0 and 2.72 above), and it will apply to all parameters, or you can give a vector if you want different priors for different parameters.\nHere’s how we’d give different priors for each parameter:\n\n\nCode\n# Different priors for each occupancy parameter\noccurrence_priors &lt;- list(\n  mean = c(0, 0.5, -0.5),    # One mean for each occupancy parameter\n  var = c(2.72, 1, 2)        # One variance for each occupancy parameter\n)\n\n# Different priors for each detection parameter\ndetection_priors &lt;- list(\n  mean = c(0, 0.2),          # One mean for each detection parameter\n  var = c(0.5, 1)           # One variance for each detection parameter\n)\n\n# Combine into the priors list\npriors &lt;- list(\n  beta.normal = occurrence_priors,\n  alpha.normal = detection_priors\n)\n\n\nThis would translate into this model:\n\\[z_i \\sim Bernoulli(\\psi_i)\\\\\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[\\beta_0 \\sim Normal(0, 2.72)\\]\n\\[\\beta_1 \\sim Normal(0.5, 1)\\]\n\\[\\beta_2 \\sim Normal(-0.5, 2)\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\n\\[\\alpha_0 \\sim Normal(0, 0.5)\\]\n\\[\\alpha_1 \\sim Normal(0.2, 1)\\]\nNow, to be very, very clear; I am choosing these priors completely at random just for demonstration. I have no reason, in this case, to think that a reasonable prior for \\(\\beta_1\\) is \\(Normal(0.5, 1)\\).\nBut we can always fit this model to see what happens:\n\n\nCode\nfit_priors &lt;- PGOcc(\n  occ.formula = ~ tree + temp,\n  det.formula = ~ rain,\n  data = etosha,\n  priors = priors,     # We add our priors here\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE\n)\n\n\nHaving fit, we can see what the posteriors look like:\n\n\nCode\nplot(fit_priors, 'beta') # Occupancy parameters.\n\n\n\n\n\n\n\n\n\nCode\nplot(fit_priors, 'alpha') # Detection parameters.\n\n\n\n\n\n\n\n\n\nAnd pull up the summary:\n\n\nCode\nsummary(fit_priors)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    priors = priors, n.samples = 2000, verbose = FALSE, n.burn = 200, \n    n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0152\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.5053 0.9644 -0.2024  1.4383 3.5766 1.0514 304\ntree         0.5140 0.9042 -1.2174  0.4798 2.3414 1.0165 412\ntemp        -1.2473 0.9496 -3.0085 -1.2664 0.7709 1.0097 365\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.6451 0.2634 -2.1407 -1.6515 -1.0959 1.0112 1004\nrain         0.1595 0.2133 -0.2483  0.1565  0.6006 1.0048 3385\n\n\nIf we compare this with the original model, where we left the priors at their defaults we can see that different priors can lead to different posteriors. Keep in mind how Bayesian statistics work - the posterior is a combination of our data and prior beliefs. We’ve changed our prior belief and our posteriors have changed as a result.\nNow in this case, the difference in posteriors are pretty minimal. It might not always be. And in truth, we would want to think a bit more carefully about the priors. Don’t be fooled here. Just because my randomly chosen priors are different from the default ones doesn’t mean the default model is necessarily better. The best option is to choose priors you think are reasonable. If you have different prior beliefs (e.g. I have some informative priors but also some uninformative priors) I can run the model with both, and see how much of a difference it makes. This is something called “prior sensitivity analysis” and is a fairly useful tool to have in your back pocket when you’re not sure about your priors.\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0172\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  2.0536 1.0657  0.1991  1.9727 4.2706 1.0151 315\ntree         0.2259 1.2968 -2.1384  0.1544 2.9006 1.0797 278\ntemp        -1.1362 1.1440 -3.2916 -1.1758 1.3257 1.0396 272\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8539 0.2623 -2.3783 -1.8533 -1.3435 1.0018 1142\nrain         0.1730 0.2207 -0.2558  0.1719  0.6087 1.0015 2994\n\n\n\n\nFin\nWith that, you should be good to run a Bayesian occupancy model. Something you can add to your CV and have employers fawn over you. Even better if you understand it, so that when they ask you about it you can have a conversation!\nThe only part left in the analysis is including “spatial autocorrelation”. We’ll cover that in the next page.\n\n\nA subtle point: Priors and the link function\nWhen we choose priors in a Bayesian model, it’s really important to remember what scale those priors live on.\nIn our occupancy model, we specify priors for the parameters on the logit scale, not directly on the probability (0–1) scale.\nFor example, if we write:\n\\[logit(p_i) = \\beta_0 + \\beta_1 \\times x_i\\]\nthen \\(\\beta_0\\) and \\(\\beta_1\\) are in logit space.\nRemember, the logit function stretches the 0–1 probability scale onto the whole real line:\n\nProbabilities near 0.5 correspond to logits near 0.\nProbabilities near 0 or 1 correspond to logits of \\(-\\infty\\) and \\(+\\infty\\).\n\nThis means that a \\(Normal\\) prior with mean 0 and large variance on the logit scale is not flat on the probability scale! Even “uninformative” \\(Normal\\) priors on the logit scale can actually imply very strong beliefs on the probability scale.\nTo build some intuition, we’ll do a little prior predictive simulation:\n\nWe’ll randomly draw values for \\(\\beta_0\\) and \\(\\beta_1\\) from a \\(Normal(0, 2.72)\\) prior.\nWe’ll simulate the relationship between \\(x\\) and \\(p(x)\\) by plugging those \\(\\beta_0\\) and \\(\\beta_1\\) values into the logit equation.\nWe’ll repeat this 100 times to show many possible relationships.\n\n\n\nCode\nset.seed(123)\n\nn_draws &lt;- 100\nx_seq &lt;- seq(-3, 3, length.out = 100)\n\nbeta_0_draws &lt;- rnorm(n_draws, mean = 0, sd = sqrt(2.72))\nbeta_1_draws &lt;- rnorm(n_draws, mean = 0, sd = sqrt(2.72))\n\nprior_simulations &lt;- map2_dfr(\n  beta_0_draws, beta_1_draws,\n  .f = function(b0, b1) {\n    tibble(\n      x = x_seq,\n      logit_p = b0 + b1 * x,\n      p = plogis(logit_p)\n    )\n  },\n  .id = \"draw\"\n)\n# Plot\nggplot(prior_simulations, aes(x = x, y = p, group = draw)) +\n  geom_line(alpha = 0.2, color = \"#FF5733\") +\n  theme_minimal() +\n  labs(\n    title = \"Prior Predictive Simulation\",\n    x = \"Covariate (x)\",\n    y = \"Probability (p)\"\n  )\n\n\n\n\n\n\n\n\n\nEach orange line is a possible relationship between \\(x\\) and \\(p(x)\\) given the priors we chose. Notice that some lines are almost flat at 0 or 1? While others are very steep, flipping from 0 to 1 over a narrow range of \\(x\\)? Even though the prior on \\(\\beta_0\\) and \\(\\beta_1\\) was centered at 0 with large variance, the resulting priors on \\(p\\) are not uniform or “neutral.”\nIf I were being hyper cautious, I might be worried these priors are pushing the model towards the extreme flipping behaviour. In some cases that might be good, in others it might be bad.\nThe broader points I am making here are:\n\nDon’t stress too much about priors. If you have a lot of data your prior will often not be terribly important.\nBut give a little thought as to what a sensible prior would be, especially when working with link functions.\n\nIf you’re in doubt speak with me! I think having a discussion about your priors would be an excellent use of one of our meetings (hint, hint)."
  },
  {
    "objectID": "Bayesian_Statistics.html",
    "href": "Bayesian_Statistics.html",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "",
    "text": "Most of the statistics you’ve probably learned so far has been based in what’s called “frequentist” statistics. The analysis you’ll be using for occupancy models (at least this particular implementation) will be using something called “Bayesian” statistics (pronounced “Bay-zee-’n”). These are the two main frameworks currently used to learn something from data (unless you want to start splitting philosophical hairs). Within this document we’re going to start with what Bayesian statistics is, how to move from frequentist to Bayesian statistics, and how a Bayesian model works. We’ll leave occupancy models to the side for the time being."
  },
  {
    "objectID": "Bayesian_Statistics.html#your-prior-belief-before-seeing-the-data",
    "href": "Bayesian_Statistics.html#your-prior-belief-before-seeing-the-data",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "Your Prior Belief (before seeing the data)",
    "text": "Your Prior Belief (before seeing the data)\nYou start off thinking you have a 50% chance of passing. That doesn’t mean you’re certain, it’s just your best guess. You might be unsure, so you’re open to the idea that it could be lower or higher. Your prior might look like this:\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\ntheta &lt;- seq(0, 1, length.out = 1000)\n\nprior &lt;- dbeta(theta, 20, 20)\n\ndata.frame(theta, prior) |&gt;\n  ggplot(aes(x = theta, y = prior)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Chance of Passing\", y = \"Belief Density\") +\n  theme_minimal()"
  },
  {
    "objectID": "Bayesian_Statistics.html#the-data-100-students-passed",
    "href": "Bayesian_Statistics.html#the-data-100-students-passed",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "The Data (100 students passed)",
    "text": "The Data (100 students passed)\nNow you find out that 100 students have taken the thesis before you, and all of them passed. That’s really strong evidence that the pass rate is high, probably very near to 100%.\nWe can visualise how likely the observed data is for different values of the pass rate, this is the likelihood.\n\n\nCode\nlikelihood &lt;- theta^100\nlikelihood &lt;- likelihood / max(likelihood)\n\ndata.frame(theta, likelihood) |&gt;\n  ggplot(aes(x = theta, y = likelihood)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Chance of Passing\", y = \"Likelihood (scaled)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIn Bayesian terms, this evidence updates your belief."
  },
  {
    "objectID": "Bayesian_Statistics.html#the-posterior-updated-belief",
    "href": "Bayesian_Statistics.html#the-posterior-updated-belief",
    "title": "The Fundamentals of the Bayesian Framework",
    "section": "The Posterior (updated belief)",
    "text": "The Posterior (updated belief)\nYour updated belief, the posterior, combines your initial guess of roughly 50% with the strong evidence we have from 100 students. The resulting curve now shifts to the right — it’s taller near 85%–90% but still not at 100%, because you haven’t thrown out your original uncertainty.\n\n\nCode\nposterior &lt;- dbeta(theta, 120, 20)\n\nposterior_df &lt;- data.frame(\n  theta = rep(theta, 3),\n  density = c(prior, likelihood, posterior),\n  belief = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(theta))\n)\n\nposterior_df |&gt;\n  ggplot(aes(x = theta, y = density, color = belief)) +\n  geom_line(size = 1.2) +\n  labs(x = \"Chance of Passing\", y = \"Belief / Likelihood (scaled)\", colour = \"\") +\n  scale_x_continuous(labels = scales::percent) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"steelblue\", \"darkgreen\", \"firebrick\"))\n\n\n\n\n\n\n\n\n\nImportantly, in Bayesian statistics, we don’t have a single value for our parameter estimate (or posterior). Instead we have a full distribution of values. This becomes an insanely valuable property that can be exploited in many different ways, which we’ll get back to later on.\nFor now, your posterior is more-or-less the multiplication of your prior belief with the likelihood (\\(P(Data | Parameters)\\times P(Parameters)\\)). In this case, giving you a happy middle ground of ca. 85% chance to pass.\nThis is how Bayesian statistics works in practice:\n\nStart with a belief (or hypothesis).\nSee some evidence (i.e. collect some data).\nUpdate your belief accordingly.\n\nIn this case, seeing 100 successful students shifts your belief quite a lot, but not all the way to 100%, because your prior belief still matters! You know yourself after all! You’d be silly to just ignore that.\nIn short, this is kinda the scientific method, isn’t it? (pls don’t tell any frequentists that I said that…)"
  },
  {
    "objectID": "OccMods_WithCovariates.html",
    "href": "OccMods_WithCovariates.html",
    "title": "Including Covariates in Occupancy Models",
    "section": "",
    "text": "On this page, we’ll begin expanding on the intercept-only occupancy model we fit in Occupancy Models: The basics and begin the process of injecting some biology into the etosha elephant occupancy model.\nAs before, we’ll be using spOccupancy to fit the occupancy model. In addition, I’m also using ggplot2 and patchwork (which allows you to stich ggplots togeher) for visualisations.\nCode\nlibrary(spOccupancy)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "OccMods_WithCovariates.html#data-preparation",
    "href": "OccMods_WithCovariates.html#data-preparation",
    "title": "Including Covariates in Occupancy Models",
    "section": "Data preparation",
    "text": "Data preparation\nJust like in the previous page, we need to include our different datasets into a list. If you’re still not sure what a list is in R, think of it like a folder on your computer. You can add lots of different files to a folder (and any type of file), but they’re all “tied” together by being within the same folder. That’s the same as a list in R.\nHere, I’m going to do something seemingly strange. I’m going to create a list and include this into our etosha list. A list within a list. The reason is that we might have more than one detection covariate, so having a list, though redundant here, will make adding additional variables easier in the future.\n\n\nCode\n# Note you wouldn't need to do the dat$X.p[,,2] bit\n# That's just because the data is simulated.\ndet.covs &lt;- list(rain = dat$X.p[,,2])\n\netosha &lt;- list(\n  y = dat$y,\n  det.covs = det.covs\n)\n\n\nAnd we’re good to go on to the modelling."
  },
  {
    "objectID": "OccMods_WithCovariates.html#fitting-the-model",
    "href": "OccMods_WithCovariates.html#fitting-the-model",
    "title": "Including Covariates in Occupancy Models",
    "section": "Fitting the model",
    "text": "Fitting the model\nA few things to note in the R code;\n\nocc.formula = ~ 1 is the equivalent to \\(logit(\\psi_{i,j}) = \\beta_0\\)\n\nI.e. an intercept only occupancy model, meaning we are not including any covariates for the probability that a site is occupied.\n\ndet.formula = ~ rain is the equivalent to \\(logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i\\)\n\nI.e. we have both an intercept and slope given we have included rain in the model.\n\nWe specify that all data is contained in the list (i.e. “folder”) called etosha.\nThe remainder of the arguments (e.g. n.chains) can be ignored for now.\n\n\n\nCode\nfit &lt;- PGOcc(\n  # The state model (i.e. what % that elephants are present?)\n  # ~ 1 means we want an intercept only model (no covariates)\n  occ.formula = ~ 1, \n  # The observation model (i.e. what % that we see elephants if present?)\n  det.formula = ~ rain, \n  # Our carefully formatted dataset\n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)"
  },
  {
    "objectID": "OccMods_WithCovariates.html#interpreting",
    "href": "OccMods_WithCovariates.html#interpreting",
    "title": "Including Covariates in Occupancy Models",
    "section": "Interpreting",
    "text": "Interpreting\nHaving now fit the model to the data, we can see what we’ve learnt:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0175\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n\n\nCompared to the model on the previous page we now have additional information for Detection (logit scale); we have both an (Intercept) and rain. These are \\(\\alpha_0\\) and \\(\\alpha_1\\) from our detection model (\\(logit(p_i) = \\alpha_0 + \\alpha_1 \\times Rain_i\\)). If we really wanted to, we could now replace the parameter labels (e.g. the \\(\\alpha\\)s and \\(\\beta\\)s) with their now estimated values which would look like (rounding the estimates to two decimal points arbitrarily):\n\\[z_i \\sim Bernoulli(\\psi_i)\\\\  \\tag{State Stochastic}\\]\n\\[\nlogit(\\psi_i) = 1.88\\\\  \\tag{State Deterministic}\n\\]\n\\[\ny_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\  \\tag{Observation Stochastic}\n\\]\n\\[\nlogit(p_{i,j}) = -1.89 + 0.69 \\times Rain_i  \\tag{Observation Deterministic}\n\\]\nWith this, we can swap out \\(Rain\\) in the Observation Deterministic equation for any value of rain that we might be interested in, to see how it changes our estimated detection probability. For example, let’s see what happens when \\(Rain = 1\\) (equivalent to lots of rain based on how I simulated rain).\n\n\nCode\n-1.89 + 0.69 * 1\n\n\n[1] -1.2\n\n\nOur logit value is -1.2. How do we get that into probabilities that we can actually understand? Backtransform out of logit using plogis():\n\n\nCode\nplogis(-1.89 + 0.69 * 1)\n\n\n[1] 0.2314752\n\n\nAnd we get a ca. 23% chance to detect an elephant when \\(Rain = 1\\). What about when \\(Rain = 0\\) (equivalent to medium rain)? Well, we can do that easily enough now that we know thew general steps:\n\n\nCode\nplogis(-1.89 + 0.69 * 0)\n\n\n[1] 0.1312445\n\n\nWhen \\(Rain = 0\\), we predict a ca. 13% chance to detect elephants.\nThis approach, whereby we make a prediction for a specific value of \\(Rain\\) can be extended into making multiple predictions at once, such that we can then draw a line through them. Here’s how we’d do that."
  },
  {
    "objectID": "OccMods_WithCovariates.html#plot-predicted-relationships",
    "href": "OccMods_WithCovariates.html#plot-predicted-relationships",
    "title": "Including Covariates in Occupancy Models",
    "section": "Plot predicted relationships",
    "text": "Plot predicted relationships\nWe start by creating a sequence of \\(Rain\\) values, rather than doing one a time like we did above. Here we use the seq() function to create a sequence, which will range from the minimum \\(Rain\\) value to the maximum within our dataset. The number of values that we want in this sequence is specified as 20 but we can choose any value here - we just need enough that the line is drawn “accurately”.\n\n\nCode\nrain &lt;- seq(from = min(det.covs$rain),\n            to = max(det.covs$rain),\n            length.out = 20)\n\nrain\n\n\n [1] -2.71815687 -2.44127881 -2.16440076 -1.88752271 -1.61064465 -1.33376660\n [7] -1.05688855 -0.78001049 -0.50313244 -0.22625439  0.05062367  0.32750172\n[13]  0.60437977  0.88125783  1.15813588  1.43501393  1.71189199  1.98877004\n[19]  2.26564809  2.54252615\n\n\nNow we have our values, we don’t want to manually enter each value into our equation. Instead we can use the fact that R works with vectors (i.e. columns of data) to do this quickly and easily:\n\n\nCode\npred &lt;- plogis(-1.89 + 0.69 * rain)\npred\n\n\n [1] 0.02263134 0.02726568 0.03281714 0.03945308 0.04736516 0.05677017\n [7] 0.06790956 0.08104689 0.09646267 0.11444547 0.13527876 0.15922259\n[13] 0.18649040 0.21722152 0.25145143 0.28908330 0.32986526 0.37337882\n[19] 0.41904309 0.46613767\n\n\nNow for each value of rain, we have the predicted probability of detecting an elephant. Useful but you wouldn’t want to throw these two columns at your audience/reader and expect them to make sense of it. It’d be better if we include these in a figure.\nTo do so, we’ll combine both columns into a single dataset and plot using ggplot2:\n\n\nCode\ndf &lt;- data.frame(\n  pred,\n  rain\n)\n\nggplot(df) +\n  geom_line(aes(x = rain, y = pred))\n\n\n\n\n\n\n\n\n\nFrom this figure it now appears much more intuitive that increasing rain makes elephants easier to detect. We can do a little “tidying” of the figure to make it more visually pleasing:\n\n\nCode\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n\n\n\n\n\n\n\n\n\nBased on our figure, we’d now be able to make some biological inference. From what we see in our figure, we have evidence that elephants are easier to spot when it rains. If we were writing this as a paper, we might discuss why that was the case. However (and this is important) we should have really considered this before we did the analysis. The risk of making sense of this now is that we rationalise something that’s actually stupid. This is something called HARKing, or Hypothesising After the Results are Known and it’s a form of scientific fraud. Keep in mind, this is simulated data where the stakes are low. My goal here is to explain the methods to you. With real analysis, we need to be more thorough and responsible - so make sure you have your hypotheses before you do your analysis!"
  },
  {
    "objectID": "OccMods_WithCovariates.html#uncertainty",
    "href": "OccMods_WithCovariates.html#uncertainty",
    "title": "Including Covariates in Occupancy Models",
    "section": "Uncertainty",
    "text": "Uncertainty\nThe above figure is a good start but we’re missing any measure of uncertainty. If we were doing frequentist statistics, we would use 95% confidence intervals but these are exclusively frequentist. There are no 95% confidence intervals when we use the Bayesian statistical framework. Instead we have credible intervals. I’ll explain the Bayesian framework in a subsequent workflow but for now here is the formal definition of a credible interval:\n\nThere is a 95% probability that the True parameter value lies within the interval range, given the data and model.\n\nThis is in contrast with the frequentist confidence interval whose formal definition of a confidence interval is so bizarre and unintuitive that it’s barely useful. 95% credible intervals are actually useful and work exactly the way people think frequentist intervals work. I’ll explain how they work in a later page, so just trust me for now that they’re better.\nBut how do we include these in the figure? Well, the model summary makes it easy to find the values:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~1, det.formula = ~rain, data = etosha, n.samples = 2000, \n    verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0175\n\nOccurrence (logit scale): \n              Mean     SD   2.5%   50%  97.5%   Rhat ESS\n(Intercept) 1.8753 0.9465 0.3078 1.772 4.0465 1.0516 349\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8872 0.2946 -2.4610 -1.8939 -1.2939 1.0165 1038\nrain         0.6930 0.2384  0.2328  0.6877  1.1801 1.0025 2604\n\n\nThey’re the 2.5% and 97.5% values in our summary table. So all we need to do is repeat the code we used to make our predictions, except now using these credible interval values to get our measure of uncertainty. Importantly, if you remember back to BI3010 where you had to multiplya parameters standard error by 1.96, we do not need to do that here. That’s frequentist nonsense - we’re fancy Bayesian scientists now.\nLet’s do just that:\n\n\nCode\ndf$low &lt;- plogis(-2.4610 + 0.2328 * df$rain)\ndf$upp &lt;- plogis(-1.2939 + 1.1801 * df$rain)\ndf\n\n\n         pred        rain        low        upp\n1  0.02263134 -2.71815687 0.04336427 0.01096960\n2  0.02726568 -2.44127881 0.04611831 0.01514457\n3  0.03281714 -2.16440076 0.04903828 0.02087495\n4  0.03945308 -1.88752271 0.05213304 0.02871039\n5  0.04736516 -1.61064465 0.05541172 0.03936862\n6  0.05677017 -1.33376660 0.05888379 0.05376451\n7  0.06790956 -1.05688855 0.06255900 0.07302436\n8  0.08104689 -0.78001049 0.06644741 0.09846565\n9  0.09646267 -0.50313244 0.07055932 0.13151304\n10 0.11444547 -0.22625439 0.07490526 0.17351714\n11 0.13527876  0.05062367 0.07949599 0.22545433\n12 0.15922259  0.32750172 0.08434241 0.28752905\n13 0.18649040  0.60437977 0.08945559 0.35877811\n14 0.21722152  0.88125783 0.09484663 0.43685701\n15 0.25145143  1.15813588 0.10052670 0.51819600\n16 0.28908330  1.43501393 0.10650691 0.59858193\n17 0.32986526  1.71189199 0.11279825 0.67399363\n18 0.37337882  1.98877004 0.11941156 0.74135968\n19 0.41904309  2.26564809 0.12635738 0.79895748\n20 0.46613767  2.54252615 0.13364590 0.84638633\n\n\nAnd we can add in our uncertainty using geom_ribbon():\n\n\nCode\nggplot(df) +\n  geom_line(aes(x = rain, y = pred)) +\n  geom_ribbon(aes(x = rain, ymin = low, ymax = upp),\n              alpha = 0.3) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,1)) +\n  theme_minimal() +\n  labs(x = \"Mean rainfall\",\n       y = \"Predicted detection\\nprobability of elephants\")\n\n\n\n\n\n\n\n\n\nWith that, we have a publication ready figure."
  },
  {
    "objectID": "OccMods_WithCovariates.html#data-preparation-1",
    "href": "OccMods_WithCovariates.html#data-preparation-1",
    "title": "Including Covariates in Occupancy Models",
    "section": "Data preparation",
    "text": "Data preparation\nAs before, we provide our survey covariates as a list but our site covariates can be supplied as a dataframe. Keep in mind, both of these are included in another list.\n\n\nCode\n# Survey covariate\ndet.covs &lt;- list(rain = dat$X.p[,,2])\n# Site covariates\nocc.covs &lt;- data.frame(tree = tree, temp = temp)\netosha &lt;- list(\n  y = dat$y, # Detection history\n  det.covs = det.covs,\n  occ.covs = occ.covs\n)"
  },
  {
    "objectID": "OccMods_WithCovariates.html#fitting-the-model-1",
    "href": "OccMods_WithCovariates.html#fitting-the-model-1",
    "title": "Including Covariates in Occupancy Models",
    "section": "Fitting the model",
    "text": "Fitting the model\nThe core model we’re going to fit is:\n\\[z_i \\sim Bernoulli(\\psi_i)\\]\n\\[logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\\\\\]\n\\[y_{i,j} \\sim Bernoulli(p_{i,j} \\times z_i)\\\\\\]\n\\[logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\]\nThe main thing that I want to highlight here, other than having included \\(Tree\\) and \\(Temp\\) as effecting occupancy probability (\\(\\psi\\)), is that the subscripts are different.\nIn the deterministic part of the state model (\\(logit(\\psi_i) = \\beta_0 + \\beta_1 \\times Tree_i + \\beta_2 \\times Temp_i\\)) the variables are subscript by \\(i\\), indicating that we have one value of \\(Tree\\) or \\(Rain\\) for each site \\(i\\).\nBut in the deterministic part of the observation model (\\(logit(p_{i,j}) = \\alpha_0 + \\alpha_1 \\times Rain_{i,j}\\)) \\(Rain\\) is subscript by both \\(i\\) and \\(j\\). That’s because we have a different \\(Rain\\) value for each survey \\(j\\). This would be something we record every time we visit the site (or extract from some online source for each sampling period).\nKeep this in mind when you’re collecting your own data. Variables that you think affect occupancy will (normally) have one value per site. Variables that you think affect detection will (generally) have one value per survey. But speak to me if you think you want to try something different.\nTo fit this is relatively straight forward. For the occupancy model, we write occ.formula = ~ tree + temp, and for the detection model we write det.formula = ~ rain, and let the machinery do it’s thing.\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # Details to get the machinery to run that we'll ignore for now\n  n.chains = 4,\n  n.samples = 2000,\n  n.burn = 200,\n  verbose = FALSE)\n\n\nOnce it’s run, we can check the summary() to see what the parameters were estimated as:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 2000, verbose = FALSE, n.burn = 200, n.chains = 4)\n\nSamples per Chain: 2000\nBurn-in: 200\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 7200\nRun Time (min): 0.0173\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.9515 1.0600  0.1862  1.8333 4.2716 1.0441 347\ntree         0.3649 1.2506 -2.0020  0.3180 2.8503 1.0383 293\ntemp        -1.1005 1.1484 -3.2573 -1.1617 1.3032 1.0452 277\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8428 0.2636 -2.3606 -1.8424 -1.3199 1.0023 1404\nrain         0.1730 0.2214 -0.2499  0.1707  0.6115 1.0023 2983\n\n\nWe see that tree has a positive effect on occupancy (0.3649) and temp has a negative effect (-1.1005). This is kind of good. These estimates are close to the values I used in the simulation - but they don’t match up particularly well. Specifically, in the simulation tree was set to 0.3 but has been estimated as 0.37 (pretty close), while temp was set to -0.2 but has been estimated as -1.1 (kind of far away).\nSo we’ve got the general relationships reasonably well estimated but it’s not perfectly accurate to the True value I used in the simulation. This is where having credible intervals is useful. Keep in mind that credible intervals are, correctly, interpreted as having a 95% chance of containing the “True” value (the True value here is what each parameter was set to in the simulation). For both tree and temp the True parameter value is indeed within the 95% credible intervals! Both 95% CI are wide, but that’s good here! The model isn’t entirely sure what the values are (partly because of sample size and the effects being subtle) and that is being conveyed to us! We’re not deluding ourselves into thinking we have a perfect understanding when we don’t!\nThere are some warning signs that the model might not be performing especially well. The first is that the Rhat values are getting uncomfortably large for the Occurence parameters. My personal rule of thumb is Rhat values close to 1.05 is where I get worried. None of our parameters are at 1.05 but they’re close enough that I’m paying attention to them and want to see if I can fix the problem. We’ll come back to Rhat values when we talk about Bayesian statistics.\nThe other warning sign is the ESS for the Occurence model are all “low”. At least noticeably lower than the Detection model parameters. Again, the rule of thumb here is that we want hundreds or thousands of “Effective Sample Size”, so we’re technically OK but I’m still concerned. We’ll also cover ESS later on.\nNow, to be clear, we ignored these issues with the first model we ran in this document, whereas here we’re going to see if we can resolve this problem."
  },
  {
    "objectID": "OccMods_WithCovariates.html#resolving-issues",
    "href": "OccMods_WithCovariates.html#resolving-issues",
    "title": "Including Covariates in Occupancy Models",
    "section": "Resolving issues",
    "text": "Resolving issues\nTo fully appreciate the solution will require a deeper understanding of Bayesian statistics. But for now, we’re not going to cover this. Instead, I’ll simply say that the “machinery” of Bayesian statistics revolves around giving a number of algorithms (called “chains”) a number of guesses (called “iterations”) to try and figure out the most likely values for our parameters.\nThe two metrics we used above, Rhat and ESS, both monitor these chains and iterations and let us know if they are not in agreement (Rhat) or when they haven’t been well estimated (ESS). When Rhat values get high, and ESS values get low, it can suggest we need to give the algorithms more iterations (i.e. chances to intelligently guess).\nInevitably, the details are more complex than I can convey in two paragraphs but the general idea is there. If either number looks a bit worrying, it’s relatively cheap and easy to just rerun the model with more iterations and see if that solves the problem.\nLet’s do just that by increasing the number of iterations to 3000 per chain:\n\n\nCode\nfit &lt;- PGOcc(\n  occ.formula = ~ tree + temp, \n  det.formula = ~ rain, \n  data = etosha, \n  \n  # We're using four chains/algorithms\n  n.chains = 4,\n  # We allow 3000 guesses (increased from 2000)\n  n.samples = 3000,\n  # We ignore the first 300 (increased from 200)\n  # We ignore them because we assume the algorithms are not particularly reliable\n  # in the first ca. 10% of guesses\n  n.burn = 300,\n  verbose = FALSE)\n\n\nOnce run, we can check how well they ran:\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nPGOcc(occ.formula = ~tree + temp, det.formula = ~rain, data = etosha, \n    n.samples = 3000, verbose = FALSE, n.burn = 300, n.chains = 4)\n\nSamples per Chain: 3000\nBurn-in: 300\nThinning Rate: 1\nNumber of Chains: 4\nTotal Posterior Samples: 10800\nRun Time (min): 0.0247\n\nOccurrence (logit scale): \n               Mean     SD    2.5%     50%  97.5%   Rhat ESS\n(Intercept)  1.9586 1.0113  0.1496  1.8785 4.0979 1.0061 533\ntree         0.3562 1.2971 -2.0341  0.3101 2.9698 1.0498 415\ntemp        -1.2475 1.0628 -3.3183 -1.2672 0.9406 1.0176 470\n\nDetection (logit scale): \n               Mean     SD    2.5%     50%   97.5%   Rhat  ESS\n(Intercept) -1.8461 0.2664 -2.3631 -1.8497 -1.3123 1.0014 1625\nrain         0.1791 0.2196 -0.2462  0.1772  0.6243 1.0016 4411\n\n\nNow when we look at the Rhat and ESS values, we are doing better. Still not perfect but good enough for me to be happy that by these two metrics alone there’s nothing to suggest the machinery is struggling and that the parameters are being estimated as robustly as possible.\n\nImportantly, these are not the only metrics or tools available to us. We’ll check out the other options in a later document.\n\nAs far as we can tell, for now, we can trust this model and move on to plotting the predicted relationships."
  },
  {
    "objectID": "OccMods_WithCovariates.html#plot-predicted-relationships-1",
    "href": "OccMods_WithCovariates.html#plot-predicted-relationships-1",
    "title": "Including Covariates in Occupancy Models",
    "section": "Plot predicted relationships",
    "text": "Plot predicted relationships\nPreviously, we made the predictions ourselves by “hand”. This time, we’re going to use the predict() function to do this for us to make life easier on ourselves. Importantly, the process is exactly the same;\n\nCreate a “fake” dataset with the covariate values we want to predict for.\nApply this fake data to our equation, with the now estimated parameter values, to generate the predictions. We’ll make life easier for ourselves by using predict() rather than doing it by hand.\nConvert from logit values to probabilities.\nPlot the predicted probabilities against the fake covariate values to show the estimated relationship.\n\nRemember that occupancy models are really two \\(Bernoulli\\) GLMs that speak to each other. When using predict() we need to specify which “model” we want to predict from.\nNormally, for a GLM I’d start by making a figure for tree height. Just like in BI3010, remember that when we have multiple explanatory variables in a model, to make the figure for one variable, we hold all others constant at some other value - often the median.\nHowever, for these models there are a few extra (annoying) things we need to do and for me to explain here. As a warning some of these get quite technical. It might be a good time to take a break and come back when you’re fresh.\nWhenever you do tackle this next part, if you get confused, don’t worry. We can talk this through in person.\n\nModel matrix\nIn the code below I create something called a model matrix (model.matrix()). This is a way to turn our covariate data (like tree and temp) into the actual numbers and form used in the model. When we fit a model like ~ tree + temp, R internally builds a matrix where each column represents a variable (or intercept), and each row is a combination of values at a site. This matrix is what the model uses to calculate predictions.\nWhen we use predict() with a fitted occupancy model, we must supply this design matrix (X.0) because it matches the structure the model expects, just like when we built it during fitting.\n\n\nqlogis and plogis\nWhen we use predict() for our occupancy model, we get thousands of posterior draws (I’ll explain this later but for now, think of these are different likely values the parameter could be) for each site’s occupancy probability (i.e. these are already backtransformed out of logit values into probabilities for us).\nBut here’s the subtle issue (and it is really, really, super subtle).\nIf we calculate the mean or credible intervals of these already backtransformed probabilities, it can sometimes end up looking distorted (e.g. it might look like it increases, peaks, then decreases). So to be safe:\n\nWe have to take our probabilities and turn them back into logit values (annoyingly) using qlogis().\nThen we calculate the mean and credible intervals.\nThen we backtransform out of logit back into probabilities.\n\n\n\napply()\nWhen we summarise the posterior, we have one row per iteration (or guess) and one column per site. So to summarise, we need to take the mean across row (i.e. across all iterations) for each column. That’s what the apply() function is doing in our code.\nHonestly, the code is more complex than I would like. I wish I could think of a simpler way to do it (and I might ask my wife, who’s better at coding than me, to see if she can improve it) but this is the best I can do for now. Speak to me if you need help.\n\n\nCode\n# Step 1: Create a new fake data frame for tree (hold temp at median)\nfake &lt;- data.frame(\n  tree = seq(min(etosha$occ.covs$tree), max(etosha$occ.covs$tree), length.out = 100),\n  temp = median(etosha$occ.covs$temp)\n)\n\n# Step 2: Convert this into a model matrix\nX.0 &lt;- model.matrix(~ tree + temp, data = fake)\n\n# Step 3: Predict occupancy from model matrix\npred_occ &lt;- predict(fit, X.0 = X.0, type = \"occupancy\")\n\n# Step 4: Transform back into logit values\nlogit_psi_samples &lt;- qlogis(pred_occ$psi.0.samples)\n\n# Step 5: Summarize the posteriors\nfake &lt;- data.frame(\n  tree = fake$tree,\n  mean.psi = apply(logit_psi_samples, 2, mean),\n  lower = apply(logit_psi_samples, 2, function(x) quantile(x, 0.025)),\n  upper = apply(logit_psi_samples, 2, function(x) quantile(x, 0.975))\n)\n\n# Step 6: Backtransform into probabilities\nfake &lt;- data.frame(\n  tree = fake$tree,\n  mean.psi = plogis(fake$mean.psi),\n  lower = plogis(fake$lower),\n  upper = plogis(fake$upper)\n)\n\n# Step 7: Plot\nggplot(fake, aes(x = tree, y = mean.psi)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  labs(x = \"Tree height\", y = \"Occupancy probability (ψ)\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe repeat this for temp:\n\n\nCode\n# Step 1: Create a new data frame for temp (hold tree at median)\nfake &lt;- data.frame(\n  tree = median(etosha$occ.covs$tree),\n  temp = seq(min(etosha$occ.covs$temp), max(etosha$occ.covs$temp), length.out = 100)\n)\n\n# Step 2: Convert this into a model matrix\nX.0 &lt;- model.matrix(~ tree + temp, data = fake)\n\n# Step 3: Predict occupancy from model matrix\npred_occ &lt;- predict(fit, X.0 = X.0, type = \"occupancy\")\n\n# Step 4: Transform back into logit values\nlogit_psi_samples &lt;- qlogis(pred_occ$psi.0.samples)\n\n# Step 5: Summarize the posteriors\nfake &lt;- data.frame(\n  temp = fake$temp,\n  mean.psi = apply(logit_psi_samples, 2, mean),\n  lower = apply(logit_psi_samples, 2, function(x) quantile(x, 0.025)),\n  upper = apply(logit_psi_samples, 2, function(x) quantile(x, 0.975))\n)\n\n# Step 6: Backtransform into probabilities\nfake &lt;- data.frame(\n  temp = fake$temp,\n  mean.psi = plogis(fake$mean.psi),\n  lower = plogis(fake$lower),\n  upper = plogis(fake$upper)\n)\n\n# Step 5: Plot\nggplot(fake, aes(x = temp, y = mean.psi)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  labs(x = \"Temperature\", y = \"Occupancy probability (ψ)\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAnd finally for rain affecting our detection probability:\n\n\nCode\n# Step 1: Create a new data frame for temp (hold tree at median)\nfake &lt;- data.frame(\n  rain = seq(min(etosha$det.covs$rain), max(etosha$det.covs$rain), length.out = 100)\n)\n\n# Step 2: Convert this into a model matrix\nX.0 &lt;- model.matrix(~ rain, data = fake)\n\n# Step 3: Predict occupancy from model matrix\npred_det &lt;- predict(fit, X.0 = X.0, type = \"detection\")\n\n# Step 4: Transform back into logit values\nlogit_p_samples &lt;- qlogis(pred_det$p.0.samples)\n\n# Step 5: Summarize the posteriors\nfake &lt;- data.frame(\n  rain = fake$rain,\n  mean.p = apply(logit_p_samples, 2, mean),\n  lower = apply(logit_p_samples, 2, function(x) quantile(x, 0.025)),\n  upper = apply(logit_p_samples, 2, function(x) quantile(x, 0.975))\n)\n\n# Step 6: Backtransform into probabilities\nfake &lt;- data.frame(\n  rain = fake$rain,\n  mean.p = plogis(fake$mean.p),\n  lower = plogis(fake$lower),\n  upper = plogis(fake$upper)\n)\n\n# Step 5: Plot\nggplot(fake, aes(x = rain, y = mean.p)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  labs(x = \"Rainfall\", y = \"Detection probability (p)\") +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal()"
  },
  {
    "objectID": "OccMods_WithCovariates.html#dataset-one",
    "href": "OccMods_WithCovariates.html#dataset-one",
    "title": "Including Covariates in Occupancy Models",
    "section": "Dataset one",
    "text": "Dataset one\nThis is your detection history. Create an excel document, where each site (or camera) is on a row, and each survey period is a column. For the sake of argument, say that your sampling period is a 12 hour period, and you have three surveys at each location. Your excel should look something like:\n\n\nCode\ndh &lt;- as.data.frame(dat$y)\ncolnames(dh) &lt;- c(\"1\", \"2\", \"3\")\ndh\n\n\n   1 2 3\n1  0 0 0\n2  0 0 0\n3  0 0 0\n4  0 0 0\n5  0 0 0\n6  1 0 0\n7  0 0 0\n8  0 0 0\n9  0 1 1\n10 0 0 0\n11 0 0 0\n12 0 0 0\n13 0 0 0\n14 0 1 0\n15 0 0 1\n16 0 0 0\n17 0 0 0\n18 0 0 0\n19 0 0 0\n20 0 0 0\n21 0 0 0\n22 0 0 0\n23 0 0 0\n24 0 0 0\n25 0 0 1\n26 0 0 0\n27 0 0 1\n28 0 0 1\n29 0 1 0\n30 0 1 0\n31 0 0 0\n32 0 0 0\n33 1 0 0\n34 0 0 0\n35 1 0 0\n36 0 0 0\n37 0 0 0\n38 0 0 0\n39 1 0 0\n40 0 1 0\n41 0 0 0\n42 0 0 0\n43 1 0 0\n44 0 0 0\n45 0 0 0\n46 0 1 0\n47 0 0 1\n48 0 0 0\n49 0 0 0\n50 0 0 0\n51 0 0 0\n52 0 0 1\n53 0 0 0\n54 0 1 0\n55 0 1 0\n56 0 0 0\n57 0 0 0\n58 0 0 1\n59 0 0 0\n60 1 0 0\n61 0 0 0\n62 0 0 0\n63 0 0 0\n64 0 0 0"
  },
  {
    "objectID": "OccMods_WithCovariates.html#dataset-two",
    "href": "OccMods_WithCovariates.html#dataset-two",
    "title": "Including Covariates in Occupancy Models",
    "section": "Dataset two",
    "text": "Dataset two\nCreate another excel spreadsheet which will contain all of your site level covariates. This should be organised in the same order as your detection history dataset (above), such that if site 1 is top of your detection history spreadsheet, it should be top here as well. THIS IS CRUCIAL - BE CAREFUL AND MAKE SURE THIS IS DONE CORRECTLY.\nIf you are interested in distance to road, have that be in here. If you are interested in distance to nearest pen, include it here. Each covariate should be a column:\n\n\nCode\ndata.frame(road = round(rgamma(50, 1, 2), digits = 3), pen = round(rgamma(50, 2, 3), digits = 1))\n\n\n    road pen\n1  1.413 0.6\n2  0.365 0.4\n3  0.748 1.3\n4  0.028 1.5\n5  0.206 0.7\n6  0.486 0.8\n7  1.142 1.0\n8  0.316 0.2\n9  0.894 0.8\n10 1.180 0.5\n11 0.108 0.6\n12 0.036 0.1\n13 0.273 1.1\n14 1.003 0.5\n15 0.381 1.2\n16 0.275 0.5\n17 0.130 0.8\n18 1.046 0.2\n19 0.269 1.7\n20 0.914 0.5\n21 0.582 0.7\n22 1.695 0.4\n23 0.016 1.1\n24 0.032 1.8\n25 1.028 0.8\n26 0.039 0.2\n27 0.011 0.5\n28 0.601 0.9\n29 0.417 1.1\n30 0.600 1.8\n31 0.041 1.7\n32 0.013 0.2\n33 0.403 0.6\n34 0.018 0.4\n35 0.607 0.6\n36 0.277 0.2\n37 0.542 1.0\n38 0.123 0.1\n39 2.061 1.0\n40 1.840 0.5\n41 0.844 0.7\n42 0.023 1.3\n43 0.630 0.2\n44 0.212 0.3\n45 1.082 0.5\n46 0.032 0.5\n47 0.600 0.1\n48 0.690 0.5\n49 0.902 0.8\n50 0.491 0.3"
  },
  {
    "objectID": "OccMods_WithCovariates.html#dataset-three",
    "href": "OccMods_WithCovariates.html#dataset-three",
    "title": "Including Covariates in Occupancy Models",
    "section": "Dataset three (+)",
    "text": "Dataset three (+)\nThe final dataset(s) contain your survey covariates, but importantly, you will need a new excel document for each survey covariate. So rather than having them all be together like in the site covariate speadsheet, here they’ll look more like the detection history spreadsheet. Each row is a site and each column is a survey.\nSo if you were collecting data on light pollution you may have:\n\n\nCode\ndata.frame(\"1\" = rnorm(50, 0, 1), \"2\" = rnorm(50, 1, 2), \"3\" = rnorm(50, -1, 1), check.names = FALSE)\n\n\n              1           2          3\n1  -2.080361780  3.31486871 -2.8792348\n2   1.193689080 -2.13245041 -0.9858541\n3   0.211096481  2.37159153 -2.5450504\n4  -0.804509302 -3.04365088  0.8123365\n5   1.242683769  1.98512256  0.5347766\n6   1.672370774 -0.86347112 -0.2739347\n7  -0.440110803  3.85228974 -1.2469645\n8   0.976970308  2.38465991  0.4144317\n9  -0.247292942  0.80483629 -2.0085643\n10  1.193662839  3.68860057 -0.7706345\n11  0.582106314  2.30774905 -0.7437351\n12 -0.212421470 -2.58811871  1.2101485\n13 -0.078270865  0.75188625 -0.5215089\n14  1.150840694 -0.18570337 -0.4912487\n15  0.575815240  1.98405202 -2.2456465\n16 -0.803085447  5.26614095 -1.3067553\n17 -0.003867098  0.83398723 -2.7237629\n18 -0.321186684  3.01674217 -1.7598763\n19  0.271986638  1.35000525 -1.1381807\n20 -0.129475777  1.40291064 -1.2840863\n21 -0.730130772  1.24783303 -0.7199777\n22  1.425657691 -0.16389270 -2.3773648\n23  1.165602885 -0.22977912 -1.5775563\n24  2.003211580 -3.12713915 -0.3410146\n25 -0.897058994  0.42866992 -0.6918805\n26  1.623248729  1.10980969  0.1850369\n27  3.082054870  0.59193629  0.3879526\n28  0.373362088  1.74746118 -1.5760394\n29 -0.138822386  0.67546058 -1.8727836\n30  0.888619009  7.36368823  0.1605281\n31  2.703796014  0.22554511 -2.8165337\n32 -0.338148148  0.48416927 -0.0640613\n33  1.038384187 -0.66954237 -1.2304174\n34 -0.344291247  4.28962590 -2.6571024\n35 -0.096846364 -0.05826384 -1.1676174\n36  0.302828810 -0.61888378 -1.3100760\n37  0.401658586  2.23291672 -1.7689837\n38 -0.093476762 -0.58908992  0.5388878\n39  0.026884978  0.87524859 -2.8523438\n40 -0.333933172  2.90140407 -2.1994696\n41  0.690967183  0.37129946 -0.7643009\n42  0.019305684  3.84486943 -0.4132671\n43 -0.357612499 -1.18882425 -0.8042535\n44  2.085887418 -1.13376454 -0.2113516\n45 -0.007865673  1.62715521 -1.8920737\n46 -2.354875012  1.25226610 -0.7344787\n47 -0.622007893  1.88331825 -1.2703962\n48  0.246180327 -0.89037300 -1.6181206\n49 -0.674860159 -0.33612045 -1.4240738\n50 -0.961984764  4.44458858 -1.2228367\n\n\nBut if you were also collecting data on if a field was being ploughed or not you may have:\n\n\n\nCode\ndata.frame(\"1\" = sample(size = 50, x = c(\"Plough\", \"No plough\"), prob = c(0.5, 0.5), replace = TRUE), \n           \"2\" = sample(size = 50, x = c(\"Plough\", \"No plough\"), prob = c(0.1, 0.9), replace = TRUE), \n           \"3\" = sample(size = 50, x = c(\"Plough\", \"No plough\"), prob = c(0.8, 0.2), replace = TRUE), \n           check.names = FALSE)\n\n\n           1         2         3\n1  No plough No plough No plough\n2  No plough No plough    Plough\n3     Plough No plough    Plough\n4     Plough No plough No plough\n5  No plough No plough    Plough\n6  No plough No plough No plough\n7     Plough No plough No plough\n8  No plough No plough    Plough\n9     Plough No plough    Plough\n10 No plough No plough    Plough\n11    Plough No plough No plough\n12    Plough    Plough    Plough\n13 No plough No plough    Plough\n14 No plough No plough    Plough\n15 No plough No plough    Plough\n16 No plough No plough    Plough\n17 No plough No plough    Plough\n18 No plough No plough No plough\n19    Plough No plough No plough\n20    Plough No plough    Plough\n21    Plough No plough    Plough\n22 No plough No plough    Plough\n23 No plough No plough    Plough\n24 No plough No plough No plough\n25    Plough No plough    Plough\n26    Plough No plough No plough\n27 No plough No plough    Plough\n28 No plough No plough    Plough\n29 No plough No plough    Plough\n30 No plough No plough    Plough\n31 No plough No plough    Plough\n32    Plough No plough    Plough\n33 No plough No plough    Plough\n34 No plough No plough No plough\n35    Plough No plough    Plough\n36 No plough No plough    Plough\n37 No plough No plough No plough\n38    Plough No plough    Plough\n39    Plough No plough    Plough\n40 No plough No plough    Plough\n41 No plough No plough No plough\n42 No plough No plough No plough\n43 No plough No plough No plough\n44 No plough No plough    Plough\n45    Plough No plough    Plough\n46 No plough No plough    Plough\n47 No plough No plough    Plough\n48    Plough No plough No plough\n49 No plough No plough    Plough\n50 No plough No plough    Plough"
  }
]